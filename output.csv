filename,page_no,label,text,bounding_box
0001_ChatGPT_is_bullshit.pdf,1,title,"blished online: 8 June 2024
he Author(s) 2024
chael Townsen Hicks1
 · James Humphries1 · Joe Slater1","[63.15428571428562, 141.90857142857138, 426.58285714285716, 183.05142857142854]"
0001_ChatGPT_is_bullshit.pdf,1,author,"bstract
The Author(s) 2024","[61.78285714285708, 188.53714285714284, 223.6114285714285, 221.45142857142852]"
0001_ChatGPT_is_bullshit.pdf,1,author,st in large language models: machi,"[229.0971428571428, 185.79428571428568, 367.61142857142846, 225.56571428571422]"
0001_ChatGPT_is_bullshit.pdf,1,author,,"[447.1542857142857, 220.07999999999993, 363.49714285714276, 189.9085714285714]"
0001_ChatGPT_is_bullshit.pdf,1,abstract,"is bullshitting, in the Frankfurtian sense (Frankfurt, 2002, 
2005). Because these programs cannot themselves be con-
cerned with truth, and because they are designed to produce 
text that looks truth-apt without any actual concern for truth, 
it seems appropriate to call their outputs bullshit.
We think that this is worth paying attention to. Descrip-
tions of new technology, including metaphorical ones, guide 
roduction
e language models (LLMs), programs which use reams 
vailable text and probability calculations in order to 
te seemingly-human-produced writing, have become 
easingly sophisticated and convincing over the last 
ral years, to the point where some commentators sug-
words Artificial intelligence · Large language models · LLMs · ChatGPT · Bullshit · Frankfurt · Assertion · 
tent","[70.01142857142851, 313.33714285714285, 791.382857142857, 449.1085714285714]"
0001_ChatGPT_is_bullshit.pdf,1,heading,"at exactly these programs are
 question about the nature an
ed, and of its connection to tr","[65.8971428571428, 523.1657142857142, 179.72571428571428, 556.0799999999999]"
0001_ChatGPT_is_bullshit.pdf,1,body,"metaphor which will misinfo
and other interested parties.
The structure of the paper 
tion, we outline how ChatGP
Next, we consider the view 
errors, they are lying or hallu
uttering falsehoods, or blam
basis of misleading input inf
ther of these ways of thinking
lying and hallucinating requir
of their statements, whereas L
to accurately represent the w
inst the view that when ChatGPT and the like produce 
e claims they are lying or even hallucinating, and in 
our of the position that the activity they are engaged in 
Michael Townsen Hicks
Michael.hicks@glasgow.ac.uk
James Humphries
James.Humphries@glasgow.ac.uk
Joe Slater
Joe.Slater@glasgow.ac.uk
University of Glasgow, Glasgow, Scotland
Content courtesy of Springer Nature, terms of use apply. Rights","[67.26857142857133, 562.9371428571428, 425.2114285714285, 855.0514285714286]"
0001_ChatGPT_is_bullshit.pdf,1,body,"ymakers and the public the 
isrepresenting the world, and 
We argue that this is an inapt 
rm the public, policymakers, 
is as follows: in the first sec-
T and similar LLMs operate. 
that when they make factual 
ucinating: that is, deliberately 
elessly uttering them on the 
ormation. We argue that nei-
g are accurate, insofar as both 
e some concern with the truth 
LLMs are simply not designed 
ay the world is, but rather to 
1 3
s reserved.","[426.58285714285716, 523.1657142857142, 785.8971428571429, 1023.7371428571428]"
0001_ChatGPT_is_bullshit.pdf,1,date,"pplications of th
ations”. We argu","[152.2971428571429, 242.0228571428571, 214.0114285714286, 257.1085714285714]"
0001_ChatGPT_is_bullshit.pdf,2,body,"gest, is very close to at least one way that Frankfurt talks 
ut bullshit. We draw a distinction between two sorts of 
lshit, which we call ‘hard’ and ‘soft’ bullshit, where the 
mer requires an active attempt to deceive the reader or 
ener as to the nature of the enterprise, and the latter only 
uires a lack of concern for truth. We argue that at mini-
m, the outputs of LLMs like ChatGPT are soft bullshit: 
lshit–that is, speech or text produced without concern for 
truth–that is produced without any intent to mislead the 
ience about the utterer’s attitude towards truth. We also 
gest, more controversially, that ChatGPT may indeed 
duce hard bullshit: if we view it as having intentions (for 
mple, in virtue of how it is designed), then the fact that it 
esigned to give the impression of concern for truth quali-
 it as attempting to mislead the audience about its aims, 
ls, or agenda. So, with the caveat that the particular kind 
bullshit ChatGPT outputs is dependent on particular 
ws of mind or meaning, we conclude that it is appropriate 
alk about ChatGPT-generated text as bullshit, and flag up 
y it matters that – rather than thinking of its untrue claims 
ies or hallucinations – we call bullshit on ChatGPT.
hat is ChatGPT?
ge language models are becoming increasingly good 
carrying on convincing conversations. The most promi-
t large language model is OpenAI’s ChatGPT, so it’s the 
 we will focus on; however, what we say carries over to 
er neural network-based AI chatbots, including Google’s 
d chatbot, AnthropicAI’s Claude (claude.ai), and Meta’s 
aMa. Despite being merely complicated bits of software, 
se models are surprisingly human-like when discussing a 
de variety of topics. Test it yourself: anyone can go to the 
human speech or writing. T
goal, insofar as they have 
text. They do so by estimatin
word will appear next, given
The machine does this by
tical model, one which is b
mostly taken from the intern
little input from human rese
system; rather, the model 
large number of nodes, whi
for a word to appear in a tex
that has come before it. Rat
ability functions by hand, re
amounts of text and train it
predictions about this trainin
tive or negative feedback de
correctly. Given enough tex
statistical model giving the l
block of text all by itself.
This model associates w
locates it in a high-dimensi
words that occur in simila
which don’t. When produci
ous string of words and con
ing the word’s surroundings
occur in the context of simil
heuristically as representing
the content of its context. 
constructed using machine 
cal analysis of large amoun
sorts of similarity are repres
high-dimensional vector spa
similar they are to what we 
The model then takes these ","[65.8971428571428, 84.30857142857138, 418.35428571428565, 487.5085714285714]"
0001_ChatGPT_is_bullshit.pdf,2,heading,"de variety of topics. Test it yourself: anyon
enAI web interface and ask for a ream of 
roduces text which is indistinguishable fro
erage English speaker or writer. The varie
milarity to human-generated text that GPT-","[64.52571428571426, 497.1085714285714, 229.0971428571428, 545.1085714285714]"
0001_ChatGPT_is_bullshit.pdf,2,body,,"[57.66857142857137, 1016.8800000000001, 422.46857142857135, 547.8514285714285]"
0001_ChatGPT_is_bullshit.pdf,2,body,"his means that their primary 
ne, is to produce human-like 
 the likelihood that a particular 
he text that has come before.
constructing a massive statis-
sed on large amounts of text, 
t. This is done with relatively 
rchers or the designers of the 
 designed by constructing a 
h act as probability functions 
 given its context and the text 
er than putting in these prob-
earchers feed the system large 
by having it make next-word 
g data. They then give it posi-
ending on whether it predicts 
 the machine can construct a 
kelihood of the next word in a 
h each word a vector which 
nal abstract space, near other 
contexts and far from those 
g text, it looks at the previ-
ructs a different vector, locat-
– its context – near those that 
 words. We can think of these 
the meaning of the word and 
But because these spaces are 
earning by repeated statisti-
 of text, we can’t know what 
nted by the dimensions of this 
e. Hence we do not know how 
ink of as meaning or context. 
wo vectors and produces a set 
rd; it selects and places one of 
h not always the most likely. 
e randomly amongst the more 
creative and human-like text; 
this is called the ‘temperature’ 
he model’s temperature makes 
d more likely to produce false-
s the process until it has a rec-
response to whatever prompt 
 surprising that LLMs have a 
 goal is to provide a normal-
pt, not to convey information 
ocutor. Examples of this are 
e, a lawyer recently prepared 
d discovered to his chagrin 
were not real (Weiser, 2023); 
s reserved.","[426.58285714285716, 78.8228571428571, 791.382857142857, 1022.3657142857143]"
0001_ChatGPT_is_bullshit.pdf,3,body,"ed with “bogus judicial decisions, with bogus quotes and 
gus internal citations”. Similarly, when computer science 
earchers tested ChatGPT’s ability to assist in academic 
ting, they found that it was able to produce surprisingly 
mprehensive and sometimes even accurate text on bio-
ical subjects given the right prompts. But when asked to 
duce evidence for its claims, “it provided five references 
ing to the early 2000s. None of the provided paper titles 
sted, and all provided PubMed IDs (PMIDs) were of dif-
ent unrelated papers” (Alkaissi and McFarland, 2023). 
ese errors can “snowball”: when the language model is 
ed to provide evidence for or a deeper explanation of 
alse claim, it rarely checks itself; instead it confidently 
ducesmore false but normal-sounding claims (Zhang et 
2023). The accuracy problem for LLMs and other gen-
tive Ais is often referred to as the problem of “AI hal-
ination”: the chatbot seems to be hallucinating sources 
 facts that don’t exist. These inaccuracies are referred 
s “hallucinations” in both technical (OpenAI, 2023) and 
pular contexts (Weise & Metz, 2023).
These errors are pretty minor if the only point of a chatbot 
o mimic human speech or communication. But the com-
ies designing and using these bots have grander plans: 
tbots could replace Google or Bing searches with a more 
r-friendly conversational interface (Shah & Bender, 
22; Zhu et al., 2023), or assist doctors or therapists in 
dical contexts (Lysandrou, 2023). In these cases, accu-
y is important and the errors represent a serious problem.
One attempted solution is to hook the chatbot up to some 
t of database, search engine, or computational program 
t can answer the questions that the LLM gets wrong 
u et al., 2023). Unfortunately, this doesn’t work very 
l either. For example, when ChatGPT is connected to 
lfram Alpha, a powerful piece of mathematical software, 
mproves moderately in answering simple mathematical 
stions. But it still regularly gets things wrong, especially 
questions which require multi-stage thinking (Davis & 
ronson, 2023). And when connected to search engines or 
er databases, the models are still fairly likely to provide 
e information unless they are given very specific instruc-
ns–and even then things aren’t perfect (Lysandrou, 2023). 
enAI has plans to rectify this by training the model to do 
p by step reasoning (Lightman et al., 2023) but this is 
te resource-intensive, and there is reason to be doubtful 
t it will completely solve the problem—nor is it clear that 
result will be a large language model, rather than some 
ader form of AI.
Solutions such as connecting the LLM to a database don’t 
rk because, if the models are trained on the database, 
n the words in the database affect the probability that the 
tbot will add one or another word to the line of text it is 
to the text in the database; doi
that it reproduces the informa
means ensures that it will.
On the other hand, the LLM
database by allowing it to c
similar to the way it consult
locutors. In this way, it can u
as text which it responds to a
this can work: when a human
model a question, it can then
query for the database. Then
database as an input and bu
back to the human questione
the chatbots might ask the da
misinterpret its answer (Davi
often struggles to formulate 
fram Alpha can accept or that
is not unrelated to the fact th
generates a query for the datab
it does so in the same way it 
estimating the likelihood that
kind of thing the database wil
One might worry that these
the accuracy of chatbots are
phor of AI hallucinations. If t
lucinating sources, one way 
it in touch with real rather t
attempts to do so have failed.
The problem here isn’t th
lucinate, lie, or misrepresent t
they are not designed to repr
they are designed to convey
when they are provided with
use this, in one way or another
convincing. But they are not 
convey or transmit the inform
rag Shah and Emily Bender 
of language models (whose tr
given context) is actually d
temporal reasoning, etc. To t
get the right answer to such q
happened to synthesize relev
their training data. No reason
language models are prone to
they are not designed to ex
information in natural langua
the form of language” (Shah 
els aren’t designed to transmi
be too surprised when their as
Content courtesy of Springer Nature, terms of use apply. Right","[65.8971428571428, 77.45142857142852, 422.46857142857135, 1030.5942857142857]"
0001_ChatGPT_is_bullshit.pdf,3,body,"g so will make it more likely 
on in the database but by no 
 can also be connected to the 
nsult the database, in a way 
or talks to its human inter-
e the outputs of the database 
d builds on. Here’s one way 
nterlocutor asks the language 
translate the question into a 
it takes the response of the 
ds a text from it to provide 
But this can misfire too, as 
abase the wrong question, or 
& Aaronson, 2023). “GPT-4 
problem in a way that Wol-
roduces useful output.” This 
t when the language model 
se or computational module, 
enerates text for humans: by 
ome output “looks like’’ the 
correspond with.
ailed methods for improving 
onnected to the inapt meta-
e AI is misperceiving or hal-
 rectify this would be to put 
an hallucinated sources. But 
 large language models hal-
e world in some way. It’s that 
ent the world at all; instead, 
convincing lines of text. So 
 database of some sort, they 
to make their responses more 
n any real way attempting to 
tion in the database. As Chi-
ut it: “Nothing in the design 
ning task is to predict words 
igned to handle arithmetic, 
e extent that they sometimes 
estions is only because they 
nt strings out of what was in 
g is involved […] Similarly, 
making stuff up […] because 
ess some underlying set of 
e; they are only manipulating 
 Bender, 2022). These mod-
nformation, so we shouldn’t 
ertions turn out to be false.
1 3
reserved.","[432.0685714285715, 76.07999999999996, 790.0114285714285, 986.7085714285714]"
0001_ChatGPT_is_bullshit.pdf,4,heading,"becaus
tante tr
are try
facts. T
nkfurtian bullshit and lying
ny popular discussions of ChatGPT call its false state-","[67.26857142857133, 76.07999999999996, 331.9542857142857, 115.85142857142854]"
0001_ChatGPT_is_bullshit.pdf,4,heading,,"[65.8971428571428, 151.50857142857143, 283.95428571428573, 115.85142857142854]"
0001_ChatGPT_is_bullshit.pdf,4,body,"the act of producing these ut
be classed as bullshit, it mus
explicit intentions that one h
a false belief in the hearer. O
accompanied by the intention
utterance. So far this story is
positive intentions be manifes
Throughout most of Fran
acterisation of bullshit is neg
requires “no conviction” from
truth is (2005: 55), that the b
to the truth (2005: 61) and tha
even intend to do so, either ab
the facts to be” (2005: 54). La
feature” of bullshit as “a lack
indifference to how things real
340). These suggest a negative
be classed as bullshit, it only n
ship to the truth.
However, in places, a pos
Frankfurt says what a bullshitt
“…does necessarily attemp
enterprise. His only indispens
is that in a certain way he mi
(2005: 54).
This is somewhat surprisin
bullshit to utterances accompa
tion. However, some of Frank
this feature. When Fania Pasca
“feeling like a dog that has jus
Wittgenstein, it stretches cred
intending to deceive him abou
how run-over dogs felt. And 
bullshit are typically described
der whether the positive condi
Bullshit distinctions
Should utterances without an 
bullshit? One reason in favou
or embracing a plurality of bu
furt’s comments on the danger
“In contrast [to merely un
ference to the truth is extrem
of civilized life, and the vital
y to think about it. We will argue that these falsehoods 
n’t hallucinations later. For now, we’ll discuss why these 
uths aren’t lies but instead are bullshit.
The topic of lying has a rich philosophical literature. In 
ing’, Saint Augustine distinguished seven types of lies, 
 his view altered throughout his life. At one point, he 
ended the position that any instance of knowingly utter-
a false utterance counts as a lie, so that even jokes 
taining false propositions, like –
 entered a pun competition and because I really 
wanted to win, I submitted ten entries. I was sure one 
of them would win, but no pun in ten did.
ould be regarded as a lie, as I have never entered such 
ompetition (Proops & Sorensen, 2023: 3). Later, this 
w is refined such that the speaker only lies if they intend 
hearer to believe the utterance. The suggestion that the 
aker must intend to deceive is a common stipulation in 
ature on lies. According to the “traditional account” of 
g:
To lie = df. to make a believed-false statement to 
nother person with the intention that the other person 
believe that statement to be true (Mahon, 2015).
 our purposes this definition will suffice. Lies are gen-
ly frowned upon. But there are acts of misleading tes-
ony which are criticisable, which do not fall under the 
brella of lying.1 These include spreading untrue gossip, 
ch one mistakenly, but culpably, believes to be true. 
other class of misleading testimony that has received 
icular attention from philosophers is that of bullshit. 
s everyday notion was analysed and introduced into the 
osophical lexicon by Harry Frankfurt.2
Frankfurt understands bullshit to be characterized not by 
ntent to deceive but instead by a reckless disregard for 
truth. A student trying to sound knowledgeable without 
particularly surprising position is espoused by Fichte, who regards 
ing not only lies of omission, but knowingly not correcting some-
who is operating under a falsehood. For instance, if I was to wear 
g, and someone believed this to be my real hair, Fichte regards this 
lie, for which I am culpable. Bacin (2021) for further discussion 
ichte’s position.
riginally published in Raritan, VI(2) in 1986. References to that 
k here are from the 2005 book version.
3
Content courtesy of Springer Nature, terms of use apply. Rights","[67.26857142857133, 156.9942857142857, 425.2114285714285, 896.1942857142858]"
0001_ChatGPT_is_bullshit.pdf,4,body,"potential voters, and a dilet-
ng story: none of these people 
are also not trying to convey 
ullshitting.
h a noun and a verb: an utter-
an instance of bullshit, as can 
erances. For an utterance to 
 not be accompanied by the 
as when lying, i.e., to cause 
f course, it must also not be 
s characterised by an honest 
 entirely negative. Must any 
ed in the utterer?
kfurt’s discussion, his char-
ative. He notes that bullshit 
m the speaker about what the 
ullshitter “pays no attention” 
 they “may not deceive us, or 
out the facts or what he takes 
er, he describes the “defining 
 of concern with truth, or an 
ly are [our emphasis]” (2002: 
 picture; that for an output to 
eeds to lack a certain relation-
itive intention is presented. 
er ….
pt to deceive us about is his 
ably distinctive characteristic 
srepresents what he is up to” 
g. It restricts what counts as 
nied by a higher-order decep-
furt’s examples seem to lack 
l describes her unwell state as 
t been run over” to her friend 
ulity to suggest that she was 
ut how much she knew about 
given how the conditions for 
d as negative, we might won-
tion is really necessary.
intention to deceive count as 
 of expanding the definition, 
llshit, is indicated by Frank-
s of bullshit.
intelligible discourse], indif-
ely dangerous. The conduct 
ty of the institutions that are 
 reserved.","[429.3257142857142, 78.8228571428571, 788.64, 842.7085714285714]"
0001_ChatGPT_is_bullshit.pdf,4,heading,,"[591.1542857142856, 871.5085714285714, 430.6971428571428, 844.08]"
0001_ChatGPT_is_bullshit.pdf,4,body,,"[426.58285714285716, 1025.1085714285714, 802.3542857142858, 886.5942857142857]"
0001_ChatGPT_is_bullshit.pdf,5,body,"has indifference towards the t
Hard bullshit Bullshit produ
lead the audience about the u
Soft bullshit Bullshit produce
lead the hearer regarding the 
The general notion of bul
sions, we might be confiden
soft bullshit or hard bullshit, b
ignorance of the speaker’s h
case, we can still call bullshit
Frankfurt’s own explicit
requirements about producer
whereas soft bullshit seems t
examples, such as that of Pa
genstein, or the work of adv
helpful to situate these distinc
On our view, hard bullshit is 
sam (2019), and Frankfurt’s
son that all of these views ho
present, rather than merely a
bullshit: a kind of “epistemi
tude towards truth on Cassam
an intent to mislead the heare
Frankfurt’s view. In our fina
ChatGPT may be a hard b
to note that it seems to us t
accounts cited here, requires o
or not LLMs can be agents, 
argumentative burdens.
Soft bullshit, by contrast, c
tive requirement – that is, the
we have classed as definitiona
reasons given above. As we a
a soft bullshitter or a bullshi
an agent then it can neither h
nor towards deceiving heare
properly, its users’) agenda.
It’s important to note tha
of bullshitting will have the
cern Frankfurt: as he says,
extremely dangerous…by th
5  It’s worth noting that somethin
and soft bullshitting we draw also 
that we might think of someone as
at bullshit, however frequently or 
they are merely “disposed to bulls
r the distinction between the true and the false. Insofar as 
e authority of this distinction is undermined by the preva-
nce of bullshit and by the mindlessly frivolous attitude 
at accepts the proliferation of bullshit as innocuous, an 
dispensable human treasure is squandered” (2002: 343).
These dangers seem to manifest regardless of whether 
ere is an intention to deceive about the enterprise a speaker 
engaged in. Compare the deceptive bullshitter, who does 
m to mislead us about being in the truth-business, with 
meone who harbours no such aim, but just talks for the 
ke of talking (without care, or indeed any thought, about 
e truth-values of their utterances).
One of Frankfurt’s examples of bullshit seems better 
ptured by the wider definition. He considers the adver-
ing industry, which is “replete with instances of bullshit 
 unmitigated that they serve among the most indisputable 
d classic paradigms of the concept” (2005:22). However, 
seems to misconstrue many advertisers to portray their 
ms as to mislead about their agendas. They are expected to 
y misleading things. Frankfurt discusses Marlboro adverts 
th the message that smokers are as brave as cowboys 
002: 341). Is it reasonable to suggest that the advertisers 
etended to believe this?
Frankfurt does allow for multiple species of bullshit 
002: 340).3 Following this suggestion, we propose to 
visage bullshit as a genus, and Frankfurt’s intentional 
llshit as one species within this genus. Other species may 
clude that produced by the advertiser, who anticipates that 
 one will believe their utterances4 or someone who has 
 intention one way or another about whether they mis-
ad their audience. To that end, consider the following 
stinction:
In making this comment, Frankfurt concedes that what Cohen calls 
ullshit” is also worthy of the name. In Cohen’s use (2002), bullshit is 
ype of unclarifiable text, which he associates with French Marxists. 
veral other authors have also explored this area in various ways in 
ent years, each adding valuable nuggets to the debate. Dennis Whit-
mb and Kenny Easwaran expand the domains to which “bullshit” 
n be applied. Whitcomb argues there can be bullshit questions (as 
ll as propositions), whereas Easwaran argues that we can fruitfully 
w some activities as bullshit (2023).While we accept that these offer 
uable streaks of bullshit insight, we will restrict our discussion to 
 Frankfurtian framework. For those who want to wade further into 
se distinctions, Neil Levy’s Philosophy, Bullshit, and Peer Review 
023) offers a taxonomical overview of the bullshit out there.
This need not undermine their goal. The advertiser may intend 
impress associations (e.g., positive thoughts like “cowboys” or 
rave” with their cigarette brand) upon their audience, or reinforce/
til brand recognition.Frankfurt describes this kind of scenario as 
curring in a “bull session”: “Each of the contributors to a bull ses-
n relies…upon a general recognition that what he expresses or says ","[61.78285714285708, 82.93714285714282, 421.0971428571427, 689.1085714285714]"
0001_ChatGPT_is_bullshit.pdf,5,body,"ruth of the utterance.
ced with the intention to mis-
tterer’s agenda.
d without the intention to mis-
utterer’s agenda.
lshit is useful: on some occa-
t that an utterance was either 
ut be unclear which, given our 
gher-order desires.5 In such a 
.
account, with the positive 
’s intentions, is hard bullshit, 
o describe some of Frankfurt’s 
scal’s conversation with Witt-
ertising agencies. It might be 
tions in the existing literature. 
most closely aligned with Cas-
positive account, for the rea-
ld that some intention must be 
bsent, for the utterance to be 
c insouciance” or vicious atti-
’s view, and (as we have seen) 
r about the utterer’s agenda on 
 section we consider whether 
ullshitter, but it is important 
hat hard bullshit, like the two 
one to take a stance on whether 
and so comes with additional 
aptures only Frankfurt’s nega-
indifference towards truth that 
al of bullshit (general) – for the 
rgue, ChatGPT is at minimum 
t machine, because if it is not 
old any attitudes towards truth 
rs about its (or, perhaps more 
 even this more modest kind 
 deleterious effects that con-
“indifference to the truth is 
e mindlessly frivolous attitude 
g like the distinction between hard 
occurs in Cohen (2002): he suggests 
 a bullshitter as “a person who aims 
nfrequently he hits his target”, or if 
hit: for whatever reason, to produce 
). While we do not adopt Cohen’s 
n his characterisation and our own 
1 3
s reserved.","[425.2114285714285, 80.19428571428567, 783.1542857142856, 898.9371428571427]"
0001_ChatGPT_is_bullshit.pdf,6,body,"ent to whether its utterances
care about the truth of its ou
Presumably ChatGPT can
ing the truth, since it can’t c
a matter of conceptual neces
criteria for bullshit. Howeve
can’t care about anything e
absurd to suggest that this
Similarly books can contain
selves bullshitters. Unlike ro
itself produces text, and loo
independently of its users a
is considerable disagreemen
has intentions, it’s widely a
duces are (typically) meanin
spensable human treasure is squandered” (2002, p343). 
reating ChatGPT and similar LLMs as being in any way 
cerned with truth, or by speaking metaphorically as if 
 make mistakes or suffer “hallucinations” in pursuit of 
 claims, we risk exactly this acceptance of bullshit, and 
squandering of meaning – so, irrespective of whether or 
ChatGPT is a hard or a soft bullshitter, it does produce 
shit, and it does matter.
atGPT is bullshit
h this distinction in hand, we’re now in a position to 
sider a worry of the following sort: Is ChatGPT hard 
shitting, soft bullshitting, or neither? We will argue, first, ","[68.63999999999997, 76.07999999999996, 416.98285714285703, 257.10857142857134]"
0001_ChatGPT_is_bullshit.pdf,6,heading," ChatGPT, and other LLMs, are clearly sof
wever, the question of whether these chat
shitting is a trickier one, and depends on
mplex questions concerning whether Cha","[67.26857142857133, 266.7085714285714, 233.21142857142854, 310.59428571428566]"
0001_ChatGPT_is_bullshit.pdf,6,body,,"[65.8971428571428, 482.0228571428571, 423.84000000000003, 316.08]"
0001_ChatGPT_is_bullshit.pdf,6,heading,,"[59.03999999999991, 525.9085714285715, 296.2971428571429, 490.25142857142856]"
0001_ChatGPT_is_bullshit.pdf,6,body,,"[52.18285714285711, 1026.48, 427.9542857142858, 528.6514285714285]"
0001_ChatGPT_is_bullshit.pdf,6,body,"e true. It does not and cannot 
t.
care about conveying or hid-
e about anything. So, just as 
y, it meets one of Frankfurt’s 
his only gets us so far – a rock 
er, and it would be patently 
eans rocks are bullshitters6. 
llshit, but they are not them-
s – or even books – ChatGPT 
like it performs speech acts 
 designers. And while there 
oncerning whether ChatGPT 
ed that the sentences it pro-
ul (see e.g. Mandelkern and 
convey truth or falsehood 
der of – to use Colbert’s apt 
 statement, and ChatGPT is 
ake attempts at bullshit effi-
ctionaries, etc., are not). So, 
tGPT is a soft bullshitter: if 
ns, there isn’t any attempt to 
ards truth, but it is nonethe-
of outputting utterances that 
 conclude that ChatGPT is a 
ter? A critic might object, it 
k of programs like ChatGPT 
 they are not agents, or relat-
 intend anything whatsoever.
st, whether or not ChatGPT 
rs do. And what they produce 
t. Second, we will argue that, 
ency, it does have a function; 
stic goals, and possibly even 
r definition of hard bullshit.
ld say what we mean when ","[432.0685714285715, 81.56571428571424, 781.7828571428572, 586.2514285714285]"
0001_ChatGPT_is_bullshit.pdf,6,heading,,"[599.3828571428571, 623.28, 438.9257142857142, 604.08]"
0001_ChatGPT_is_bullshit.pdf,6,body," actually is expressing proposi-
hich agents express propositions. 
even see ChatGPT as expressing 
ommunicative intentions, and so 
ngless. Even accepting this, we 
 as expressing propositions. This 
 - has recently been discussed by 
eserved.","[434.81142857142856, 641.1085714285715, 780.4114285714286, 872.8800000000001]"
0001_ChatGPT_is_bullshit.pdf,7,body,"ful one when combined wit
and soft bullshit. Reaching
dodgy student paper: we’v
where it was obvious that a d
deployed with a crushing la
lar words are used not beca
even because they serve to 
because the author wants to 
standing and sophistication
call the dictionary a bullshit
not be inappropriate to call
we should, strictly, say not t
it outputs bullshit in a way t
vector of bullshit: it does no
of its output, and the person
truth or falsehood but rather
text was written by a interes
ChatGPT may be a hard bulls
Is ChatGPT itself a hard b
intentions or goals: it must
not about the content of its
its agenda. Recall that hard 
student or the incompetent
their statements are true or 
ny literal sense, be said to have goals or aims? If so, does 
tend to deceive us about the content of its utterances, or 
ely have the goal to appear to be a competent speaker? 
s it have beliefs—internal representational states which 
to track the truth? If so, do its utterances match those 
efs (in which case its false statements might be some-
g like hallucinations) or are its utterances not matched 
he beliefs—in which case they are likely to be either lies 
ullshit? We will consider these questions in more depth 
elow.
here are other philosophically important aspects of 
nthood that we will not be considering. We won’t be 
sidering whether ChatGPT makes decisions, has or lacks 
nomy, or is conscious; we also won’t worry whether 
tGPT is morally responsible for its statements or its 
ons (if it has any of those).
tGPT is a bullshit machine
will argue that even if ChatGPT is not, itself, a hard 
shitter, it is nonetheless a bullshit machine. The bullshit-
s the person using it, since they (i) don’t care about the 
h of what it says, (ii) want the reader to believe what the 
ication outputs. On Frankfurt’s view, bullshit is bullshit 
n if uttered with no intent to bullshit: if something is 
shit to start with, then its repetition “is bullshit as he [or ","[68.63999999999997, 84.30857142857138, 416.98285714285703, 391.5085714285714]"
0001_ChatGPT_is_bullshit.pdf,7,heading,,"[68.63999999999997, 429.9085714285714, 246.92571428571432, 407.9657142857143]"
0001_ChatGPT_is_bullshit.pdf,7,body,"its agenda. We don’t think 
intentions in precisely the 
Levinstein and Herrmann (
the issues here). But when s
easy to use intentional langu
GPT trying to do? Does it c
is accurate? We will argue
perhaps not literal, sense in
deceive us about its agenda
the content of its utterances
a ‘normal’ interlocutor like
no similarly strong sense i
lies, or hallucinates.
Our case will be simple:
to imitate human speech. If
precisely the sort of intentio
be a hard bullshitter: in perf
attempting to deceive the au
ically, it’s trying to seem lik
when in many cases it does
this function gives rise to, o
tion. In the next section, we
his just pushes the question back to who the origina-
s, though: take the (increasingly frequent) example of 
tudent essay created by ChatGPT. If the student cared 
t accuracy and truth, they would not use a program that 
mously makes up sources whole-cloth. Equally, though, 
ey give it a prompt to produce an essay on philosophy 
ience and it produces a recipe for Bakewell tarts, then 
on’t have the desired effect. So the idea of ChatGPT 
bullshit machine seems right, but also as if it’s miss-
something: someone can produce bullshit using their 
e, a pen or a word processor, after all, but we don’t stan-
ly think of these things as being bullshit machines, or 
utputting bullshit in any particularly interesting way – 
ersely, there does seem to be something particular to 
GPT, to do with the way that it operates, which makes 
ore than a mere tool, and which suggests that it might 
opriately be thought of as an originator of bullshit. In 
, it doesn’t seem quite right either to think of Chat-
 as analogous to a pen (can be used for bullshit, but can 
e nothing without deliberate and wholly agent-directed 
n) nor as to a bullshitting human (who can intend and 
uce bullshit on their own initiative).
Content courtesy of Springer Nature, terms of use apply. Righ","[71.38285714285705, 440.88, 414.23999999999995, 1018.2514285714285]"
0001_ChatGPT_is_bullshit.pdf,7,body,"ain for the example of the 
l, I take it, marked papers 
onary or thesaurus had been 
of subtlety; where fifty-dol-
they’re the best choice, nor 
uscate the truth, but simply 
vey an impression of under-
would be inappropriate to 
st in this case; but it would 
 result bullshit. So perhaps 
ChatGPT is bullshit but that 
goes beyond being simply a 
d cannot care about the truth 
ng it does so not to convey 
convince the hearer that the 
and attentive agent.
er
hitter? If so, it must have 
end to deceive its listener, 
tements, but instead about 
shitters, like the unprepared 
itician, don’t care whether 
e, but do intend to deceive 
re doing. If so, it must have ","[435.49714285714293, 88.63714285714286, 781.3542857142859, 411.3514285714286]"
0001_ChatGPT_is_bullshit.pdf,7,heading,"atements, but instead about 
ChatGPT is an agent or has ","[434.21142857142866, 426.78000000000003, 633.497142857143, 442.2085714285715]"
0001_ChatGPT_is_bullshit.pdf,7,body,"coming) for a discussion of 
king loosely it is remarkably 
 to describe it: what is Chat-
whether the text it produces 
 there is a robust, although 
ich ChatGPT does intend to 
goal is not to convince us of 
t instead to portray itself as 
selves. By contrast, there is 
hich ChatGPT confabulates, 
atGPT’s primary function is 
 function is intentional, it is 
at is required for an agent to 
ng the function, ChatGPT is 
nce about its agenda. Specif-
mething that has an agenda, 
 We’ll discuss here whether 
best thought of, as an inten-
l argue that ChatGPT has no 
1 3
reserved.","[434.21142857142866, 462.78000000000003, 777.497142857143, 1016.9228571428571]"
0001_ChatGPT_is_bullshit.pdf,8,body,"Since this reason for think
ter involves committing to o
on mind and meaning, it 
ply thinking of it as a bull
whether or not the program
an attempt to deceive the h
of the enterprise somewhere
that justifies calling the outp
So, though it’s worth ma
to us that it significantly affe
talk about ChatGPT and bul
out some paper or talk isn’t
ing or covering up the tru
attention to what the truth 
system itself. Minimally, it
given certain controversial a
intentional ascription, it pro
texture of the bullshit is no
either way, ChatGPT is a bu
Bullshit? hallucinations? con
terminology
We have argued that we s
bullshit, rather than “halluc
ances produced by ChatGP
cination” terminology is ina
by Edwards (2023), who 
tion” instead. Why is our p
alternatives?
We object to the term hal
tain misleading implication
they have a non-standard pe
actually perceive some feat
2013), where “perceive” is
such that they do not actua
erty. This term is inappropr
reasons. First, as Edwards (2
cination anthropomorphises
that attributing resulting p
the models may allow creat
faulty outputs instead of ta
puts themselves”, and we m
of responsibility. LLMs do 
not “mis-perceive”. Second
LLM delivering false uttera
ant form of the process it 
claim is the case in hallucin
perception). The very same 
happen to be true.
ow do we know that ChatGPT functions as a hard 
hitter? Programs like ChatGPT are designed to do a 
 and this task is remarkably like what Frankfurt thinks 
ullshitter intends, namely to deceive the reader about 
nature of the enterprise – in this case, to deceive the 
er into thinking that they’re reading something pro-
d by a being with intentions and beliefs.
hatGPT’s text production algorithm was developed and 
d in a process quite similar to artificial selection. Func-
 and selection processes have the same sort of direct-
ss that human intentions do; naturalistic philosophers 
ind have long connected them to the intentionality of 
an and animal mental states. If ChatGPT is understood 
aving intentions or intention-like states in this way, its 
tion is to present itself in a certain way (as a conver-
nal agent or interlocutor) rather than to represent and 
ey facts. In other words, it has the intentions we associ-
with hard bullshitting.
ne way we can think of ChatGPT as having intentions 
 adopting Dennett’s intentional stance towards it. Den-
(1987: 17) describes the intentional stance as a way 
edicting the behaviour of systems whose purpose we 
t already know.
To adopt the intentional stance […] is to decide – ten-
ely, of course – to attempt to characterize, predict, and 
ain […] behavior by using intentional idioms, such as 
eves’ and ‘wants,’ a practice that assumes or presup-
s the rationality” of the target system (Dennett, 1983: 
.
ennett suggests that if we know why a system was 
gned, we can make predictions on the basis of its design 
7). While we do know that ChatGPT was designed 
hat, its exact algorithm and the way it produces its 
onses has been developed by machine learning, so we 
ot know its precise details of how it works and what it 
. Under this ignorance it is tempting to bring in inten-
l descriptions to help us understand and predict what 
GPT is doing.
When we adopt the intentional stance, we will be making 
predictions if we attribute any desire to convey truth 
hatGPT. Similarly, attributing “hallucinations” to Chat-
 will lead us to predict as if it has perceived things that 
t there, when what it is doing is much more akin to 
ng something up because it sounds about right. The 
er intentional attribution will lead us to try to correct 
eliefs, and fix its inputs --- a strategy which has had 
ed if any success. On the other hand, if we attribute 
hatGPT the intentions of a hard bullshitter, we will be 
r able to diagnose the situations in which it will make 
3
Content courtesy of Springer Nature, terms of use apply. Righ","[71.6400000000001, 89.92285714285714, 416.21142857142866, 1000.2085714285714]"
0001_ChatGPT_is_bullshit.pdf,8,body," ChatGPT is a hard bullshit-
or more controversial views 
more tendentious than sim-
machine; but regardless of 
 intentions, there clearly is 
r or reader about the nature 
ng the line, and in our view 
ard bullshit.
 the caveat, it doesn’t seem 
how we should think of and 
t: the person using it to turn 
ncerned either with convey-
ince both of those require 
ually is), and neither is the 
urns out soft bullshit, and, 
mptions about the nature of 
es hard bullshit; the specific 
or our purposes, important: 
itter.
ulations? The need for new 
ld use the terminology of 
ions” to describe the utter-
The suggestion that “hallu-
opriate has also been noted 
ours the term “confabula-
sal better than this or other ","[435.49714285714293, 88.63714285714286, 780.0685714285715, 461.4942857142857]"
0001_ChatGPT_is_bullshit.pdf,8,heading,"When someone hallucinates 
ptual experience, but do not 
of the world (Macpherson, 
derstood as a success term, ","[435.49714285714293, 482.06571428571425, 771.0685714285715, 515.4942857142858]"
0001_ChatGPT_is_bullshit.pdf,8,body,"erceive the object or prop-
for LLMs for a variety of 
) points out, the term hallu-
LLMs. Edwards also notes 
ms to “hallucinations” of 
o “blame the AI model for 
responsibility for the out-
e wary of such abdications 
perceive, so they surely do 
at occurs in the case of an 
 is not an unusual or devi-
lly goes through (as some 
s, e.g., disjunctivists about 
ess occurs when its outputs 
eserved.","[438.0685714285715, 534.78, 778.7828571428572, 1014.3514285714285]"
0001_ChatGPT_is_bullshit.pdf,9,body,"Calling chatbot inaccur
to overblown hype about th
cheerleaders, and could lea
among the general public. I
inaccuracy problems which
to misguided efforts at AI 
It can also lead to the wron
when it gets things right: t
bullshitting, even when it’s 
‘bullshit’ rather than ‘hallu
rate (as we’ve argued); it’s
communication in an area th
Acknowledgements Thanks to N
Tanswell, and the University of
reading group for helpful discuss
Open Access  This article is l
Attribution 4.0 International L
adaptation, distribution and repr
as long as you give appropriate c
source, provide a link to the Cre
if changes were made. The imag
article are included in the article
indicated otherwise in a credit l
included in the article’s Creative
use is not permitted by statutory
use, you will need to obtain pe
holder. To view a copy of this l
org/licenses/by/4.0/.
References
Alkaissi, H., & McFarlane, S. I.,
cinations in ChatGPT: Impl
15(2), e35179. https://doi.or
Bacin, S. (2021). My duties and
and the good example in F
S. Bacin, & O. Ware (Eds.)
guide. Cambridge Universit
Cassam, Q. (2019). Vices of the m
Cohen, G. A. (2002). Deeper in
(Eds.), The contours of Ag
Frankfurt. MIT Press.
Davis, E., & Aaronson, S. (2023
and code interpreter plub-
Arxiv Preprint: arXiv, 2308
Dennett, D. C. (1983). Intentiona
panglossian paradigm defen
6, 343–390.
Dennett, D. C. (1987). The inten
Dennis Whitcomb (2023). B
299–304.
Easwaran, K. (2023). Bullshit 
1–23. https://doi.org/10.111
Edwards, 
B. 
(2023). 
Why 
so good at making t
arstechnica.com/infor
n human psychology, a “confabulation” occurs when 
omeone’s memory has a gap and the brain convinc-
gly fills in the rest without intending to deceive oth-
s. ChatGPT does not work like the human brain, but 
e term “confabulation” arguably serves as a better 
etaphor because there’s a creative gap-filling prin-
ple at work […].
Edwards notes, this is imperfect. Once again, the use of 
man psychological term risks anthropomorphising the 
Ms.
his term also suggests that there is something excep-
al occurring when the LLM makes a false utterance, 
that in these occasions - and only these occasions - it 
s in” a gap in memory with something false. This too is 
eading. Even when the ChatGPT does give us correct 
wers, its process is one of predicting the next token. In 
view, it falsely indicates that ChatGPT is, in general, 
mpting to convey accurate information in its utterances. 
there are strong reasons to think that it does not have 
efs that it is intending to share in general–see, for exam-
Levinstein and Herrmann (forthcoming). In our view, it 
ly indicates that ChatGPT is, in general, attempting to 
vey accurate information in its utterances. Where it does 
k truth, it does so indirectly, and incidentally.
his is why we favour characterising ChatGPT as a 
shit machine. This terminology avoids the implications 
perceiving or remembering is going on in the workings 
he LLM. We can also describe it as bullshitting when-
 it produces outputs. Like the human bullshitter, some 
e outputs will likely be true, while others not. And as 
 the human bullshitter, we should be wary of relying 
n any of these outputs.
nclusion
stors, policymakers, and members of the general public 
e decisions on how to treat these machines and how to 
t to them based not on a deep technical understanding of 
 they work, but on the often metaphorical way in which 
 abilities and function are communicated. Calling their 
akes ‘hallucinations’ isn’t harmless: it lends itself to 
confusion that the machines are in some way misper-
ng but are nonetheless trying to convey something that 
 believe or have perceived. This, as we’ve argued, is the 
ng metaphor. The machines are not trying to communi-
something they believe or perceive. Their inaccuracy 
ot due to misperception or hallucination. As we have ","[70.35428571428581, 88.63714285714286, 416.21142857142866, 731.4942857142858]"
0001_ChatGPT_is_bullshit.pdf,9,heading,Content ,"[71.6400000000001, 763.6371428571429, 160.35428571428582, 785.4942857142858]"
0001_ChatGPT_is_bullshit.pdf,9,body,,"[70.35428571428581, 802.2085714285714, 414.9257142857144, 1016.9228571428571]"
0001_ChatGPT_is_bullshit.pdf,9,body,"s ‘hallucinations’ feeds in 
abilities among technology 
unnecessary consternation 
o suggests solutions to the 
ht not work, and could lead 
nment amongst specialists. 
titude towards the machine 
naccuracies show that it is 
. Calling these inaccuracies 
ions’ isn’t just more accu-
od science and technology 
orely needs it.
McDonnell, Bryan Pickel, Fenner 
sgow’s Large Language Model 
nd comments.
ed under a Creative Commons 
e, which permits use, sharing, 
tion in any medium or format, 
to the original author(s) and the 
Commons licence, and indicate ","[436.78285714285727, 89.92285714285714, 776.2114285714287, 338.06571428571425]"
0001_ChatGPT_is_bullshit.pdf,9,heading," stance. The MIT.
t questions. Analysis, ","[436.78285714285727, 633.78, 521.6400000000001, 651.78]"
0001_ChatGPT_is_bullshit.pdf,9,references,"ities. Analytic Philosophy, 00, 
b.12328.
atGPT 
and 
bing 
chat 
are 
 up. Ars Tecnica. https://
ion-technology/2023/04/
1 3
eserved.","[436.78285714285727, 672.3514285714285, 778.7828571428572, 1027.2085714285713]"
0001_ChatGPT_is_bullshit.pdf,10,references,"why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-
hope-to-fix-them/, accesssed 19th April, 2024.
Frankfurt, H. (2002). Reply to cohen. In S. Buss, & L. Overton (Eds.),
The contours of agency: Essays on themes from Harry Frankfurt.
MIT Press.
Frankfurt, H. (2005). On Bullshit, Princeton.
Knight, W. (2023). Some glimpse AGI in ChatGPT. others call it a
mirage. Wired, August 18 2023, accessed via https://www.wired.
com/story/chatgpt-agi-intelligence/.
Levinstein, B. A., & Herrmann, D. A. (forthcoming). Still no lie detec-
tor for language models: Probing empirical and conceptual road-
blocks. Philosophical Studies, 1–27.
Levy, N. (2023). Philosophy, Bullshit, and peer review. Cambridge
University.
Lightman, H., et al. (2023). Let’s verify step by step. Arxiv Preprint:
arXiv, 2305, 20050.
Lysandrou (2023). Comparative analysis of drug-GPT and ChatGPT
LLMs for healthcare insights: Evaluating accuracy and rele-
vance in patient and HCP contexts. ArXiv Preprint: arXiv, 2307,
16850v1.
Macpherson, F. (2013). The philosophy and psychology of hallucina-
tion: an introduction, in Hallucination, Macpherson and Platchias
(Eds.), London: MIT Press.
Mahon, J. E. (2015). The definition of lying and deception. The Stan-
ford Encyclopedia of Philosophy (Winter 2016 Edition), Edward
N. Zalta (Ed.), https://plato.stanford.edu/archives/win2016/
entries/lying-definition/.
Mallory, F. (2023). Fictionalism about chatbots. Ergo, 10(38),
1082–1100.
Mandelkern, M., & Linzen, T. (2023). Do language models’ Words
Refer?. ArXiv Preprint: arXiv, 2308, 05576.","[49.91701863354038, 57.84596273291926, 288.7244720496894, 366.41739130434786]"
0001_ChatGPT_is_bullshit.pdf,10,references,"OpenAI (2023). GPT-4 technical report. ArXiv Preprint: arXiv, 2303, 
08774v3.
Proops, I., & Sorensen, R. (2023). Destigmatizing the exegetical attri-
bution of lies: the case of kant. Pacific Philosophical Quarterly. 
https://doi.org/10.1111/papq.12442.
Sarkar, A. (2023). ChatGPT 5 is on track to attain artificial gen-
eral intelligence. The Statesman, April 12, 2023. Accesses via 
https://www.thestatesman.com/supplements/science_supple-
ments/chatgpt-5-is-on-track-to-attain-artificial-general-intel-
ligence-1503171366.html.
Shah, C., & Bender, E. M. (2022). Situating search. CHIIR ‘22: Pro-
ceedings of the 2022 Conference on Human Information Inter-
action and Retrieval March 2022 Pages 221–232 https://doi.
org/10.1145/3498366.3505816.
Weise, K., & Metz, C. (2023). When AI chatbots hallucinate. New 
York Times, May 9, 2023. Accessed via https://www.nytimes.
com/2023/05/01/business/ai-chatbots-hallucination.html.
Weiser, B. (2023). Here’s what happens when your lawyer uses Chat-
GPT. New York Times, May 23, 2023. Accessed via https://www.
nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chat-
gpt.html.
Zhang (2023). How language model hallucinations can snowball. 
ArXiv preprint: arXiv:, 2305, 13534v1.
Zhu, T., et al. (2023). Large language models for information retrieval: 
A survey. Arxiv Preprint: arXiv, 2308, 17107v2.","[305.71826086956514, 56.95155279503106, 544.5257142857143, 306.49192546583845]"
0002_jit.2015.5.pdf,1,title,"Big other: surveillance capitalism and the
prospects of an information civilization","[41.86732919254653, 84.32944099378884, 520.3766459627329, 138.88844720496894]"
0002_jit.2015.5.pdf,1,author,Shoshana Zuboff1,"[41.86732919254653, 150.51577639751554, 128.6250931677019, 163.93192546583853]"
0002_jit.2015.5.pdf,1,abstract,"This article describes an emergent logic of accumulation in the networked sphere,
‘surveillance capitalism,’ and considers its implications for ‘information civilization.’ The
institutionalizing practices and operational assumptions of Google Inc. are the primary lens
for this analysis as they are rendered in two recent articles authored by Google Chief
Economist Hal Varian. Varian asserts four uses that follow from computer-mediated
transactions: ‘data extraction and analysis,’ ‘new contractual forms due to better monitor-
ing,’ ‘personalization and customization,’ and ‘continuous experiments.’ An examination of
the nature and consequences of these uses sheds light on the implicit logic of surveillance
capitalism and the global architecture of computer mediation upon which it depends. This
architecture produces a distributed and largely uncontested new expression of power that I
christen: ‘Big Other.’ It is constituted by unexpected and often illegible mechanisms of
extraction, commodification, and control that effectively exile persons from their own
behavior while producing new markets of behavioral prediction and modification. Surveil-
lance capitalism challenges democratic norms and departs in key ways from the centuries-
long evolution of market capitalism.","[41.86732919254653, 273.04993788819877, 426.46360248447206, 449.24869565217386]"
0002_jit.2015.5.pdf,1,heading,"Introduction
A","[40.972919254658336, 539.5840993788819, 90.16546583850933, 548.5281987577639]"
0002_jit.2015.5.pdf,1,body,"A
recent White House report on ‘big data’ concludes, ‘The
technological trajectory, however, is clear: more and
more data will be generated about individuals and will
persist under the control of others’ (White House, 2014: 9).
Reading this statement brought to mind a 2009 interview with
Google Chairperson Eric Schmidt when the public first
discovered that Google retained individual search histories
that were also made available to state security and law
enforcement agencies, ‘If you have something that you don’t
want anyone to know, maybe you shouldn’t be doing it in the
first place, but if you really need that kind of privacy, the
reality is that search engines including Google do retain this
information for some time … It is possible that that informa-
tion could be made available to the authorities’ (Newman,
2009). What these two statements share is the attribution of
agency to ‘technology.’ ‘Big data’ is cast as the inevitable
consequence of a technological juggernaut with a life of its
own entirely outside the social. We are but bystanders.","[42.76173913043473, 550.3170186335403, 288.7244720496894, 746.1927950310559]"
0002_jit.2015.5.pdf,1,body,"Most articles on the subject of ‘big data’ commence with
an effort to define ‘it.’ This suggests to me that a reasonable
definition has not yet been achieved. My argument here is
that we have not yet successfully defined ‘big data’ because we
continue to view it as a technological object, effect or
capability. The inadequacy of this view forces us to return
over and again to the same ground. In this article I take a
different approach. ‘Big data,’ I argue, is not a technology or an
inevitable technology effect. It is not an autonomous process,
as Schmidt and others would have us think. It originates in the
social, and it is there that we must find it and know it. In this
article I explore the proposition that ‘big data’ is above all the
foundational component in a deeply intentional and highly
consequential new logic of accumulation that I call surveil-
lance capitalism. This new form of information capitalism
aims to predict and modify human behavior as a means to
produce revenue and market control. Surveillance capitalism
has gradually constituted itself during the last decade, embodying","[305.71826086956514, 549.4226086956522, 552.5754037267079, 747.087204968944]"
0002_jit.2015.5.pdf,2,body,"a new social relations and politics that have not yet been well
delineated or theorized. While ‘big data’ may be set to other
uses, those do not erase its origins in an extractive project
founded on formal indifference to the populations that
comprise both its data sources and its ultimate targets.
Constantiou and Kallinikos (2014) provide important clues
to this new direction in their article ‘New games, new rules: big
data and the changing context of strategy,’ as they lift the veil on
the black box, that is ‘big data,’ to reveal its epistemic contents
and their indigenous problematics. ‘New games’ is a powerful
and necessary contribution to this opaque intellectual territory.
The article builds on earlier warnings (e.g. boyd and Crawford,
2011; Bhimani and Willcocks, 2014) to sharply delineate the
epistemic features of ‘big data’ – heterogeneous, unstructured,
trans-semiotic, decontextualized, agnostic – and to illuminate
the epistemological discontinuities such data entail for the
methods and mindsets of corporate strategy’s formal, deductive,
inward-focused, and positivistic conventions.
In
claiming this
black
box
for the known
world,
Constantiou and Kallinikos (2014) also insist on the mys-
teries that remain unsolved. ‘Big data,’ they warn, heralds
‘a transformation of contemporary economy and society … a
much wider shift that makes everydayness qua data imprints
an intrinsic component of organizational and institutional
life … and also a primary target of commercialization
strategies …’ Such changes, they say, concern ‘the blurring
of long-established social and institutional divisions … the
very nature of firms and organizations and their relations to
individuals qua users, customers or clients, and citizens.’
These challenges also ‘recast management … as a field and
social practice in a new context whose exact outlines still
remain unclear … (10).’
In this brief article, I aim to contribute to a new discussion
on these still untheorized new territories in which the roiling
ephemera of Constantiou’s and Kallinikos’s ‘big data’ are
embedded: the migration of everydayness as a commercializa-
tion strategy; the blurring of divisions; the nature of the firm
and its relation to populations. In preparation for the argu-
ments I want to make here, I begin with a very brief review of a
few foundational concepts. I then move on to a close examina-
tion of two articles by Google Chief Economist Hal Varian that
disclose the logic and implications of surveillance capitalism as
well as ‘big data’s’ foundational role in this new regime.","[41.86732919254653, 70.91329192546586, 289.6188819875776, 538.6896894409938]"
0002_jit.2015.5.pdf,2,heading,"Computer mediation meets the logic of accumulation
 fi","[41.86732919254653, 561.9443478260869, 252.9480745341615, 571.7828571428571]"
0002_jit.2015.5.pdf,2,body,"Nearly 35 years ago I first developed the notion of ‘computer
mediation’ in an MIT Working Paper called ‘The Psycholo-
gical
and
Organizational
Implications
of
Computer-
Mediated Work’ (Zuboff 1981; see also Zuboff, 2013 for a
history of this concept and its meaning). In that paper and
subsequent writing I distinguished ‘computer-mediated’
work from earlier generations of mechanization and auto-
mation designed to substitute for or simplify human labor
(e.g. Zuboff, 1988, 1985, 1982). I observed that information
technology is characterized by a fundamental duality that
had not yet been fully appreciated. It can be applied to
automate operations according to a logic that hardly differs
from that of centuries past: replace the human body with
machines that enable more continuity and control. But when
it comes to information technology, automation simulta-
neously generates information that provides a deeper level of","[41.86732919254653, 572.6772670807453, 289.6188819875776, 748.8760248447204]"
0002_jit.2015.5.pdf,2,body,"transparency to activities that had been either partially or
completely opaque. It not only imposes information (in the
form of programmed instructions), but it also produces
information. The action of a machine is entirely invested in
its object, but information technology also reflects back on its
activities and on the system of activities to which it is related.
This produces action linked to a reflexive voice, as computer-
mediation symbolically renders events, objects, and pro-
cesses that become visible, knowable, and shareable in a new
way. This distinction, to put it simply, marks the difference
between ‘smart’ and ‘dumb.’
The word I coined to describe this unique capacity is
informate. Information technology alone has the capacity to
automate and to informate. As a result of the informating
process, computer-mediated work extends organizational
codification resulting in a comprehensive ‘textualization’ of
the work environment – what I called ‘the electronic text.’
That text created new opportunities for learning and therefore
new contests over who would learn, how, and what. Once a
firm is imbued with computer mediation, this new ‘division of
learning’ becomes more salient than the traditional division of
labor. Even at the early stages of these developments in the
1980s, the text was somewhat heterogeneous. It reflected
production flows and administrative processes along with
customer interfaces, but it also revealed human behavior:
phone calls, keystrokes, bathroom breaks and other signals of
attentional continuity, actions, locations, conversations, net-
works, specific engagements with people and equipment, and
so forth. I recall writing the words in the summer of 1985 that
appeared in the final chapter of In the Age of the Smart
Machine. They were regarded as outlandish then. ‘Science
fiction,’ some said; ‘subversive,’ others complained: ‘The
informated workplace, which may no longer be a “place” at
all, is an arena through which information circulates, informa-
tion to which intellective effort is applied. The quality, rather
than the quantity, of effort will be the source from which
added value is derived … learning is the new form of labor’
(Zuboff, 1988: 395).
Today we must strain to imagine when these conditions –
computer mediation, textualization, learning as labor – were
not the case, at least for broad sectors of the labor force. Real-
time
information-based
computer-mediated
learning
has
become so endogenous to everyday business activities that the
two domains are more or less conflated. This is what most of us
do now as work. These new facts are institutionalized in
thousands, if not millions, of new species of action within firms.
Some of these are more formal: continuous improvement
methodologies, enterprise integration, employee monitoring,
ICT systems that enable the global coordination of distributed
manufacturing operations, professional activities, teams, custo-
mers, supply chains, inter-firm projects, mobile and temporary
workforces, and marketing approaches to diverse configura-
tions of consumers. Some are less formal: the unceasing flow of
email, online search, smartphone activities, apps, texts, video
meetings, social media interactions, and so forth.
The division of learning, however, is no pure form. During
20 years of fieldwork, I encountered the same lesson in
hundreds of variations. The division of learning, like the
division of labor, is always shaped by contests over these
questions: Who participates and how? Who decides who
participates? What happens when authority fails? In the
market sphere, the electronic text and what can be learned","[304.823850931677, 70.01888198757767, 554.3642236024846, 748.8760248447204]"
0002_jit.2015.5.pdf,11,body,"‘world-spanning living organism’ from ‘a God’s eye view’
(Pentland, 2009: 76, 80). This is yet another rendering of the
‘extended order,’ fully explicated by computer-mediation. The
electronic text of the informated workplace has morphed into a
world-spanning living organism – an inter-operational, beha-
vior-modifying, market-making, and proprietary God view.
Nearly 70 years ago historian Karl Polanyi observed that the
market economies of the 19th and 20th centuries depended
upon three astonishing mental inventions that he called
‘fictions.’ The first was that human life can be subordinated
to market dynamics and be reborn as ‘labor.’ Second, nature
can be subordinated and reborn as ‘real estate.’ Third, that
exchange can be reborn as ‘money.’ The very possibility of
industrial capitalism depended upon the creation of these
three
critical
‘fictional
commodities.’
Life,
nature,
and
exchange were transformed into things, that they might be
profitably bought and sold. ‘[T]he commodity fiction,’ he
wrote, ‘disregarded the fact that leaving the fate of soil and
people to the market would be tantamount to annihilating
them.’
With the new logic of accumulation that is surveillance
capitalism, a fourth fictional commodity emerges as a domi-
nant characteristic of market dynamics in the 21st century.
Reality itself is undergoing the same kind of fictional meta-
morphosis as did persons, nature, and exchange. Now ‘reality’
is subjugated to commodification and monetization and
reborn as ‘behavior.’ Data about the behaviors of bodies,
minds, and things take their place in a universal real-time
dynamic index of smart objects within an infinite global
domain of wired things. This new phenomenon produces the
possibility of modifying the behaviors of persons and things
for profit and control. In the logic of surveillance capitalism
there are no individuals, only the world-spanning organism
and all the tiniest elements within it.","[40.972919254658336, 68.23006211180126, 289.6188819875776, 440.30459627329185]"
0002_jit.2015.5.pdf,11,heading,Conclusion,"[42.76173913043473, 462.66484472049683, 85.69341614906827, 473.39776397515527]"
0002_jit.2015.5.pdf,11,body,"Technologies are constituted by unique affordances, but the
development and expression of those affordances are shaped by
the institutional logics in which technologies are designed,
implemented, and used. This is, after all, the origin of the hack.
Hacking intends to liberate affordances from the institutional
logics in which they are frozen and redistribute them in
alternative configurations for new purposes. In the market
sphere, these circumscribing logics are logics of accumulation.
With this view in mind, my aim has been to begin to identify and
theorize the currently institutionalizing logic of accumulation
that produces hyperscale assemblages of objective and subjective
data about individuals and their habitats for the purposes of
knowing, controlling, and modifying behavior to produce new
varieties of commodification, monetization, and control.
The development of the Internet and methods to access the
World Wide Web spread computer mediation from bounded
sites of work and specialized action to global ubiquity both at
the institutional interface and in the intimate spheres of
everyday experience. High tech firms, led by Google, perceived
new profit opportunities in these facts. Google understood
that were it to capture more of these data, store them, and
analyze them, they could substantially affect the value of
advertising. As Google’s capabilities in this arena developed
and attracted historic levels of profit, it produced successively
ambitious practices that expand the data lens from past virtual","[41.86732919254653, 474.2921739130434, 289.6188819875776, 748.8760248447204]"
0002_jit.2015.5.pdf,11,body,"behavior to current and future actual behavior. New moneti-
zation opportunities are thus associated with a new global
architecture of data capture and analysis that produces
rewards and punishments aimed at modifying and commodi-
tizing behavior for profit.
Many of the practices associated with capitalizing on these
newly perceived opportunities challenged social norms asso-
ciated with privacy and are contested as violations of rights
and laws. In result, Google and other actors learned to obscure
their operations, choosing to invade undefended individual
and social territory until opposition is encountered, at which
point they can use their substantial resources to defend at low
cost what had already been taken. In this way, surveillance
assets are accumulated and attract significant surveillance
capital while producing their own surprising new politics and
social relations.
These new institutional facts have been allowed to stand for
a variety of reasons: they were constructed at high velocity and
designed to be undetectable. Outside a narrow realm of
experts, few people understood their meaning. Structural
asymmetries of knowledge and rights made it impossible for
people to learn about these practices. Leading tech companies
were respected and treated as emissaries of the future. Nothing
in past experience prepared people for these new practices,
and so there were few defensive barriers for protection.
Individuals quickly came to depend upon the new information
and communication tools as necessary resources in the
increasingly stressful, competitive, and stratified struggle for
effective life. The new tools, networks, apps, platforms, and
media thus became requirements for social participation.
Finally, the rapid buildup of institutionalized facts – data
brokerage, data analytics, data mining, professional specializa-
tions, unimaginable cash flows, powerful network effects, state
collaboration, hyperscale material assets, and unprecedented
concentrations of information power – produced an over-
whelming sense of inevitability.
These developments became the basis for a fully institutio-
nalized new logic of accumulation that I have called surveil-
lance capitalism. In this new regime, a global architecture of
computer mediation turns the electronic text of the bounded
organization into an intelligent world-spanning organism that
I call Big Other. New possibilities of subjugation are produced
as this innovative institutional logic thrives on unexpected and
illegible mechanisms of extraction and control that exile
persons from their own behavior.
Under these conditions, the division of learning and its
contests are civilizational in scope. To the question ‘who
participates?’ the answer is – those with the material, knowl-
edge, and financial resources to access Big Other. To the
question ‘who decides?’ the answer is, access to Big Other is
decided by new markets in the commodification of behavior:
markets in behavioral control. These are composed of those
who sell opportunities to influence behavior for profit and
those who purchase such opportunities. Thus Google, for
example, may sell access to an insurance company, and this
company purchases the right to intervene in an information
loop in your car or your kitchen in order to increase its
revenues or reduce its costs. It may shut off your car, because
you are driving too fast. It may lock your fridge when you put
yourself at risk of heart disease or diabetes by eating too much
ice cream. You might then face the prospect of either higher
premiums or loss of coverage. Google’s Chief Economist Hal","[305.71826086956514, 69.12447204968946, 553.4698136645962, 749.7704347826086]"
0002_jit.2015.5.pdf,12,body,"Varian celebrates such possibilities as new forms of contract,
when in fact they represent the end of contracts. Google’s
rendering of information civilization replaces the rule of law
and the necessity of social trust as the basis for human
communities with a new life-world of rewards and punish-
ments, stimulus and response. Surveillance capitalism offers a
new regime of comprehensive facts and compliance with facts.
It is, I have suggested, a coup from above – the installation of a
new kind of sovereign power.
The automated ubiquitous architecture of Big Other, its
derivation in surveillance assets, and its function as pervasive
surveillance, highlights other surprising new features of this
logic of accumulation. It undermines the historical relationship
between markets and democracies, as it structures the firm as
formally indifferent to and radically distant from its populations.
Surveillance capitalism is immune to the traditional reciprocities
in which populations and capitalists needed one another for
employment and consumption. In this new model, populations
are targets of data extraction. This radical disembedding from
the social is another aspect of surveillance capitalism’s anti-
democratic character. Under surveillance capitalism, democracy
no longer functions as a means to prosperity; democracy
threatens surveillance revenues.
Will surveillance capitalism be the hegemonic logic of
accumulation in our time, or will it be an evolutionary dead-
end that cedes to other emerging information-based market
forms? What alternative trajectories to the future might be
associated with these competing forms? I suggest that the
prospects of information civilization rest on the answers to
these questions. There are many dimensions of surveillance
capitalism that require careful analysis and theorization if
we are to reckon with these prospects. One obvious dimen-
sion is the imbrication of public and private authority in the
surveillance project. Since Edward Snowden, we have
learned of the blurring of public and private boundaries in
surveillance activities including collaborations and con-
structive interdependencies between state security authori-
ties and high tech firms. Another key set of issues involves
the relationship of surveillance capitalism – and its potential
competitors – to overarching global concerns such as
equality and climate disruptions that effect all our future
prospects. A third issue concerns the velocity of social
evolution compared to that at which the surveillance project
is institutionalized. It seems clear that the waves of lawsuits
breaking on the shores of the new surveillance fortress are
unlikely to alter the behavior of surveillance capitalists.
Were surveillance capitalists to abandon their contested
practices according to the demands of aggrieved parties, the
very logic of accumulation responsible for their rapid rise to
immense wealth and historic concentrations of power
would be undermined. The value of the steady flow of legal
actions is rather to establish new precedents and ultimately
new laws. The question is whether the lag in social evolution
can be remedied before the full consequences of the
surveillance project take hold.
Finally, and most important for all scholars and citizens, is
the fact that we are at the very beginning of the narrative that
will carry us toward new answers. The trajectory of this
narrative depends in no small measure on the scholars drawn
to this frontier project and the citizens who act in the knowl-
edge that deception-induced ignorance is no social contract,
and freedom from uncertainty is no freedom.","[41.86732919254653, 70.01888198757767, 290.51329192546575, 748.8760248447204]"
0002_jit.2015.5.pdf,12,heading,References,"[304.823850931677, 670.1679503105589, 350.4387577639751, 681.7952795031056]"
0002_jit.2015.5.pdf,12,references,"Acemoglu, D. and Robinson, J.A. (2012). Why Nations Fail: The origins of power,
prosperity, and poverty, New York, NY: Crown Business.
Anderson, N. (2010). Why Google keeps your data forever, tracks you with ads,
ArsTechnica.8 March [WWW document] http://arstechnica.com/tech-policy/
news/2010/03/google-keeps-your-data-to-learn-from-good-guys-fight-off-bad-
guys.ars(accessed 21 November 2014).","[303.9294409937888, 687.1617391304347, 551.6809937888198, 744.4039751552795]"
0002_jit.2015.5.pdf,13,references,"Angwin, J. (2012). Google faces new privacy probes, Wall Street Journal. 16 March
[WWW document] http://online.wsj.com/articles/SB100014240527023046
92804577283821586827892 (accessed 21 November 2014).
Angwin, J. (2014). Dragnet Nation: A quest for privacy, security, and freedom in a
world of relentless surveillance, New York: Times Books.
Arendt, H. (1998). The Human Condition, Chicago, IL: University of Chicago Press.
Assange, J. (2014). When Google Met WikiLeaks, New York, NY: OR Books.
Auletta, K. (2009). Googled: The end of the world as we know it, New York, NY:
Penguin Books.
Barker, A. and Fontanella-Khan, J. (2014). Google feels political wind shift against
it in Europe, Financial Times. 21 May [WWW document] http://www.ft.com/
intl/cms/s/2/7848572e-e0c1-11e3-a934-00144feabdc0.html#axzz3JjXPNno5
(accessed 21 November 2014).
BBC (2010). Internet access ‘a human right’, BBC News. 8 March [WWW
document] http://news.bbc.co.uk/2/hi/8548190.stm.
BBC News (2014). Wearables tracked with Raspberry Pi. 1 August [WWW
document] http://www.bbc.com/news/technology-28602997 (accessed 22
November 2014).
Benkler, Y. (2006). The Wealth of Networks: How social production transforms
markets and freedom, New Haven, CT: Yale University Press.
Berle, A.A. and Means, G.C. (1991). The Modern Corporation and Private
Property, New Brunswick, NJ: Transaction Publishers.
Bhimani, A. and Willcocks, L. (2014). Digitisation, ‘Big Data’ and the
Transformation of Accounting Information, Accounting and Business Research
44(4): 469–490.
Bond, R.M., Fariss, C.J., Jones, J.J., Kramer, A.D.I., Marlow, C., Settle, J.E. and
Fowler, J.H. (2012). A 61-million-person Experiment in Social Influence and
Political Mobilization, Nature 482(Sept 13): 295.
boyd, danah and Crawford, K. (2011). Six provocations for big data. Presented at
the A Decade in Internet Time: Symposium on the Dynamics of the Internet and
Society, Oxford Internet Institute. [WWW document] http://www.ssrn.com/
abstract=1926431.
Bradshaw, T. (2014a). Google bets on ‘internet of things’ with $3.2bn Nest deal,
Financial Times. , 13 January [WWW document] http://www.ft.com/intl/cms/s/
0/90b8714a-7c99-11e3-b514-00144feabdc0.html#axzz3hbfec0he (accessed 22
November 2014).
Bradshaw, T. (2014b). Google buys UK artificial intelligence start-up, Financial
Times. 27 January [WWW document] http://www.ft.com/intl/cms/s/0/
f92123b2-8702-11e3-aa31-00144feab7de.html#axzz3HBfEc0HE (accessed 22
November 2014).
Braudel, F. (1984). The Perspective of the World, New York, NY: Harper & Row.
Brewster, T. (2014). Traffic lights, fridges and how they’ve all got it in for us,
Register. 23 June [WWW document] http://www.theregister.co.uk/2014/06/23/
hold_interthreat/ (accessed 22 November 2014).
Burdon, M. and McKillop, A. (2013). The Google Street View Wi-Fi Scandal and
Its Repercussions for Privacy Regulation (Research Paper No. 14-07), University
of Queensland TC Beime School of Law. [WWW document] http://papers.ssrn.
com/sol3/papers.cfm?abstract_id=2471316.
Calo, R. (2014). Digital Market Manipulation, George Washington Law Review
82(4): 995–1051.
Chandler, Jr A.D. (1977). The Visible Hand: The Managerial Revolution in
American Business, Cambridge, MA: Belknap Press.
Cisco (2013a). Embracing the internet of everything to capture your share of $14.4
trillion, Cisco Systems, Inc. [WWW document] http://www.cisco.com/web/
about/ac79/docs/innov/IoE_Economy.pdf (accessed 9 June 2014).
Cisco (2013b). The internet of everything: global private sector economic analysis,
Cisco Systems, Inc. [WWW document] http://www.cisco.com/web/about/ac79/
docs/innov/IoE_Economy_FAQ.pdf (accessed 22 November 2014).
CNIL (2014, September 25). Google privacy policy: WP29 proposes a compliance
package, Commission Nationale de L’informatique et Des Libertés.
[WWW document] http://www.cnil.fr/english/news-and-events/news/article/
google-privacy-policy-wp29-proposes-a-compliance-package/ (accessed 21
November 2014).
Cohen, L. (2003). A Consumers’ Republic: The politics of mass consumption in
postwar America, New York, NY: Knopf.
Constantiou, I.D. and Kallinikos, J. (2014). New Games, New Rules: Big data and
the changing context of strategy, Journal of Information Technology, advance
online publication 9 September, doi: 10.1057/jit.2014.17.
Davis, G. (2011). The Twilight of the Berle and Means Corporation, Seattle
University Law Review 34(4): 1121–1138.
Davis, G. (2013). After the Corporation, Politics & Society 41(2): 283–308.","[40.972919254658336, 70.91329192546586, 290.51329192546575, 742.6151552795031]"
0002_jit.2015.5.pdf,13,references,"Dean, J. (2009). Challenges in building large-scale information retrieval systems,
Google Fellow Presentation. [WWW document] http://static.googleusercontent.
com/media/research.google.com/en/us/people/jeff/WSDM09-keynote.pdf
(accessed 22 November 2014).
Dean, J. and Ghemawat, S. (2008). MapReduce: Simplified data processing on
large clusters, Communications of the ACM 51(1): 107.
Döpfner, M. (2014). Why we fear Google, Frankfurter Allgemeine Zeitung.
[WWW document] http://www.faz.net/aktuell/feuilleton/debatten/mathias-
doepfner-s-open-letter-to-eric-schmidt-12900860.html (accessed 17 April
2014).
Doyle, J. (2013, November 15). Google facing legal action in EVERY EU country
over ‘data goldmine’ collected about users, Daily Mail Online. [WWW
document] http://www.dailymail.co.uk/sciencetech/article-2302870/Google-
facing-legal-action-EVERY-EU-country-data-goldmine-collected-users.html
(accessed 21 November 2014).
Durkheim, E. (1964). The Division of Labor in Society, New York, NY: Free Press.
Dwoskin, E. (2014). What secrets your phone is sharing about you, Wall Street
Journal. 14 January [WWW document] http://online.wsj.com/articles/
SB10001424052702303453004579290632128929194.
Economist (2014). The new GE: Google, everywhere. 18 January [WWW
document] http://www.economist.com/news/business/21594259-string-deals-
internet-giant-has-positioned-itself-become-big-inventor-and.
EPIC (2014a). Google glass and privacy, Electronic Privacy Information Center.
[WWW document] https://epic.org/privacy/google/glass/ (accessed 15
November 2014).
EPIC (2014b). Investigations of Google Street View, Electronic Privacy Information
Center. [WWW document] https://epic.org/privacy/streetview/ (accessed 21
November 2014).
Farahany, N.A. (2012). Searching Secrets, University of Pennsylvania Law Review
160(5): 1239–1308.
Farzad, R. (2014). Google at $400 billion: a new no. 2 in market cap,
BusinessWeek: Technology. 12 February [WWW document] http://www
.businessweek.com/articles/2014-02-12/google-at-400-billion-a-new-no-dot-
2-in-market-cap.
Finamore, E. and Dutta, K. (2014). ‘Summoning the demon’: artificial intelligence
is real threat to humanity, says PayPal founder, The Independent. [WWW
document] http://www.independent.co.uk/life-style/gadgets-and-tech/news/
tesla-boss-elon-musk-warns-artificial-intelligence-development-is-summoning-
the-demon-9819760.html (accessed 22 November 2014).
Fink, E. (2014). This drone can steal what’s on your phone, CNNMoney. 20 March
[WWW document] http://money.cnn.com/2014/03/20/technology/security/
drone-phone/index.html (accessed 22 November 2014).
Flynn, K. (2014). Facebook will share users’ political leanings with ABC news,
BuzzFeed, Huffington Post. 31 October [WWW document] http://www.
huffingtonpost.com/2014/10/31/facebook-buzzfeed-politics_n_6082312.html
(accessed 22 November 2014).
Foroohar, R. (2014). Tech titans are living in a naïve, dangerously insular bubble,
Time. 24 January[WWW document] http://business.time.com/2014/01/24/eric-
schmidt-george-soros-a-tale-of-two-titans/.
Forster, N. (1767). An Enquiry into the Causes of the Present High Price of
Provisions, London, UK: J. Fletcher and Co.
Gabriel, S. (2014). Sigmar Gabriel political consequences of the Google debate,
Frankfurter Allgemeine Zeitung. 20 May [WWW document] http://www.faz.net/
aktuell/feuilleton/debatten/the-digital-debate/sigmar-gabriel-consequences-of-
the-google-debate-12948701.html.
Gapper, J. (2014). We are the product facebook has been testing, FT. [WWW
document] http://www.ft.com/intl/cms/s/0/6576b0c2-0138-11e4-a938-
00144feab7de.html#axzz3R6dH0dDm (accessed 5 July 2014).
Garside, J. (2014). From Google to Amazon: EU goes to war against power of US
digital giants, Guardian. 5 July [WWW document] http://www.theguardian.
com/technology/2014/jul/06/google-amazon-europe-goes-to-war-power-digital-
giants (accessed 21 November 2014).
Gibbs, S. (2014). Google’s founders on the future of health, transport – and robots,
Guardian. 7 July [WWW document] http://www.theguardian.com/technology/
2014/jul/07/google-founders-larry-page-sergey-brin-interview (accessed 21
November 2014).
Hayek, F.A. (1988). The Fatal Conceit: The errors of socialism, Chicago, IL:
University of Chicago Press.
Herold, B. (2014). Google under fire for data-mining student email messages –
education week, Education Week. 26 March [WWW document] http://www
.edweek.org/ew/articles/2014/03/13/26google.h33.html.","[305.71826086956514, 70.01888198757767, 553.4698136645962, 743.5095652173914]"
0002_jit.2015.5.pdf,14,references,"Hilbert, M. (2013). Technological Information Inequality As an Incessantly
Moving Target: The redistribution of information and communication capacities
between 1986 and 2010, Journal of the American Society for Information Science
and Technology 65(4): 821–835.
Hoofnagle, C.J., King, J., Li, S. and Turow, J. (2010). How different are young
adults from older adults when it comes to information privacy attitudes and
policies? SSRN Electronic Journal [WWW document] http://www.ssrn.com/
abstract=1589864.
Jammet, A. (2014). The Evolution of EU Law on the Protection of Personal Data,
Center for European Law and Legal Studies 3(6): 1–18.
Kelly, H. (2014). Smartphones are fading. Wearables are next,
CNNMoney. 19 March [WWW document] http://money.cnn.com/2014/03/
19/technology/mobile/wearable-devices/index.html (accessed 22 November
2014).
Kopczynski, P. (2014). French consumer rights watchdog sues
Google, Facebook, Twitter for privacy violations, Reuters. 25 March
[WWW document] http://rt.com/news/france-facebook-google-suit-129/
(accessed 21 November 2014).
Kovach, S. (2013). Google’s plan to take over the world, Business Insider. 18 May
[WWW document] http://www.businessinsider.com/googles-plan-to-take-over-
the-world-2013-5 (accessed 22 November 2014).
Kramer, A.D.I., Guillory, J.E. and Hancock, J.T. (2014). Experimental Evidence
of Massive-Scale Emotional Contagion through Social Networks, Proceedings of
the National Academy of Sciences 111(24): 8788–8790.
Kwong, R. (2014). Did privacy concerns change your online behaviour?, FT Data
Blog. 17 September [WWW document] http://blogs.ft.com/ftdata/2014/09/17/
didprivacy-concerns-change-your-online-behaviour/ (accessed 21 November
2014).
Lanier, J. (2013). Who Owns the Future? New York, NY: Simon & Schuster.
Lanier, J. (2014). Should Facebook manipulate users?: Lack of transparency in
Facebook study, The New York Times. 30 June [WWW document] http://www
.nytimes.com/2014/07/01/opinion/jaron-lanier-on-lack-of-transparency-in-
facebook-study.html.
LA Times, A. T. S (2014, August 1). 911 calls about Facebook outage angers L.A.
County sheriff’s officials, Los Angeles Times. [WWW document] http://www
.latimes.com/local/lanow/la-me-ln-911-calls-about-facebook-outage-angers-la-
sheriffs-officials-20140801-htmlstory.html.
Levy, S. (2009). Secret of googlenomics: data-fueled recipe brews profitability,
Wired. [WWW document] http://archive.wired.com/culture/culturereviews/
magazine/17-06/nep_googlenomics (accessed 22 November 2014).
Lin, P. (2014). What if your autonomous car keeps routing you past Krispy Kreme?
The Atlantic. 22 January [WWW document] http://www.theatlantic.com/
technology/archive/2014/01/what-if-your-autonomous-car-keeps-routing-you-
past-krispy-kreme/283221/ (accessed 22 November 2014).
Locke, J. (2010). Two Treatises of Government. New York: Kessinger Publishing,
LLC.
Madden, M. (2014). Public perceptions of privacy and security in the post-
Snowden era [WWW document] http://www.pewinternet.org/2014/11/12/
public-privacy-perceptions/.
Mance, H., Ahmed, M. and Barker, A. (2014). Google break-up plan emerges
from Brussels, Financial Times. 21 November [WWW document] http://www.ft.
com/intl/cms/s/0/617568ea-71a1-11e4-9048-00144feabdc0.
html#axzz3JjXPNno5 (accessed 21 November 2014).
Manyika, J. and Chui, M. (2014). Digital era brings hyperscale challenges,
Financial Times. 13 August [WWW document] http://www.ft.com/intl/cms/s/0/
f30051b2-1e36-11e4-bb68-00144feabdc0.html?siteedition=intl#axzz3JjXPNno5
(accessed 22 November 2014).
Marthews, A. and Tucker, C. (2014). Government Surveillance and Internet Search
Behavior, Cambridge, MA: Digital Fourth. [WWW document] http://www.ssrn
.com/abstract=2412564.
Mayer-Schönberger, V. and Cukier, K. (2013). Big Data: A revolution that will
transform how we live, work, and think. Reprint edn Boston, MA: Houghton
Mifflin Harcourt.
McKendrick, N. (1982). The Consumer Revolution of Eighteenth-Century
England, in N. McKendrick, J. Brewer and J.H. Plumb (eds.) The Birth of a
Consumer Society: The commercialization of eighteenth-century England,
Bloomington, IL: Indiana University Press.
Menn, J., Schåfer, D. and Bradshaw, T. (2010). Google set for probes on data
harvesting, Financial Times. 17 May [WWW document] http://www.ft.com/intl/
cms/s/2/254ff5b6-61e2-11df-998c-00144feab49a.html#axzz3JjXPNno5
(accessed 21 November 2014).","[42.76173913043473, 69.12447204968946, 289.6188819875776, 742.6151552795031]"
0002_jit.2015.5.pdf,14,references,"Michaels, J.D. (2008). All the President’s Spies: Private-public intel-
ligence partnerships in the war on terror, California Law Review 96(4):
901–966.
Mick, J. (2011). ACLU fights for answers on police phone location data tracking,
Daily Tech. 4 August [WWW document] http://www.dailytech.com/ACLU
+Fights+for+Answers+on+Police+Phone+Location+Data+Tracking/
article22352.htm (accessed 21 November 2014).
Münstermann, B., Smolinski, B. and Sprague, K. (2014). The Enterprise IT
Infrastructure Agenda for 2014, McKinsey & Company White Paper : 1–8.
Newman, J. (2009). Google’s Schmidt Roasted for Privacy Comments, PCWorld.
11 December [WWW document] http://www.pcworld.com/article/184446/
googles_schmidt_roasted_for_privacy_comments.html (accessed 21 November
2014).
Newman, N. (2014). The costs of lost privacy: consumer harm and rising economic
inequality in the age of Google, William-Mitchell Law Review 40(2): 12.
Nicas, J. (2014). JetBlue to add bag fees, reduce legroom, Wall Street Journal.
20 November [WWW document] http://online.wsj.com/articles/jetblue-to-add-
bag-fees-reduce-legroom-1416406199.
Nissembaum, H. (2011). A Contextual Approach to Privacy Online, Daedalus
140(4): 32–48.
O’Brien, K.J. (2012). European regulators may reopen Google Street View
inquiries, The New York Times. 2 May [WWW document] http://www.nytimes.
com/2012/05/03/technology/european-regulators-to-reopen-google-street-view-
inquiries.html.
O’Brien, K.J. and Crampton, T. (2007). E.U. probes Google over data retention
policy, The New York Times. 26 May [WWW document] http://www.nytimes.
com/2007/05/26/business/26google.html.
O’Brien, K.J. and Miller, C.C. (2013). Germany’s complicated relationship with
Google Street View, Bits Blog. 23 April [WWW document] http://bits.blogs.
nytimes.com/2013/04/23/germanys-complicated-relationship-with-google-
street-view/ (accessed 21 November 2014).
Office of the Privacy Commission of Canada (2010). Google contravened
Canadian privacy law, investigation finds, Office of the Privacy Commissioner of
Canada. 19 October [WWW document] https://www.priv.gc.ca/media/nr-c/
2010/nr-c_101019_e.asp (accessed 21 November 2014).
Owen, J. (2014). Google in court again over ‘right to be above British law’ on
alleged secret monitoring, The Independent. 8 December.
Palfrey, J. (2008). The Public and the Private the United States Border with
Cyberspace, Mississippi Law Journal 78(2): 241–294.
Parnell, B.-A. (2014). Is Google building SKYNET? Ad kingpin buys AI firm
DeepMind, Register. 27 January [WWW document] http://www.theregister.co.
uk/2014/01/27/google_deep_mind_buy/ (accessed 22 November 2014).
Pentland, A. (2009). Reality mining of mobile communications: Toward a new deal
on data, in The Global Information Technology Report, World Economic Forum
& INSEAD, pp. 75–80.
PEW Research Center (2014). Digital life in 2015. (Research Report). [WWW
document] http://www.pewinternet.org/2014/03/11/digital-life-in-2025/.
Piketty, T. (2014). Capital in the Twenty-First Century, Cambridge, MA: Belknap
Press of Harvard University Press.
Plummer, Q. (2014). Google email tip-off draws privacy concerns, Tech Times.
5 August [WWW document] http://www.techtimes.com/articles/12194/
20140805/google-email-tip-off-draws-privacy-concerns.htm (accessed 21
November 2014).
Reidenberg, J.R. (2014). Data surveillance state in the United States and Europe,
Wake Forest Law Review 48(1): 583.
Richards, N.M. (2013). The Dangers of Surveillance, Harvard Law Review
126: 1934–1965.
Richards, N.M. and King, J.H. (2014). Big Data Ethics. (Accepted Paper Series)
Saint Louis, MO: Wake Forest Law Review.
Schmarzo, B. (2014). The value of data: Google gets It!!, EMC InFocus. 10 June
[WWW document] https://infocus.emc.com/william_schmarzo/the-value-of-
data-google-gets-it/.
Schmidt, E. (2014). A chance for growth, Frankfurter Allgemeine Zeitung. 9 April
[WWW document] http://www.faz.net/aktuell/feuilleton/debatten/eric-schmidt-
about-the-good-things-google-does-a-chance-for-growth-12887909.html.
Schwartz, P. (1989). The Computer in German and American Constitutional Law:
Towards an American right of informational self-determination, American
Journal of Comparative Law 37(4): 675–701.
Semitsu, J.P. (2011). From Facebook to Mug Shot: How the dearth of social
networking privacy rights revolutionized online government surveillance, Pace
Law Review 31(1): 291.","[303.9294409937888, 69.12447204968946, 551.6809937888198, 743.5095652173914]"
0002_jit.2015.5.pdf,15,references,"Sklar, M.J. (1988). The Corporate Reconstruction of American Capitalism:
1890–1916: The market, the law, and politics, New York, NY: Cambridge
University Press.
Smith, A. (1994). The Wealth of Nations. (E. Cannan, ed.), Later Printing edn
New York: Modern Library.
Snelling, D. (2014). Google Maps is tracking you! How your smartphone knows
your every move, Express.co.uk. 18 August http://www.express.co.uk/life-style/
science-technology/500811/Google-Maps-is-tracking-your-every-move
(accessed 21 November 2014).
Solove, D.J. (2007). ‘I’ve Got Nothing to Hide’ and Other Misunderstandings of
Privacy, San Diego Law Review 44: 745.
Solove, D.J. (2013). Introduction: Privacy self-management and the consent
dilemma, Harvard Law Review 126(7): 1880–1904.
Steingart, G. (2014). Google debate our weapons in the digital battle for freedom,
Frankfurter Allgemeine Zeitung. 23 June [WWW document] http://www.faz.net/
aktuell/feuilleton/debatten/the-digital-debate/google-debatte-waffen-im-
digitalen-freiheitskampf-13005653.html.
Streitfeld, D. (2013). Google admits Street View project violated privacy, New York
Times. 12 March [WWW document] http://www.nytimes.com/2013/03/13/
technology/google-pays-fine-over-street-view-privacy-breach.html.
Trotman, A. (2014). Google boss Larry Page: Europe needs to be more like
Silicon Valley and support technology, Telegraph. 31 October [WWW
document] http://www.telegraph.co.uk/technology/google/11202850/Google-boss-
Larry-Page-Europe-needs-to-be-more-like-Silicon-Valley-and-support-technology
.html.
Unger, R.M. (2007). Free Trade Reimagined: The world division of labor and the
method of economics, Princeton, NJ: Princeton University Press.
U.S. Committee on Commerce, Science, and Transportation (2013). A review of
the data broker industry: collection, use and sale of consumer data for marketing
purposes, Office of Oversight and Investigations. [WWW document] http://
www.commerce.senate.gov/public/?a=Files.Serve&File_id=0d2b3642-6221-
4888-a631-08f2f255b577.
Vaidhyanathan, Siva. (2011). The Googilization of Everything, Berkeley, CA:
University of California Press.
Varian, H.R. (2010). Computer Mediated Transactions, American Economic
Review 100(2): 1–10.
Varian, H.R. (2014). Beyond Big Data, Business Economics 49(1): 27–31.
Vasagar, J. (2014). Google could face ‘cyber courts’ in Germany over privacy rights,
Financial Times. 27 May [WWW document] http://www.ft.com/intl/cms/s/0/
a7580826-e59d-11e3-8b90-00144feabdc0.html#axzz3JjXPNno5 (accessed 21
November 21 2014).
Wallbank, P. (2012). How much server space do Internet companies need to run
their sites? Decoding the New Economy. 23 August [WWW document] http://
paulwallbank.com/2012/08/23/how-much-server-space-do-internet-companies-
need-to-run-their-sites/ (accessed 22 November 2014).
Waters, R. (2014). FT interview with Google co-founder and CEO Larry Page – FT.
com, Financial Times. 31 October [WWW document] http://www.ft.com/intl/
cms/s/2/3173f19e-5fbc-11e4-8c27-00144feabdc0.html#axzz3JjXPNno5
(accessed 21 November 2014).
Weatherill, L. (1993). The Meaning of Consumer Behavior in the Seventeenth and
Early Eighteenth-Century England, in J. Brewer and R. Porter (eds.)
Consumption and the World of Goods, London, UK: Routledge.","[41.86732919254653, 70.01888198757767, 291.40770186335396, 564.6275776397515]"
0002_jit.2015.5.pdf,15,references,"Weber, M. (1978). Economy and Society: An outline of interpretive sociology.
Vol. 1 Berkeley, CA: University of California Press.
Weiser, M. (1991). The Computer for the 21st Century, Scientific American 265(3):
94–104.
White House (2014). Big Data: seizing opportunities, preserving values (Report for
the President), Washington D.C., USA: Executive Office of the President.
[WWW document] http://www.whitehouse.gov/sites/default/files/docs/
big_data_privacy_report_may_1_2014.pdf.
Williamson, O.E. (1985). The Economic Institutions of Capitalism, New York;
London: Free Press.
Winkler, R. and Wakabayashi, D. (2014). Google to buy nest labs for $3.2 billion –
update, EuroInvestor. 14 January [WWW document] http://www.euroinvestor.
com/news/2014/01/14/google-to-buy-nest-labs-for-32-billion-update/12658007
(accessed 22 November 2014).
Ziegler, C. (2012). Facebook IPO facts and figures: the house that 100 petabytes
built, Verge. 1 February [WWW document] http://www.theverge.com/2012/2/1/
2764905/facebook-ipo-facts-and-figures-the-house-that-100-petabytes-built
(accessed 22 November 2014).
Zittrain, J. (2014). Facebook could decide an election without anyone ever finding
out, New Republic. 1 June [WWW document] http://www.newrepublic.com/
article/117878/information-fiduciary-solution-facebook-digital-
gerrymandering.
Zuboff, S. (1981). Psychological and Organizational Implications of Computer-
Mediated Work, MIT Working Paper, Center for Information Systems Research.
Zuboff, S. (1982). New Worlds of Computer-Mediated Work, Harvard Business
Review 60(5): 142–152.
Zuboff, S. (1985). Automate/Informate: The two faces of intelligent technology,
Organizational Dynamics 14(2): 5–18.
Zuboff, S. (1988). In the Age of the Smart Machine: The future of work and power,
New York, NY: Basic Books.
Zuboff, S. (2013). Computer-Mediated Work, In Sociology of Work: An
Encyclopedia, Thousand Oaks, CA: SAGE Publications, Inc. [WWW document]
http://knowledge.sagepub.com/view/sociology-of-work/n41.xml.
Zuboff, S. and Maxmin, J. (2002). The Support Economy: Why corporations are
failing individuals and the next episode of capitalism, New York, NY: Viking
Penguin.","[305.71826086956514, 70.01888198757767, 552.5754037267079, 409.8946583850932]"
0002_jit.2015.5.pdf,3,body,"from it were never – and can never be – ‘things in themselves.’
They are always already constituted by the answers to these
questions. In other words, they are already embedded in the
social, their possibilities circumscribed by authority and
power.
The key point here is that when it comes to the market
sphere, the electronic text is already organized by the logic
of accumulation in which it is embedded and the conflicts
inherent to that logic. The logic of accumulation organizes
perception and shapes the expression of technological affor-
dances at their roots. It is the taken-for-granted context of any
business model. Its assumptions are largely tacit, and its power
to shape the field of possibilities is therefore largely invisible. It
defines objectives, successes, failures, and problems. It deter-
mines what is measured, and what is passed over; how
resources and people are allocated and organized; who is
valued in what roles; what activities are undertaken – and to
what purpose. The logic of accumulation produces its own
social relations and with that its conceptions and uses of
authority and power.
In the history of capitalism, each era has run toward a
dominant logic of accumulation – mass production-based
corporate capitalism in the 20th century shaded into financial
capitalism by that century’s end – a form that continues to hold
sway. This helps to explain why there is so little real competitive
differentiation within industries. Airlines, for example, have
immense information flows that are interpreted along more or
less similar lines toward similar aims and metrics, because firms
are all evaluated according to the terms of a single shared logic
of accumulation.1 The same could be said for banks, hospitals,
telecommunications companies, and so forth.
Still, capitalism’s success over the longue durée has
depended upon the emergence of new market forms expres-
sing new logics of accumulation that are more successful at
meeting the ever-evolving needs of populations and their
expression in the changing nature of demand.2 As Piketty
acknowledges in his Capital in the Twenty-First Century,
‘There is no single variety of capitalism or organization of
production … This will continue to be true in the future, no
doubt more than ever: new forms of organization and owner-
ship remain to be invented’ (Piketty, 2014: 483). The philoso-
pher and legal scholar Roberto Unger has also written
persuasively on this point:
The concept of a market economy is institutionally indeter-
minate … it is capable of being realized in different legal and
institutional directions, each with dramatic consequences for
every aspect of social life, including the class structure of
society and the distribution of wealth and power … Which of
its institutional realizations prevails has immense importance
for the future of humanity … a market economy can adopt
radically divergent institutional forms, including different
regimes of property and contract and different ways of
relating government and private producers. The forms now
established in the leading economies represent the fragment
of a larger and open-ended field of possibilities.
(Unger 2007: 8, 41)
New market forms emerge in distinct times and places. Some
rise to hegemony, others exist in parallel to the dominant
form, and others are revealed in time as evolutionary dead
ends.","[41.86732919254653, 69.12447204968946, 290.51329192546575, 747.087204968944]"
0002_jit.2015.5.pdf,3,body,"How can these conceptual building blocks help us make
sense of ‘big data’? Some points are obvious: three of the
world’s seven billion people are now computer-mediated in a
wide range of their daily activities far beyond the traditional
boundaries of the workplace. For them the old dream of
ubiquitous computing (Weiser, 1991) is a barely noticeable
truism. As a result of pervasive computer mediation, nearly
every aspect of the world is rendered in a new symbolic
dimension as events, objects, processes, and people become
visible, knowable, and shareable in a new way. The world is
reborn as data and the electronic text is universal in scale and
scope.3 Just a moment ago, it still seemed reasonable to focus
our concerns on the challenges of an information workplace
or an information society. Now the enduring questions of
authority and power must be addressed to the widest possible
frame that is best defined as ‘civilization’ or more specifically –
information civilization. Who learns from global data flows,
what, and how? Who decides? What happens when authority
fails? What logic of accumulation will shape the answers to
these questions? Recognizing their civilizational scale lends
these questions new force and urgency. Their answers will
shape the character of information civilization in the century
to come, just as the logic of industrial capitalism and its
successors shaped the character of industrial civilization over
the last two centuries.
In the brief space of this work, my ambition is to begin the
task of illuminating an emergent logic of accumulation that vies
for hegemony in today’s networked spaces. My primary lens for
this brief exploration is Google, the world’s most popular
website. Google is widely considered to be the pioneer of ‘big
data’ (e.g. Mayer-Schönberger and Cukier, 2013), and on the
strength of those accomplishments it has also pioneered the
wider logic of accumulation I call surveillance capitalism, of
which ‘big data’ is both a condition and an expression. This
emerging logic is not only shared by Facebook and many other
large Internet-based firms, it also appears to have become the
default model for most online startups and applications. Like
Constantiou and Kallinikos (2014), I begin this discussion with
the characteristics of the data in ‘big data’ and how they are
generated. But where those authors trained their sights on the
data’s epistemic features, I want to consider their individual,
social, and political significance.
This discussion here is organized around two extraordinary
documents written by Google’s Chief Economist Hal Varian
(Varian, 2014, 2010). His claims and observations offer a
starting point for insights into the systemic logic of accumula-
tion in which ‘big data’ are embedded. I note here that while
Varian is not a Google line executive, his articles invite a close
inspection of Google’s practices as a prime exemplar of this
new logic of accumulation. In both pieces, Varian illustrates
his points with examples from Google. He often uses the first
person plural in these instances, such as, ‘Google has been so
successful with our own experiments that we have made them
available to our advertisers and publishers in two programs.’
Or, ‘Google has seen 30 trillion URLs, crawls over 20 billion of
those a day and answers 100 billion search queries a month …
we have had to develop new types of databases that can store
data in massive tables spread across thousands of machines
and can process queries on more than a trillion records in a
few seconds. We published descriptions of these tools …’
(Varian, 2014: 27, 29). It therefore seems fair to assume that
Varian’s perspectives
reflect
the substance
of
Google’s","[305.71826086956514, 70.01888198757767, 555.2586335403727, 747.9816149068322]"
0002_jit.2015.5.pdf,4,body,"business practices, and, to a certain extent, the worldview that
underlies those practices.
In the two articles I examine here, Varian’s theme is the
universality of ‘computer-mediated economic transactions.’
He writes, ‘The computer creates a record of the transaction
… I argue that these computer-mediated transactions have
enabled significant improvements in the way transactions are
carried out and will continue to impact the economy for the
foreseeable future’ (2010: 2). The implications of Varian’s
observation are significant. The informating of the economy,
as he observes, is constituted by a pervasive and continuous
recording of the details of each transaction. In this vision,
computer mediation renders an economy transparent and
knowable in new ways. This is a sharp contrast to the classic
neoliberal ideal of ‘the market’ as intrinsically ineffable and
unknowable. Hayek’s conception of the market was as an
incomprehensible ‘extended order’ to which mere individuals
must subjugate their wills (Hayek, 1988: 14–15). It was
precisely the unknowability of the universe of market transac-
tions that anchored Hayek’s claims for the necessity of radical
freedom from state intervention or regulation. Given Varian’s
new facts of a knowable market, he asserts four new ‘uses’ that
follow from computer-mediated transactions: ‘data extraction
and analysis,’ ‘new contractual forms due to better monitor-
ing,’ ‘personalization and customization,’ and ‘continuous
experiments’ (Varian, 2014). Each one of these provides
insights into an emerging logic of accumulation, the division
of learning that it shapes, and the character of the information
civilization toward which it leads.","[41.86732919254653, 70.01888198757767, 289.6188819875776, 384.8511801242236]"
0002_jit.2015.5.pdf,4,heading,"Data, extraction, analysis
The first of Varians new ","[42.76173913043473, 407.21142857142854, 142.93565217391298, 417.04993788819877]"
0002_jit.2015.5.pdf,4,body,"Data, extraction, analysis
The first of Varian’s new uses is ‘data extraction and analysis
… what everyone is talking about when they talk about big
data’ (Varian, 2014: 27). I want to examine each word in this
phrase – ‘data,’ ‘extraction,’ and ‘analysis’ – as each conveys
insights into the new logic of accumulation.
Data
The data from computer-mediated economic transactions is a
significant dimension of ‘big data.’ There are other sources too,
including flows that arise from a variety of computer-mediated
institutional and trans-institutional systems. Among these we
can include a second source of computer-mediated flows that is
expected to grow exponentially: data from billions of sensors
embedded in a widening range of objects, bodies, and places. An
often cited Cisco White Paper predicts $14.4 trillion of new
value associated with this ‘Internet of Everything’ (Cisco,
2013a, b). Google’s new investments in machine learning,
drones, wearables, self-driving cars, nano particles that ‘patrol’
the body for signs of disease, and smart devices for the home are
each essential components of this growing network of smart
sensors and Internet-enabled devices intended as a new intelli-
gent infrastructure for objects and bodies (Bradshaw, 2014a, b;
Kovach, 2013; BBC News, 2014; Brewster, 2014; Dwoskin, 2014;
Economist, 2014; Fink, 2014; Kelly, 2014; Lin, 2014; Parnell,
2014; Winkler and Wakabayashi, 2014). A third source of data
flows from corporate and government databases including those
associated with banks, payment-clearing intermediaries, credit
rating agencies, airlines, tax and census records, health care
operations, credit card, insurance, pharmaceutical, and telecom
companies, and more. Many of these data, along with the data","[41.86732919254653, 416.15552795031056, 290.51329192546575, 748.8760248447204]"
0002_jit.2015.5.pdf,4,body,"flows of commercial transactions, are purchased, aggregated,
analyzed, packaged, and sold by data brokers who operate, in the
US at least, in secrecy – outside of statutory consumer protec-
tions and without consumers’ knowledge, consent, or rights of
privacy and due process (U.S. Committee on Commerce,
Science, and Transportation, 2013).
A fourth source of ‘big data,’ one that speaks to its
heterogeneous and trans-semiotic character, flows from pri-
vate and public surveillance cameras, including everything
from smartphones to satellites, Street View to Google Earth.
Google has been at the forefront of this contentious data
domain. For example, Google Street View was launched in
2007 and encountered opposition around the world. German
authorities discovered that some Street View cars were
equipped with scanners to scrape data from private Wi-Fi
networks (O’Brien and Miller, 2013). According to the
Electronic Privacy Information Center’s (EPIC) summary of
a lawsuit filed by 38 states’ Attorneys General and the District
of Columbia, the court concluded that ‘the company engaged
in unauthorized collection of data from wireless networks,
including private WiFi networks of residential Internet users.’
The EPIC report summarizes a redacted version of an FCC
report revealing that ‘Google intentionally intercepted payload
data for business purposes and that many supervisors and
engineers within the company reviewed the code and the
design documents associated with the project’ (EPIC, 2014b).
According to the New York Times account of Google’s
eventual seven million dollar settlement of the case, ‘the search
company for the first time is required to aggressively police its
own employees on privacy issues …’ (Streitfeld, 2013). Street
View was restricted in many countries and continues to face
litigation over what claimants have characterized as ‘secret,’
‘illicit,’ and ‘illegal’ data gathering tactics in the US, Europe,
and elsewhere (Office of the Privacy Commission of Canada,
2010; O’Brien, 2012; Jammet, 2014).
In Street View, Google developed a declarative method
that it has repeated in other data ventures. This modus
operandi is that of incursion into undefended private terri-
tory until resistance is encountered. As one consumer watch-
dog summarized it for the New York Times, ‘Google puts
innovation ahead of everything and resists asking permis-
sion’ (Streitfeld, 2013; see also Burdon and McKillop, 2013).
The firm does not ask if it can photograph homes for its
databases. It simply takes what it wants. Google then
exhausts its adversaries in court or eventually agrees to pay
fines that represent a negligible investment for a significant
return.4 It is a process that Siva Vaihyanathan has called
‘infrastructure imperialism’ (Vaidhyanathan, 2011). EPIC
maintains a comprehensive online record of the hundreds of
cases launched against Google by countries, states, groups,
and individuals, and there are many more cases that never
become public (EPIC, 2014a, b).
These institutionally produced data flows represent the
‘supply’ side of the computer-mediated interface. With these
data alone it is possible to construct detailed individual
profiles. But the universality of computer-mediation has
occurred through a complex process of causation that includes
subjective activities too – the demand side of computer-
mediation. Individual needs drove the accelerated penetration
curves of the Internet. In less than two decades after the
Mosaic web browser was released to the public, enabling easy
access to the World Wide Web, a 2010 BBC poll found that","[305.71826086956514, 69.12447204968946, 552.5754037267079, 747.9816149068322]"
0002_jit.2015.5.pdf,5,body,"79% of people in 26 countries considered Internet access to be
a fundamental human right (BBC, 2010).
Outside the market-based hierarchical spaces of the work-
place, Internet access, indexing, and search meant that indivi-
duals were finally free to pursue the resources they needed for
effective life unimpeded by the monitoring, metrics, insecurity,
role requirements, and secrecy imposed by the firm and its logic
of accumulation. Individual needs for self-expression, voice,
influence, information, learning, empowerment, and connection
summoned all sorts of new capabilities into existence in just a
few years: Google’s searches, iPod’s music, Facebook’s pages,
YouTube’s videos, blogs, networks, communities of friends,
strangers, and colleagues, all reaching out beyond the old
institutional and geographical boundaries in a kind of exultation
of hunting and gathering and sharing information for every
purpose or none at all. It was mine, and I could do with it what I
wished!5 These subjectivities of self-determination found expres-
sion in a new networked individual sphere characterized by what
Benkler (2006) aptly summarized as non-market forms of ‘social
production.’
These non-market activities are a fifth principal source of
‘big data’ and the origin of what Constantiou and Kallinikos
(2014) refer to as its ‘everydayness.’ ‘Big data’ are constituted by
capturing small data from individuals’ computer-mediated
actions and utterances in their pursuit of effective life. Nothing
is too trivial or ephemeral for this harvesting: Facebook ‘likes,’
Google searches, emails, texts, photos, songs, and videos, loca-
tion, communication patterns, networks, purchases, movements,
every click, misspelled word, page view, and more. Such data are
acquired, datafied, abstracted, aggregated, analyzed, packaged,
sold, further analyzed and sold again. These data flows have been
labeled by technologists as ‘data exhaust.’ Presumably, once the
data are redefined as waste material, their extraction and eventual
monetization are less likely to be contested.
Google became the largest and most successful ‘big data’
company, because it is the most visited website and therefore has
the largest data exhaust. Like many other born-digital firms,
Google rushed to meet the waves of pent-up demand that
flooded the networked individual sphere in the first years of the
World Wide Web. It was a heroic exemplar of individual
empowerment in the quest for effective life. But as pressures
for profit mounted, Google’s leaders were concerned about the
effect that fees-for-service might have on user growth. They
opted instead for an advertising model. The new approach
depended upon the acquisition of user data as the raw material
for proprietary analyses and algorithm production that could
sell and target advertising through a unique auction model with
ever more precision and success. As Google’s revenues rapidly
grew, they motivated ever more comprehensive data collection.6
The new science of big data analytics exploded, driven largely by
Google’s spectacular success.
Eventually it became clear that Google’s business is the
auction business, and its customers are advertisers (see useful
discussions
of
this
turning
point
in
Auletta,
2009;
Vaidhyanathan, 2011; and Lanier, 2013). AdWords, Google’s
algorithmic auction method for selling online advertising,
analyzes massive amounts of data to determine which adver-
tisers get which one of 11 sponsored links on each results page.
In a 2009 Wired article on ‘Googlenomics,’ Google’s Varian
commented, ‘Why does Google give away products … ?
Anything that increases Internet use ultimately enriches
Google …’ The article continues, ‘… more eyeballs on the","[41.86732919254653, 69.12447204968946, 290.51329192546575, 748.8760248447204]"
0002_jit.2015.5.pdf,5,body,"Web lead inexorably to more ad sales for Google … And since
prediction and analysis are so crucial to AdWords, every bit of
data, no matter how seemingly trivial, has potential value’
(Levy, 2009). The theme is reiterated in Mayer-Schönberger
and Cukier’s Big Data: ‘Many companies design their systems
so that they can harvest data exhaust … Google is the
undisputed leader … every action a user performs is consid-
ered a signal to be analyzed and fed back into the system’
(2013: 113). This helps explain why Google outbid all
competitors for the privilege of providing free Wi-Fi to
Starbuck’s 3 billion yearly customers (Schmarzo, 2014). More
users produce more exhaust that improves the predictive value
of analyses and results in more lucrative auctions. What
matters is quantity not quality. Another way of saying this is
that Google is ‘formally indifferent’ to what its users say or do,
as long as they say it and do it in ways that Google can capture
and convert into data.","[305.71826086956514, 69.12447204968946, 553.4698136645962, 254.26732919254664]"
0002_jit.2015.5.pdf,5,heading,Extraction,"[306.6126708074534, 277.5219875776398, 343.2834782608695, 287.36049689440995]"
0002_jit.2015.5.pdf,5,body,"This ‘formal indifference’ is a prominent, perhaps decisive,
characteristic of the emerging logic of accumulation under
examination here. The second term in Varian’s phrase,
‘extraction,’ also sheds light on the social relations implied by
formal indifference. First, and most obvious, extraction is a
one-way process, not a relationship. Extraction connotes a
‘taking from’ rather than either a ‘giving to,’ or a reciprocity of
‘give and take.’ The extractive processes that make big data
possible typically occur in the absence of dialogue or consent,
despite the fact that they signal both facts and subjectivities of
individual lives. These subjectivities travel a hidden path to
aggregation and decontextualization, despite the fact that they
are produced as intimate and immediate, tied to individual
projects and contexts (Nissembaum, 2011). Indeed, it is the
status of such data as signals of subjectivities that makes them
most valuable for advertisers. For Google and other ‘big data’
aggregators, however, the data are merely bits. Subjectivities
are converted into objects that repurpose the subjective for
commodification. Individual users’ meanings are of no inter-
est to Google or other firms in this chain. In this way, the
methods of production of ‘big data’ from small data and the
ways in which ‘big data’ are valued reflect the formal
indifference that characterizes the firm’s relationship to its
populations of ‘users.’ Populations are the sources from which
data extraction proceeds and the ultimate targets of the
utilities such data produce.
Formal indifference is evident in the aggressiveness with
which Google pursues its interests in extracting signals of
individual subjectivities. In these extractive activities it follows
the Street View model: incursions into legally and socially
undefended territory until resistance is encountered. Its
practices appear designed to be undetectable or at least
obscure, and had it not been for NSA whistleblower Edward
Snowden aspects of its operations, especially as they overlap
state security interests, would still be hidden. Most of what was
known about Google’s practices erupted from the conflicts it
produced (Angwin, 2014). For example, Google has faced legal
opposition and social protest in relation to claims of (1) the
scanning of email, including those of non-Gmail users and those
of students using its educational apps (Herold, 2014; Plummer,
2014), (2) the capture of voice communications (Menn
et al., 2010), (3) bypassing privacy settings (Angwin, 2012;","[304.823850931677, 288.25490683229816, 554.3642236024846, 749.7704347826086]"
0002_jit.2015.5.pdf,6,body,"Owen, 2014), (4) unilateral practices of data bundling across
its online services (CNIL, 2014; Doyle, 2013), (5) its extensive
retention of search data (Anderson, 2010; O’Brien and
Crampton, 2007), (6) its tracking of smartphone location data
(Mick, 2011; Snelling, 2014), and (7) its wearable technologies
and facial recognition capabilities (EPIC, 2014a, https://epic.
org/privacy/google/glass/). These contested data gathering
moves face substantial opposition in the EU as well as the US
(Barker and Fontanella-Khan, 2014; Gabriel, 2014; Garside,
2014; Kopczynski, 2014; Mance et al., 2014; Steingart, 2014;
Vasagar, 2014).
‘Extraction’ summarizes the absence of structural recipro-
cities between the firm and its populations. This fact alone lifts
Google, and other participants in its logic of accumulation, out
of the historical narrative of Western market democracies. For
example, the 20th-century corporation canonized by scholars
like Berle and Means (1991) and Chandler Jr (1977) originated
in and was sustained by deep interdependencies with its
populations. The form and its bosses had many failings and
produced many violent facts that have been well documented,
but I focus here on a different point. That market form
intrinsically valued its populations of newly modernizing
individuals as its source of employees and customers; it
depended upon its populations in ways that led over time to
institutionalized reciprocities. In return for its rigors, the form
offered a quid pro quo that was consistent with the self-
understanding and demand characteristics of its populations.
On the inside were durable employment systems, career
ladders, and steady increases in wages and benefits for more
workers (Sklar, 1988). On the outside were the dramas of
access to affordable goods and services for more consumers
(Cohen, 2003).
The ‘five dollar day’ was emblematic of this systemic logic,
recognizing as it did that the whole enterprise rested upon a
consuming population. The firm, Ford realized, had to value
the worker-consumer as a fundamental unity and the essential
component of a new mass production capitalism. This social
contract hearkened back to Adam Smith’s original insights
into the productive reciprocities of capitalism, in which price
increases were balanced with wage increases, ‘so that the
labourer may still be able to purchase that quantity of those
necessary articles which the state of the demand for labour …
requires that he should have’ (Smith, 1994: 939–940). It was
these reciprocities that helped constitute a broad middle class
with steady income growth and a rising standard of living.
Indeed, considered from the vantage point of the last 30-plus
years during which this market form was systematically
deconstructed, its embeddedness in the social order through
these structural reciprocities appears to have been one of its
most salient features (Davis, 2011, 2013).
Google and the ‘big data’ project represent a break with this
past. Its populations are no longer necessary as the source of
customers or employees. Advertisers are its customers along
with other intermediaries who purchase its data analyses.
Google employs only about 48,000 people as of this writing,
and is known to have thousands of applicants for every job
opening. (As contrast: at the height of its power in 1953,
General Motors was the world’s largest private employer.)
Google, therefore, has little interest in its users as employees.
This pattern is true of hyperscale high tech companies
that achieve growth mainly by leveraging automation. For
example, the top three Silicon Valley companies in 2014 had","[41.86732919254653, 69.12447204968946, 288.7244720496894, 747.9816149068322]"
0002_jit.2015.5.pdf,6,body,"revenues of $247 billion, only 137,000 employees, and a
combined market capitalization of $1.09 trillion. In contrast,
even as late as 1990, the three top Detroit automakers
produced revenues of $250 billion with 1.2 million employees
and a combined market capitalization of $36 billion (Manyika
and Chui, 2014).
This structural independence of the firm from its popula-
tions is a matter of exceptional importance in light of the
historical relationship between market capitalism and democ-
racy. For example, Acemoglu and Robinson elaborate the
mutual structuring of (1) early industrial capitalism’s depen-
dency on the masses, (2) prosperity, and (3) the rise of
democracy in 19th-century Britain. Examining that era’s
successful new market forms and the accompanying shift
toward democratic institutions they observe, ‘Clamping down
on popular demands and undertaking a coup against inclusive
political institutions would … destroy … gains, and the elites
opposing greater democratization and greater inclusiveness
might find themselves among those losing their fortunes from
this destruction’ (2012: 313–314). Google bears no such risks.
On the contrary, despite its role as the ‘chief utility for the
World Wide Web’ (Vaidhyanathan, 2011: 17) and its sub-
stantial investments in technologies with explosive social
consequences such as artificial intelligence, robotics, facial
recognition, wearables, nanotechnology, smart devices, and
drones, Google has not been subject to any meaningful public
oversight (see e.g. the discussion in Vaidhyanathan, 2011: 44–
50; see also Finamore and Dutta, 2014; Gibbs, 2014; Trotman,
2014; Waters, 2014). In an open letter to Europe, Google
Chairperson Eric Schmidt recently expressed his frustration
with the prospect of public oversight, characterizing it as
‘heavy-handed regulation’ and threatening that it would create
‘serious economic dangers’ for Europe (Schmidt, 2014).","[304.823850931677, 70.01888198757767, 554.3642236024846, 430.46608695652174]"
0002_jit.2015.5.pdf,6,heading,Analysis,"[305.71826086956514, 451.9319254658385, 337.0226086956522, 462.66484472049683]"
0002_jit.2015.5.pdf,6,body,"Google’s formal indifference toward and functional distance
from populations is further institutionalized in the necessities
of ‘analysis’ that Varian emphasizes. Google is the pioneer of
hyperscale. Like other hyperscale businesses – Facebook,
Twitter, Alibaba, and a growing list of high volume informa-
tion businesses such as telecoms and global payments firms –
data centers require millions of ‘virtual servers’ that exponen-
tially increase computing capabilities without requiring sub-
stantial expansion of physical space, cooling, or electrical
power demands.7 Hyperscale businesses exploit digital mar-
ginal cost economics to achieve scale quickly at costs that
approach zero.8 In addition to these material capabilities,
Varian notes that analysis requires data scientists who have
mastered the new methods associated with predictive analy-
tics, reality mining, patterns-of-life analysis, and so forth.
These highly specialized material and knowledge require-
ments further separate subjective meaning from objective
result. In doing so, they eliminate the need for, or possibility
of, feedback loops between the firm and its populations. The
data travel through many phases of production, only to return
to their source in a second phase of extraction in which the
objective is no longer data but revenue. The cycle then begins
again in the form of new computer-mediated transactions.
This examination of Varian’s combination of data, extrac-
tion, and analysis begins to suggest some key features of the
new logic of accumulation associated with big data and","[305.71826086956514, 465.3480745341614, 553.4698136645962, 749.7704347826086]"
0002_jit.2015.5.pdf,7,body,"spearheaded by Google. First, revenues depend upon data
assets appropriated through ubiquitous automated operations.
These constitute a new asset class: surveillance assets. Critics of
surveillance capitalism might characterize such assets as
‘stolen goods’ or ‘contraband’ as they were taken, not given,
and do not produce, as I shall argue below, appropriate
reciprocities. The cherished culture of social production in
the networked individual sphere relies on the very tools that
are now the primary vehicles for the surveillance-based
appropriation of the most lucrative data exhaust. These
surveillance assets attract significant investment that can be
called surveillance capital. Google has, so far, triumphed in the
networked world through the pioneering construction of this
new market form that is a radically disembedded and extrac-
tive variant of information capitalism, one that can be
identified as surveillance capitalism. This new market form
has quickly developed into the default business model for
most online companies and startups, where valuations routi-
nely depend upon ‘eyeballs’ rather than revenue as a predictor
of remunerative surveillance assets.","[41.86732919254653, 70.01888198757767, 289.6188819875776, 287.36049689440995]"
0002_jit.2015.5.pdf,7,heading,Monitoring and contracts,"[40.078509316770145, 308.8263354037267, 143.8300621118012, 320.4536645962733]"
0002_jit.2015.5.pdf,7,body,"Varian
says,
‘Because
transactions
are
now
computer
mediated, we can observe behavior that was previously
unobservable and write contracts on it. This enables transac-
tions that were simply not feasible before … Computer-
mediated transactions have enabled new business models …’
(2014: 30). Varian offers examples: If someone stops making
monthly car payments, the lender can ‘instruct the vehicular
monitoring system not to allow the car to be started and to
signal the location where it can be picked up.’ Insurance
companies, he suggests, can rely on similar monitoring
systems to check if customers are driving safely and thus
determine whether or not to maintain their insurance or pay
claims. He also suggests that one can hire an agent in a remote
location to perform tasks and use data from their smartphones
– geolocation, time stamping, photos – to ‘prove’ that they
actually performed according to the contract.
Varian does not appear to realize that what he is celebrating
here is not new contract forms but rather the ‘un-contract.’
His version of a computer-mediated world transcends the
contract form by stripping away governance and the rule of
law. Varian appears to be aiming for what Oliver Williamson
calls ‘a condition of contract utopia’ (1985: 67). In William-
son’s transaction economics, contracts exist to mitigate the
inevitability of uncertainty. They operate to economize on
‘bounded rationality’ and safeguard against ‘opportunism’ –
both intractable conditions of contracts in the real world of
human
endeavor.
He
observes
that
certainty
requires
‘unbounded rationality’ derived from ‘unrestricted cognitive
competence,’ which in turn derives from ‘fully described’
adaptations
to
‘publicly
observable’
contingent
events.
Williamson notes that such conditions inhere to ‘a world of
planning’ rather than to ‘the world of governance’ in which
‘other things being equal … relations that feature personal
trust will survive greater stress and will display greater
adaptability’ (31–32, 63).
Varian’s vision of the uses of computer-mediated transac-
tions empties the contract of uncertainty. It eliminates the
need for – and therefore the possibility to develop – trust.
Another way of saying this is that contracts are lifted from the","[42.76173913043473, 321.3480745341615, 289.6188819875776, 747.9816149068322]"
0002_jit.2015.5.pdf,7,body,"social and reimagined as machine processes. Consensual
participation in the values from which legitimate authority is
derived, along with free will and reciprocal rights and obliga-
tions, are traded in for the universal equivalent of the prison-
er’s electronic ankle bracelet. Authority, which I have
elsewhere described as ‘the spiritual dimension of power,’
relies on social construction animated by shared foundational
values. In Varian’s economy, authority is supplanted by
technique, what I have called ‘the material dimension of
power,’ in which impersonal systems of discipline and control
produce certain knowledge of human behavior independent of
consent (Zuboff, 1988). This subject deserves a more detailed
exploration than is possible here, so I limit myself to a few key
themes.
In describing this ‘new use,’ Varian lays claim to vital
political territory for the regime of surveillance capitalism. From
Locke to Durkheim, the contract and the rule of law that
supports it have been understood as derived from the social
and the trust and organic solidarity of which the social is an
effect (Durkheim, 1964: 215; Locke, 2010: 112–115, 339). For
Weber, ‘the most essential feature of modern substantive law,
especially private law is the greatly increased significance of legal
transactions, particularly contracts, as a source of claims guar-
anteed by legal coercion … one can … designate the contem-
porary type of society … as a “contractual” one’ (1978: 669).
As Hannah Arendt suggests, ‘the great variety of contract
theories since the Romans attests to the fact that the power of
making promises has occupied the center of political thought
over the centuries.’ Most vivid is the operation of the contract
as it enhances the mastery of individuals and the resilience of
society. These goods derive precisely from the unpredictability
‘which the act of making promises at least partially dispels …’
For Arendt, human fallibility in the execution of contracts is
the price of freedom. The impossibility of perfect control
within a community of equals is the consequence of ‘plurality
and reality … the joy of inhabiting together with others a
world whose reality is guaranteed for each by the presence of
all.’ Arendt insists that ‘the force of mutual promise or
contract’ is the only alternative ‘to a mastery which relies on
domination of one’s self and rule over others; it corresponds
exactly to the existence of freedom which was given under the
condition of non-sovereignty’ (1998: 244).
In contrast to Arendt, Varian’s vision of a computer-
mediated world strikes me as an arid wasteland – not a
community of equals bound through laws in the inevitable
and ultimately fruitful human struggle with uncertainty. In
this futurescape, the human community has already failed. It
is a place adapted to the normalization of chaos and terror
where the last vestiges of trust have long since withered and
died. Human replenishment from the failures and triumphs of
asserting predictability and exercising over will in the face of
natural uncertainty gives way to the blankness of perpetual
compliance. Rather than enabling new contractual forms,
these arrangements describe the rise of a new universal
architecture existing somewhere between nature and God that
I christen Big Other. It is a ubiquitous networked institutional
regime that records, modifies, and commodifies everyday
experience from toasters to bodies, communication
to
thought, all with a view to establishing new pathways to
monetization and profit. Big Other is the sovereign power of
a near future that annihilates the freedom achieved by the rule
of law. It is a new regime of independent and independently","[306.6126708074534, 70.91329192546586, 555.2586335403727, 748.8760248447204]"
0002_jit.2015.5.pdf,8,body,"controlled facts that supplants the need for contracts, govern-
ance, and the dynamism of a market democracy. Big Other is
the 21st-century incarnation of the electronic text that aspires
to encompass and reveal the comprehensive immanent facts of
market, social, physical, and biological behaviors. The institu-
tional processes that constitute the architecture of Big Other
can be imagined as the material instantiation of Hayek’s
‘extended order’ come to life in the explicated transparency
of computer-mediation.
These processes reconfigure the structure of power, con-
formity, and resistance inherited from mass society and
symbolized for over half a century as Big Brother. Power can
no longer be summarized by that totalitarian symbol of
centralized command and control. Even the panopticon of
Bentham’s design, which I used as a central metaphor in my
earlier work (Zuboff, 1988, Ch. 9,10), is prosaic compared to
this new architecture. The panopticon was a physical design
that privileged a single point of observation. The anticipatory
conformity it induced required the cunning production of
specific behaviors while one was inside the panopticon, but
that behavior could be set aside once one exited that physical
place. In the 1980s it was an apt metaphor for the hierarchical
spaces of the workplace. In the world implied by Varian’s
assumptions, habitats inside and outside the human body are
saturated with data and produce radically distributed oppor-
tunities for observation, interpretation, communication, influ-
ence, prediction, and ultimately modification of the totality of
action. Unlike the centralized power of mass society, there is
no escape from Big Other. There is no place to be where the
Other is not.
In this world of no escape, the chilling effects of anticipatory
conformity9 give way as the mental agency and self-possession
of anticipation is gradually submerged into a new kind of
automaticity. Anticipatory conformity assumes a point of
origin in consciousness from which a choice is made to
conform for the purposes of evasion of sanctions and social
camouflage. It also implies a difference, or at least the
possibility of a difference, between the behavior one would
have performed and the behavior one chooses to perform as
an instrumental solution to invasive power. In a world of Big
Other, without avenues of escape, the agency implied in the
work of anticipation is gradually submerged into a new kind of
automaticity – a lived experience of pure stimulus-response.
Conformity is no longer a 20th century-style act of submission
to the mass or group, no loss of self to the collective produced
by fear or compulsion, no psychological craving for accep-
tance and belonging. Conformity now disappears into the
mechanical order of things and bodies, not as action but as
result, not cause but effect. Each one of us may follow a
distinct path, but that path is already shaped by the financial
and, or, ideological interests that imbue Big Other and invade
every aspect of ‘one’s own’ life. False consciousness is no
longer produced by the hidden facts of class and their relation
to production, but rather by the hidden facts of commoditized
behavior modification. If power was once identified with the
ownership of the means of production, it is now identified
with ownership of the means of behavioral modification.
Indeed, there is little difference between the ineffable
‘extended order’ of the neoliberal ideal and the ‘vortex of
stimuli’ responsible for all action in the vision of the classical
theorists of behavioral psychology. In both worldviews,
human autonomy is irrelevant and the lived experience of","[41.86732919254653, 70.91329192546586, 290.51329192546575, 747.9816149068322]"
0002_jit.2015.5.pdf,8,body,"psychological self-determination is a cruel illusion. Varian
adds a new dimension to both hegemonic ideals in that now
this ‘God view’ can be fully explicated, specified, and known,
eliminating all uncertainty. The result is that human persons
are reduced to a mere animal condition, bent to serve the new
laws of capital imposed on all behavior through an implacable
feed of ubiquitous fact-based real-time records of all things
and creatures. Hannah Arendt treated these themes decades
ago with remarkable insight as she lamented the devolution of
our conception of ‘thought’ to something that is accomplished
by a ‘brain’ and is therefore transferable to ‘electronic
instruments’:
The last stage of the laboring society, the society of jobholders,
demands of its members a sheer automatic functioning, as
though individual life had actually been submerged in the
over-all life process of the species and the only active decision
still required of the individual were to let go, so to speak, to
abandon his individuality, the still individually sensed pain
and trouble of living, and acquiesce in a dazed, ‘tranquilized,’
functional type of behavior. The trouble with modern theories
of behaviorism is not that they are wrong but that they could
become true, that they actually are the best possible concep-
tualization of certain obvious trends in modern society. It is
quite conceivable that the modern age – which began with
such an unprecedented and promising outburst of human
activity – may end in the deadliest, most sterile passivity
history has ever known.
(Arendt, 1998: 322)
Surveillance capitalism establishes a new form of power in
which contract and the rule of law are supplanted by the
rewards and punishments of a new kind of invisible hand.
A more complete theorization of this new power, while a
central concern of my new work, exceeds the scope of this
article. I do want to highlight, however, a few key themes that
can help us appreciate the unique character of surveillance
capitalism.
According to Varian, people agree to the ‘invasion of
privacy’ represented by Big Other if they ‘get something they
want in return … a mortgage, medical advice, legal advice – or
advice from your personal digital assistant’ (2014: 30). He is
quoted in a similar vein by a PEW Research report, ‘Digital
Life in 2025:’ ‘There is no putting the genie back in the bottle
… Everyone will expect to be tracked and monitored, since the
advantages, in terms of convenience, safety, and services, will
be so great … continuous monitoring will be the norm (PEW
Research, 2014). How to establish the validity of this assertion?
To what extent are these supposed reciprocities the product of
genuine consent? This question opens the way to another
radical, perhaps even revolutionary, aspect of the politics of
surveillance capitalism. This concerns the distribution of
privacy rights and with it the knowledge of and choice to
accede to Big Other.
Covert data capture is often regarded as a violation,
invasion, or erosion of privacy rights, as Varian’s language
suggests. In the conventional narrative of the privacy threat,
institutional secrecy has grown, and individual privacy rights
have been eroded. But that framing is misleading, because
privacy and secrecy are not opposites but rather moments in a
sequence. Secrecy is an effect of privacy, which is its cause.
Exercising one’s right to privacy produces choice, and one can","[305.71826086956514, 71.80770186335405, 553.4698136645962, 747.087204968944]"
0002_jit.2015.5.pdf,9,body,"choose to keep something secret or to share it. Privacy rights
thus confer decision rights; privacy enables a decision as to
where one wants to be on the spectrum between secrecy and
transparency in each situation. US Supreme Court Justice
Douglas articulated this view of privacy in 1967: ‘Privacy
involves the choice of the individual to disclose or to reveal
what he believes, what he thinks, what he possesses …’
(Warden v. Hayden, 387 US 294,323, 1967, Douglas, J.,
dissenting, quoted in Farahany, 2012: 1271).
The work of surveillance, it appears, is not to erode privacy
rights but rather to redistribute them. Instead of many people
having some privacy rights, these rights have been concen-
trated within the surveillance regime. Surveillance capitalists
have extensive privacy rights and therefore many opportu-
nities for secrets. These are increasingly used to deprive
populations of choice in the matter of what about their lives
remains secret. This concentration of rights is accomplished in
two ways. In the case of Google, Facebook, and other
exemplars of surveillance capitalism, many of their rights
appear to come from taking others’ without asking – in
conformance with the Street View model. Surveillance capi-
talists have skillfully exploited a lag in social evolution as the
rapid development of their abilities to surveil for profit outrun
public understanding and the eventual development of law
and regulation that it produces. In result, privacy rights, once
accumulated and asserted, can then be invoked as legitimation
for maintaining the obscurity of surveillance operations.10
The mechanisms of this growing concentration of privacy
rights and its implications received significant scrutiny from
legal scholars in the US and Europe, even before Edward
Snowden accelerated the discussion. This is a rich and growing
literature that raises many substantial concerns associated
with the anti-democratic implications of the concentration of
privacy rights among private and public surveillance actors
(Schwartz, 1989; Solove, 2007; Michaels, 2008; Palfrey, 2008;
Semitsu, 2011; Richards, 2013; Calo, 2014; Reidenberg, 2014;
Richards and King, 2014). The global reach and implications
of this extraction of rights – as well as data – present many
challenges for conceptualization, including how to overcome
the very secrecy that makes them problematic in the first
place. Further, the dynamics I describe occur in what was until
quite recently a blank area – one that is not easily captured by
our existing social, economic, and political categories. The
new business operations frequently elude existing mental
models and defy conventional expectations.
These arguments suggest that the logic of accumulation that
undergirds surveillance capitalism is not wholly captured by the
conventional institutional terrain of the private firm. What is
accumulated here is not only surveillance assets and capital, but
also rights. This occurs through a unique assemblage of business
processes that operate outside the auspices of legitimate demo-
cratic mechanisms or the traditional market pressures of con-
sumer reciprocity and choice. It is accomplished through a form
of unilateral declaration that most closely resembles the social
relations of a pre-modern absolutist authority. In the context of
this new market form that I call surveillance capitalism, hypers-
cale becomes a profoundly anti-democratic threat.
Surveillance capitalism thus qualifies as a new logic of
accumulation with a new politics and social relations that
replaces contracts, the rule of law, and social trust with the
sovereignty of Big Other. It imposes a privately administered
compliance regime of rewards and punishments that is","[41.86732919254653, 70.91329192546586, 290.51329192546575, 748.8760248447204]"
0002_jit.2015.5.pdf,9,body,"sustained by a unilateral redistribution of rights. Big Other
exists in the absence of legitimate authority and is largely free
from detection or sanction. In this sense Big Other may be
described as an automated coup from above: not a coup d’état,
but rather a coup des gens.","[305.71826086956514, 70.91329192546586, 552.5754037267079, 122.78906832298136]"
0002_jit.2015.5.pdf,9,heading,Personalization and communication,"[304.823850931677, 146.04372670807453, 448.823850931677, 155.8822360248447]"
0002_jit.2015.5.pdf,9,body,"Varian claims that ‘nowadays, people have come to expect
personalized search results and ads.’ He says that Google
wants to do even more. Instead of having to ask Google
questions, it should ‘know what you want and tell you before
you ask the question.’ ‘That vision,’ he asserts, ‘has now been
realized by Google Now …’ Varian concedes that ‘Google
Now has to know a lot about you and your environment to
provide these services. This worries some people’ (2014: 28).
However, Varian reasons that people share such knowledge
with doctors, lawyers, and accountants whom they trust. He
then continues, ‘Why am I willing to share all this private
information? Because I get something in return …’ (2014: 28).
In fact, surveillance capitalism is the precise opposite of the
trust-based relationships to which Varian refers. Doctors,
attorneys, and other trusted professionals are held to account
by mutual dependencies and reciprocities overlain by the force
of professional sanction and public law. Google, as we have seen,
does not bear such burdens. Its formal indifference and distance
from ‘users,’ combined with its current freedom from mean-
ingful regulation, sanction, or law, buffer it and other surveil-
lance capitalists from the consequences of mistrust. Instead of
Varian’s implied reciprocities, the coup des gens introduces
substantial new asymmetries of knowledge and power.
For example, Google knows far more about its populations
than they know about themselves. Indeed, there are no means
by which populations can cross this divide, given the material,
intellectual, and proprietary hurdles required for data analysis
and the absence of feedback loops. Another asymmetry is
reflected in the fact that the typical user has little or no
knowledge of Google’s business operations, the full range of
personal data that they contribute to Google’s servers, the
retention of those data, or how those data are instrumentalized
and monetized. It is by now well known that users have few
meaningful options for privacy self-management (for a recent
review of the ‘consent dilemma,’ see Solove, 2013). Surveil-
lance capitalism thrives on the public’s ignorance.
These asymmetries in knowledge are sustained by asymme-
tries of power. Big Other is institutionalized in the automatic
undetectable functions of a global infrastructure that is also
regarded by most people as essential for basic social participa-
tion. The tools on offer by Google and other surveillance
capitalist firms respond to the needs of beleaguered second
modernity individuals – like the apple in the garden, once
tasted they are impossible to live without. When Facebook
crashed in some US cities for a few hours during the summer
of 2014, many Americans called their local emergency services
at 911 (LA Times, 2014). Google’s tools are not the objects of a
value exchange. They do not establish constructive producer-
consumer reciprocities. Instead they are the ‘hooks’ that lure
users into extractive operations and turn ordinary life into the
daily renewal of a 21st-century Faustian pact. This social
dependency is at the heart of the surveillance project. Powerful
felt needs for effective life vie against the inclination to resist
the surveillance project. This conflict produces a kind of","[306.6126708074534, 156.7766459627329, 553.4698136645962, 748.8760248447204]"
0002_jit.2015.5.pdf,10,body,"psychic numbing that inures people to the realities of being
tracked, parsed, mined, and modified – or disposes them to
rationalize the situation in resigned cynicism (Hoofnagle et al.,
2010). The key point here is that this Faustian deal is
fundamentally illegitimate; it is a choice that –21st-century
individuals should not have to make. In the world of
surveillance capitalism, the Faustian pact required to ‘get
something in return’ eliminates the older entanglements of
reciprocity and trust in favor of a wary resentment, frustra-
tion, active defense, and, or, desensitization.
Varian’s confidence in Google Now appears to be buoyed
by the facts of inequality. He counsels that the way to predict
the future is to observe what rich people have, because that is
what the middle class and the poor will want too. ‘What do
rich people have now?’ he asks. ‘Personal assistants’ is his
answer. The solution? ‘That’s Google Now (2014: 29),’ he says.
Varian’s bet is that Google Now will be so vital a resource in
the struggle for effective life that ordinary people will accede to
the ‘invasions of privacy’ that are its quid pro quo.
In this formulation Varian exploits a longstanding insight
of capitalism but bends it to the objectives of the surveillance
project. Adam Smith wrote insightfully on the evolution of
luxuries into necessities. Goods in use among the upper class
and deemed to be luxuries can in time be recast as ‘neces-
saries,’ he noted. The process occurs as ‘the established rules of
decency’ change to reflect new customs and patterns intro-
duced by elites. These changing rules both reflect and trigger
new lower cost production methods that transform former
luxuries into affordable necessities (Smith, 1994: 938–939).
Scholars of early modern consumption describe the ‘consumer
boom’ that ignited the first industrial revolution in late 18th-
century Britain as new middle-class families began to buy the
sorts of goods – china, furniture, textiles – that only the rich
had enjoyed. Historian Neil McKendrick describes this new
‘propensity to consume … unprecedented in the depth to
which
it penetrated
the
lower
reaches
of society
…’
(McKendrick, 1982: 11) as luxuries were reinterpreted as
‘decencies’ and those were reinterpreted as ‘necessities’
(Weatherill, 1993). In 1767, the political economist Nathaniel
Forster worried that ‘fashionable luxury’ was spreading ‘like a
contagion,’ as he complained of the ‘perpetual restless ambi-
tion in each of the inferior ranks to raise themselves to the
level of those immediately above them’ (Forster, 1767: 41).
Historically, this powerful evolutionary characteristic of
demand led to the expansion of production, jobs, higher
wages, and lower cost goods. Varian has no such reciprocities
in mind. Instead, he regards this mechanism of demand
growth as the inevitable force that will push ordinary people
into Google Now’s Faustian pact of ‘necessaries’ in return for
surveillance assets.
Varian is confident that psychic numbing will ease the way
for this unsavory drama. He writes, ‘Of course there will be
challenges. But these digital assistants will be so useful that
everyone will want one, and the statements you read today
about them will just seem quaint and old fashioned’ (2014:
29). But perhaps not. There is a growing body of evidence to
suggest that people in many countries may resist the coup des
gens as trust in the surveillance capitalists is hollowed out by
fresh outbreaks of evidence that suggest the remorseless
prospect of Varian’s future society. These issues are now a
matter of serious political debate within Germany and the EU
where proposals to ‘break up’ Google are already being","[41.86732919254653, 70.91329192546586, 289.6188819875776, 748.8760248447204]"
0002_jit.2015.5.pdf,10,body,"discussed (Mance et al., 2014; see also Barker and Fontanella-
Khan, 2014; Döpfner, 2014; Gabriel, 2014; Vasagar, 2014).
A recent survey by the Financial Times indicates that both
Europeans and Americans are substantially altering their
online behavior as they seek more privacy (Kwong, 2014).
One group of scholars behind a major study of youth online
behavior concludes that a ‘lack of knowledge’ rather than a
‘cavalier attitude toward privacy,’ as tech leaders have alleged,
is an important reason why large numbers of youth ‘engage
with the digital world in a seemingly unconcerned manner’
(Hoofnagle et al., 2010). New legal scholarship reveals the
consumer harm in lost privacy associated with Google and
surveillance capitalism (Newman, 2014). WikiLeaks founder,
Julian Assange, has published a sobering account of Google’s
leadership, politics, and global ambitions (Assange, 2014).
The PEW Research Center’s latest report on public percep-
tions of privacy in the post-Snowden Era indicates that 91% of
US adults agree or strongly agree that consumers have lost
control over their personal data, while only 55% agree or
strongly agree that they are willing to ‘share some information
about myself with companies in order to use online services
for free’ (Madden, 2014).","[304.823850931677, 70.01888198757767, 554.3642236024846, 309.72074534161493]"
0002_jit.2015.5.pdf,10,heading,"Continuous experiments
Becausebig data analy","[306.6126708074534, 332.08099378881985, 402.31453416149066, 342.81391304347824]"
0002_jit.2015.5.pdf,10,body,"Because ‘big data’ analysis yields only correlational patterns,
Varian advises the need for continuous experiments that can
tease out issues of causality. Such experiments ‘are easy to do
on the web,’ assigning treatment and control groups based
on traffic, cookies, usernames, geographic areas, and so on
(2014: 29). Google has been so successful at experimentation
that they have shared their techniques with advertisers and
publishers. Facebook has consistently made inroads here too,
as it conducts experiments in modifying users’ behavior with
a view to eventually monetizing its knowledge, predictive
capability, and control. Whenever these experiments have
been revealed, however, they have ignited fierce public debate
(Bond et al., 2012; Flynn, 2014; Gapper, 2014; Goel, 2014;
Kramer et al., 2014; Lanier, 2014; Zittrain, 2014).
Varian’s enthusiasm for experimentation speaks to a larger
point, however. The business opportunities associated with
the new data flows entail a shift from the a posteriori analysis
to which Constantiou and Kallinikos (2014) refer, to the real-
time observation, communication, analysis, prediction, and
modification of actual behavior now and soon (Foroohar,
2014; Gibbs, 2014; Lin, 2014; Trotman, 2014; Waters, 2014).
This entails another shift in the source of surveillance assets
from virtual behavior to actual behavior, while monetization
opportunities are refocused to blend virtual and actual beha-
vior. This is a new business frontier comprised of knowledge
about real-time behavior that creates opportunities to inter-
vene in and modify behavior for profit. The two entities at the
vanguard of this new wave of ‘reality mining,’ ‘patterns of life
analysis,’ and ‘predictive analytics’ are Google and the NSA.
As the White House report puts it, ‘there is a growing
potential for big data analytics to have an immediate effect
on a person’s surrounding environment or decisions being
made about his or her life’ (2014: 5). This is what I call the
reality business, and it reflects an evolution in the frontier of
data science from data mining to reality mining in which,
according to MIT’s Sandy Pentland, ‘sensors, mobile phones,
and other data capture devices’ provide the ‘eyes and ears’ of a","[306.6126708074534, 343.70832298136645, 553.4698136645962, 747.087204968944]"
0003_s41586-022-05543-x.pdf,1,title,"Papers and patents are becoming less 
disruptive over time","[39.184099378881946, 48.007453416149076, 476.55055900621113, 100.77763975155281]"
0003_s41586-022-05543-x.pdf,1,author,"Michael Park1,","[217.17167701863352, 144.60372670807453, 268.1530434782608, 156.23105590062113]"
0003_s41586-022-05543-x.pdf,1,author, Erin Leahey2,"[270.8362732919254, 143.70931677018632, 314.66236024844716, 154.4422360248447]"
0003_s41586-022-05543-x.pdf,1,author, Russell J. Funk1 ,"[325.39527950310554, 145.49813664596275, 382.6375155279503, 154.4422360248447]"
0003_s41586-022-05543-x.pdf,1,abstract,"Theories of scientific and technological change view discovery and invention as 
endogenous processes1,2, wherein previous accumulated knowledge enables future 
progress by allowing researchers to, in Newton’s words, ‘stand on the shoulders of 
giants’3–7. Recent decades have witnessed exponential growth in the volume of new 
scientific and technological knowledge, thereby creating conditions that should be 
ripe for major advances8,9. Yet contrary to this view, studies suggest that progress is 
slowing in several major fields10,11. Here, we analyse these claims at scale across six 
decades, using data on 45 million papers and 3.9 million patents from six large-scale 
datasets, together with a new quantitative metric—the CD index12—that characterizes 
how papers and patents change networks of citations in science and technology.  
We find that papers and patents are increasingly less likely to break with the past in 
ways that push science and technology in new directions. This pattern holds universally 
across fields and is robust across multiple different citation- and text-based metrics1,13–17. 
Subsequently, we link this decline in disruptiveness to a narrowing in the use of previous 
knowledge, allowing us to reconcile the patterns we observe with the ‘shoulders of 
giants’ view. We find that the observed declines are unlikely to be driven by changes in 
the quality of published science, citation practices or field-specific factors. Overall, 
our results suggest that slowing rates of disruption may reflect a fundamental shift in 
the nature of science and technology.","[217.17167701863352, 175.90807453416147, 560.6250931677018, 420.9763975155279]"
0003_s41586-022-05543-x.pdf,1,date, 4 January 2023,"[105.37043478260864, 192.0074534161491, 164.40149068322978, 203.63478260869564]"
0003_s41586-022-05543-x.pdf,1,body,"Although the past century witnessed an unprecedented expan-
sion of scientific and technological knowledge, there are concerns
that innovative activity is slowing18–20. Studies document declining
research productivity in semiconductors, pharmaceuticals and other
fields10,11. Papers, patents and even grant applications have become less
novel relative to prior work and less likely to connect disparate areas
of knowledge, both of which are precursors of innovation21,22. The gap
between the year of discovery and the awarding of a Nobel Prize has also
increased23,24, suggesting that today’s contributions do not measure
up to the past. These trends have attracted increasing attention from
policymakers, as they pose substantial threats to economic growth,
human health and wellbeing, and national security, along with global
efforts to combat grand challenges such as climate change25,26.
Numerous explanations for this slowdown have been proposed.
Some point to a dearth of ‘low-hanging fruit’ as the readily available
productivity-enhancing innovations have already been made19,27. Others
emphasize the increasing burden of knowledge; scientists and inven-
tors require ever more training to reach the frontiers of their fields,
leaving less time to push those frontiers forward18,28. Yet much remains
unknown, not merely about the causes of slowing innovative activity,
but also the depth and breadth of the phenomenon. The decline is
difficult to reconcile with centuries of observation by philosophers of
science, who characterize the growth of knowledge as an endogenous
process, wherein previous knowledge enables future discovery, a view
captured famously in Newton’s observation that if he had seen further,
it was by ‘standing on the shoulders of giants’3. Moreover, to date, the","[38.289689440993754, 443.33664596273286, 294.09093167701855, 724.1813664596273]"
0003_s41586-022-05543-x.pdf,1,body,"evidence pointing to a slowdown is based on studies of particular fields, 
using disparate and domain-specific metrics10,11, making it difficult to 
know whether the changes are happening at similar rates across areas 
of science and technology. Little is also known about whether the pat-
terns seen in aggregate indicators mask differences in the degree to 
which individual works push the frontier.
We address these gaps in understanding by analysing 25 million 
papers (1945–2010) in the Web of Science (WoS) (Methods) and 3.9 mil-
lion patents (1976–2010) in the United States Patent and Trademark 
Office’s (USPTO) Patents View database (Methods). The WoS data 
include 390 million citations, 25 million paper titles and 13 million 
abstracts. The Patents View data include 35 million citations, 3.9 million 
patent titles and 3.9 million abstracts. Subsequently, we replicate our 
core findings on four additional datasets—JSTOR, the American Physical 
Society corpus, Microsoft Academic Graph and PubMed—encompass-
ing 20 million papers. Using these data, we join a new citation-based 
measure12 with textual analyses of titles and abstracts to understand 
whether papers and patents forge new directions over time and across 
fields.","[305.71826086956514, 445.12546583850934, 561.51950310559, 649.0509316770186]"
0003_s41586-022-05543-x.pdf,1,heading,Measurement of disruptiveness,"[305.71826086956514, 668.727950310559, 454.19031055900615, 679.4608695652174]"
0003_s41586-022-05543-x.pdf,1,body,"To characterize the nature of innovation, we draw on foundational 
theories of scientific and technological change2,29,30, which distinguish 
between two types of breakthroughs. First, some contributions improve 
existing streams of knowledge, and therefore consolidate the status ","[305.71826086956514, 682.144099378882, 563.3083229813665, 725.0757763975155]"
0003_s41586-022-05543-x.pdf,2,body,"quo. Kohn and Sham (1965)31, a Nobel-winning paper used established 
theorems to develop a method for calculating the structure of elec-
trons, which cemented the value of previous research. Second, some 
contributions disrupt existing knowledge, rendering it obsolete, and 
propelling science and technology in new directions. Watson and Crick 
(1953)32, also a Nobel winner, introduced a model of the structure of 
DNA that superseded previous approaches (for example, Pauling’s 
triple helix). Kohn and Sham and Watson and Crick were both impor-
tant, but their implications for scientific and technological change 
were different.
We quantify this distinction using a measure—the CD index12—that 
characterizes the consolidating or disruptive nature of science and 
technology (Fig. 1). The intuition is that if a paper or patent is disrup-
tive, the subsequent work that cites it is less likely to also cite its prede-
cessors; for future researchers, the ideas that went into its production 
are less relevant (for example, Pauling’s triple helix). If a paper or patent 
is consolidating, subsequent work that cites it is also more likely to 
cite its predecessors; for future researchers, the knowledge upon 
which the work builds is still (and perhaps more) relevant (for exam-
ple, the theorems Kohn and Sham used). The CD index ranges from −1  
(consolidating) to 1 (disruptive). We measure the CD index five years 
after the year of each paper’s publication (indicated by CD5, see 
Extended Data Fig. 1 for the distribution of CD5 among papers and  
patents and Extended Data Fig. 2 for analyses using alternative 
windows)33. For example, Watson and Crick and Kohn and Sham both 
received over a hundred citations within five years of being published. 
However, the Kohn and Sham paper has a CD5 of −0.22 (indicating 
consolidation), whereas the Watson and Crick paper has a CD5 of 
0.62 (indicating disruption). The CD index has been validated exten-
sively in previous research, including through correlation with expert  
assessments12,34.","[39.184099378881946, 414.7155279503105, 296.77416149068324, 747.4360248447205]"
0003_s41586-022-05543-x.pdf,2,heading,Declining disruptiveness,"[305.71826086956514, 421.87080745341603, 422.8859627329192, 432.60372670807453]"
0003_s41586-022-05543-x.pdf,2,body,"Across fields, we find that science and technology are becoming less 
disruptive. Figure 2 plots the average CD5 over time for papers (Fig. 2a) 
and patents (Fig. 2b). For papers, the decrease between 1945 and 2010 
ranges from 91.9% (where the average CD5 dropped from 0.52 in 1945 
to 0.04 in 2010 for ‘social sciences’) to 100% (where the average CD5 
decreased from 0.36 in 1945 to 0 in 2010 for ‘physical sciences’); for pat-
ents, the decrease between 1980 and 2010 ranges from 78.7% (where the 
average CD5 decreased from 0.30 in 1980 to 0.06 in 2010 for ‘computers 
and communications’) to 91.5% (where the average CD5 decreased from 
0.38 in 1980 to 0.03 in 2010 for ‘drugs and medical’). For both papers 
and patents, the rates of decline are greatest in the earlier parts of the 
time series, and for patents, they appear to begin stabilizing between 
the years 2000 and 2005. For papers, since about 1980, the rate of 
decline has been more modest in ‘life sciences and biomedicine’ and 
physical sciences, and most marked and persistent in social sciences 
and ‘technology’. Overall, however, relative to earlier eras, recent papers 
and patents do less to push science and technology in new directions. 
The general similarity in trends we observe across fields is noteworthy 
in light of ‘low-hanging fruit’ theories19,27, which would probably predict 
greater heterogeneity in the decline, as it seems unlikely fields would 
‘consume’ their low-hanging fruit at similar rates or times.","[304.823850931677, 434.3925465838509, 563.3083229813665, 658.8894409937888]"
0003_s41586-022-05543-x.pdf,2,heading,Linguistic change,"[304.823850931677, 678.5664596273292, 389.79279503105585, 690.1937888198757]"
0003_s41586-022-05543-x.pdf,2,body,"The decline in disruptive science and technology is also observable 
using alternative indicators. Because they create departures from 
the status quo, disruptive papers and patents are likely to introduce 
new words (for example, words used to create a new paradigm might 
differ from those that are used to develop an existing paradigm)35,36. ","[305.71826086956514, 691.9826086956521, 563.3083229813665, 746.5416149068323]"
0003_s41586-022-05543-x.pdf,11,body,"In addition, we also include controls for the ‘mean age of team mem-
bers’ (that is, ‘career age’, defined as the difference between the publi-
cation year of the focal paper or patent and the first year in which each 
author or inventor published a paper or patent) and the ‘mean number 
of previous works produced by team members’. Although increases 
in rates of self-citations may indicate that scientists and inventors 
are becoming more narrowly focused on their own work, these rates 
may also be driven in part by the amount of previous work available 
for self-citing. Similarly, although increases in the age of work cited 
in papers and patents may indicate that scientists and inventors are 
struggling to keep up, they may also be driven by the rapidly aging 
workforce in science and technology78,79. For example, older scientists 
and inventors may be more familiar with or more attentive to older 
work, or may actively resist change80. These control variables help to 
account for these alternative explanations.
Supplementary Table 3 shows summary statistics for variables used in 
the ordinary-least-squares regression models. The diversity of work cited 
is measured by normalized entropy, which ranges from 0 to 1. Greater 
values on this measure indicate a more uniform distribution of citations 
to a wider range of existing work; lower values indicate a more concen-
trated distribution of citations to a smaller range of existing work. The 
tables show that the normalized entropy in a given field and year has a 
nearly maximal average entropy of 0.98 for both science and technol-
ogy. About 16% of papers cited in a paper are by an author of the focal 
paper; the corresponding number for patents is about 7%. Papers tend 
to rely on older work and work that varies more greatly in age (measured 
by standard deviation) than patents. In addition, the average CD5 of a 
paper is 0.04 whereas the average CD5 of a patent is 0.12, meaning that 
the average paper tends to be less disruptive than the average patent.
We find that using more diverse work, less of one’s own work and 
older work tends to be associated with the production of more disrup-
tive science and technology, even after accounting for the average age 
and number of previous works produced by team members. These 
findings are based on our regression results, shown in Extended Data 
Table 1. Models 6 and 12 present the full regression models. The models 
indicate a consistent pattern for both science and technology, wherein 
the coefficients for diversity of work cited are positive and significant 
for papers (0.159, P < 0.01) and patents (0.069, P < 0.01), indicating that 
in fields in which there is more use of diverse work, there is greater 
disruption. Holding all other variables at their means, the predicted 
CD5 of papers and patents increases by 303.5% and 1.3%, respectively, 
when the diversity of work cited increases by 1 s.d. The coefficients of 
the ratio of self-citations to total work cited is negative and significant 
for papers (−0.011, P < 0.01) and patents (−0.060, P < 0.01), showing that 
when researchers or inventors rely more on their own work, discovery 
and invention tends to be less disruptive. Again holding all other vari-
ables at their means, the predicted CD5 of papers and patents decreases 
by 622.9% and 18.5%, respectively, with a 1 s.d. increase in the ratio. The 
coefficients of the interaction between mean age of work cited and 
dispersion in age of work cited is positive and significant for papers 
(0.000, P < 0.01) and patents (0.001, P < 0.01), suggesting that—holding 
the dispersion of the age of work cited constant—papers and patents 
that engage with older work are more likely to be disruptive. The pre-
dicted CD5 of papers and patents increases by a striking 2,072.4% and 
58.4%, respectively, when the mean age of work cited increases by 1 s.d. 
(about nine and eight years for papers and patents, respectively), again 
holding all other variables at their means. In summary, the regression 
results suggest that changes in the use of previous knowledge may 
contribute to the production of less disruptive science and technology.","[37.395279503105556, 47.11304347826088, 296.77416149068324, 683.0385093167702]"
0003_s41586-022-05543-x.pdf,11,heading,Data availability,"[304.823850931677, 53.373913043478275, 384.4263354037267, 67.68447204968945]"
0003_s41586-022-05543-x.pdf,11,body,"Data associated with this study are freely available in a public reposi-
tory at https://doi.org/10.5281/zenodo.7258379. Our study draws on 
data from six sources: the American Physical Society, JSTOR, Microsoft 
Academic Graph, Patents View, PubMed and WoS. Data from Microsoft 
Academic Graph, Patents View and PubMed are publicly available, 
and our repository includes complete data for analyses from these 
sources. Data from the American Physical Society, JSTOR and WoS are 
not publicly available, and were used under licence from their respec-
tive publishers. To facilitate replication, our repository includes limited 
versions of the data from these sources, which will enable calculation of 
basic descriptive statistics. The authors will make full versions of these 
data available upon request and with permission from their respective 
publishers. Source data are provided with this paper.","[305.71826086956514, 68.57888198757766, 562.4139130434781, 209.00124223602484]"
0003_s41586-022-05543-x.pdf,11,references,,"[80.32695652173909, 240.30559006211175, 149.19652173913045, 229.5726708074534]"
0003_s41586-022-05543-x.pdf,3,body,"Therefore, if disruptiveness is declining, we would expect a decline 
in the diversity of words used in science and technology. To evaluate 
this, Fig. 3a,d documents the type-token ratio (that is, unique/total 
words) of paper and patent titles over time (Supplementary Informa-
tion section 1). We observe substantial declines, especially in the ear-
lier periods, before 1970 for papers and 1990 for patents. For paper 
titles (Fig. 3a), the decrease (1945–2010) ranges from 76.5% (social 
sciences) to 88% (technology); for patent titles (Fig. 3d), the decrease 
(1980–2010) ranges from 32.5% (chemical) to 81% (computers and 
communications). For paper abstracts (Extended Data Fig. 3a), the 
decrease (1992–2010) ranges from 23.1% (life sciences and biomedicine) 
to 38.9% (social sciences); for patent abstracts (Extended Data Fig. 3b), 
the decrease (1980–2010) ranges from 21.5% (mechanical) to 73.2% 
(computers and communications). In Fig. 3b,e, we demonstrate that 
these declines in word diversity are accompanied by similar declines in 
combinatorial novelty; over time, the particular words that scientists 
and inventors use in the titles of their papers and patents are increas-
ingly likely to have been used together in the titles of previous work. 
Consistent with these trends in language, we also observe declining nov-
elty in the combinations of previous work cited by papers and patents, 
based on a previously established measure of ‘atypical combinations’14 
(Extended Data Fig. 4).
The decline in disruptive activity is also apparent in the specific 
words used by scientists and inventors. If disruptiveness is declining, 
we reasoned that verbs alluding to the creation, discovery or percep-
tion of new things should be used less frequently over time, whereas 
verbs alluding to the improvement, application or assessment of exist-
ing things may be used more often35,36. Figure 3 shows the most com-
mon verbs in paper (Fig. 3c) and patent titles (Fig. 3f) in the first and 
last decade of each sample (Supplementary Information section 2). 
Although precisely and quantitatively characterizing words as ‘con-
solidating’ or ‘disruptive’ is challenging in the absence of context, the 
figure highlights a clear and qualitative shift in language. In the earlier 
decades, verbs evoking creation (for example, ‘produce’, ‘form’, ‘pre-
pare’ and ‘make’), discovery (for example, ‘determine’ and ‘report’) 
and perception (for example, ‘measure’) are prevalent in both paper 
and patent titles. In the later decades, however, these verbs are almost 
completely displaced by those tending to be more evocative of the 
improvement (for example, ‘improve’, ‘enhance’ and ‘increase’), appli-
cation (for example, ‘use’ and ‘include’) or assessment (for example, ","[39.184099378881946, 316.3304347826087, 296.77416149068324, 745.647204968944]"
0003_s41586-022-05543-x.pdf,3,body,"‘associate’, ‘mediate’ and ‘relate’) of existing scientific and technologi-
cal knowledge and artefacts. Taken together, these patterns suggest a 
substantive shift in science and technology over time, with discovery 
and invention becoming less disruptive in nature, consistent with our 
results using the CD index.","[305.71826086956514, 316.3304347826087, 562.4139130434781, 369.9950310559006]"
0003_s41586-022-05543-x.pdf,3,heading,Conservation of highly disruptive work,"[303.9294409937888, 388.7776397515528, 489.96670807453415, 400.4049689440994]"
0003_s41586-022-05543-x.pdf,3,body,"The aggregate trends we document mask considerable heterogeneity 
in the disruptiveness of individual papers and patents and remarkable 
stability in the absolute number of highly disruptive works (Methods 
and Fig. 4). Specifically, despite large increases in scientific productiv-
ity, the number of papers and patents with CD5 values in the far right tail 
of the distribution remains nearly constant over time. This ‘conserva-
tion’ of the absolute number of highly disruptive papers and patents 
holds despite considerable churn in the underlying fields responsible 
for producing those works (Extended Data Fig. 5, inset). These results 
suggest that the persistence of major breakthroughs—for example, 
measurement of gravity waves and COVID-19 vaccines—is not incon-
sistent with slowing innovative activity. In short, declining aggregate 
disruptiveness does not preclude individual highly disruptive works.","[305.71826086956514, 402.19378881987575, 563.3083229813665, 541.7217391304348]"
0003_s41586-022-05543-x.pdf,3,heading,Alternative explanations,"[306.6126708074534, 560.5043478260869, 423.78037267080737, 571.2372670807454]"
0003_s41586-022-05543-x.pdf,3,body,"What is driving the decline in disruptiveness? Earlier, we suggested our 
results are not consistent with explanations that link slowing innova-
tive activity to diminishing ‘low-hanging fruit’. Extended Data Fig. 5 
shows that the decline in disruptiveness is unlikely to be due to other 
field-specific factors by decomposing variation in CD5 attributable to 
field, author and year effects (Methods).
Declining rates of disruptive activity are unlikely to be caused by the 
diminishing quality of science and technology22,37. If they were, then 
the patterns seen in Fig. 2 should be less visible in high-quality work. 
However, when we restrict our sample to articles published in premier 
publication venues such as Nature, Proceedings of the National Academy 
of Sciences and Science or to Nobel-winning discoveries38 (Fig. 5), the 
downward trend persists.
Furthermore, the trend is not driven by characteristics of the WoS and 
UPSTO data or our particular derivation of the CD index; we observe 
similar declines in disruptiveness when we compute CD5 on papers ","[305.71826086956514, 573.9204968944099, 562.4139130434781, 746.5416149068323]"
0003_s41586-022-05543-x.pdf,5,body,"in JSTOR, the American Physical Society corpus, Microsoft Academic 
Graph and PubMed (Methods), the results of which are shown in 
Extended Data Fig. 6. We further show that the decline is not an arte-
fact of the CD index by reporting similar patterns using alternative 
derivations13,15 (Methods and Extended Data Fig. 7).
Declines in disruptiveness are also not attributable to changing 
publication, citation or authorship practices (Methods). First, using 
approaches from the bibliometrics literature39–43, we computed several 
normalized versions of the CD index that adjusted for the increasing 
tendency for papers and patents to cite previous work44,45. Results using 
these alternative indicators (Extended Data Fig. 8a,d) were similar to 
those we reported in Fig. 2. Second, using regression, we estimated 
models of CD5 as a function of indicator variables for each paper or 
patent’s publication year, along with specific controls for field × year 
level—number of new papers/patents, mean number of papers/patents  
cited, mean number of authors or inventors per paper—and paper or 
patent-level—number of papers or patents cited—factors. Predictions 
from these models indicated a decline in disruptive papers and patents 
(Extended Data Fig. 8b,e and Supplementary Table 1) that was con-
sistent with our main results. Finally, using Monte Carlo simulations,  ","[38.289689440993754, 531.8832298136646, 295.8797515527951, 745.647204968944]"
0003_s41586-022-05543-x.pdf,5,body,"we randomly rewired the observed citation networks while preserving 
key characteristics of scientists’ and inventors’ citation behaviour, 
including the number of citations made and received by individual 
papers and patents and the age gap between citing and cited works. 
We find that observed CD5 values are lower than those from the simu-
lated networks (Extended Data Fig. 8c,f), and the gap is widening: over 
time, papers and patents are increasingly less disruptive than would be 
expected by chance. Taken together, these additional analyses indicate 
that the decline in CD5 is unlikely to be driven by changing publication, 
citation or authorship practices.","[304.823850931677, 47.11304347826088, 562.4139130434781, 156.23105590062113]"
0003_s41586-022-05543-x.pdf,5,heading,Growth of knowledge and disruptiveness,"[305.71826086956514, 172.3304347826087, 496.2275776397516, 184.8521739130435]"
0003_s41586-022-05543-x.pdf,5,body,"We also considered how declining disruptiveness relates to the growth 
of knowledge (Extended Data Fig. 9). On the one hand, scientists and 
inventors face an increasing knowledge burden, which may inhibit 
discoveries and inventions that disrupt the status quo. On the other 
hand, as previously noted, philosophers of science suggest that exist-
ing knowledge fosters discovery and invention3,6,7. Using regression 
models, we evaluated the relationship between the stock of papers 
and patents (a proxy for knowledge) within fields and their CD5 (Sup-
plementary Information section 3 and Supplementary Table 2).  
We find a positive effect of the growth of knowledge on disruptive-
ness for papers, consistent with previous work20; however, we find a 
negative effect for patents.
Given these conflicting results, we considered the possibility that 
the availability of knowledge may differ from its use. In particular, the 
growth in publishing and patenting may lead scientists and inventors 
to focus on narrower slices of previous work18,46, thereby limiting the 
‘effective’ stock of knowledge. Using three proxies, we document a 
decline in the use of previous knowledge among scientists and inventors 
(Fig. 6). First, we see a decline in the diversity of work cited (Fig. 6a,d), 
indicating that contemporary science and technology are engaging 
with narrower slices of existing knowledge. Moreover, this decline in 
diversity is accompanied by an increase in the share of citations to the 
1% most highly cited papers and patents (Fig. 6a (i),d(i)), which are also 
decreasing in semantic diversity (Fig. 6a (ii),d (ii)). Over time, scientists 
and inventors are increasingly citing the same previous work, and that 
previous work is becoming more topically similar. Second, we see an 
increase in self-citation (Fig. 6b,e), a common proxy for the continua-
tion of one’s pre-existing research stream47–49, which is consistent with 
scientists and inventors relying more on highly familiar knowledge. 
Third, the mean age of work cited, a common measure for the use of 
dated knowledge50–52, is increasing (Fig. 6c,f), suggesting that scientists 
and inventors may be struggling to keep up with the pace of knowledge 
expansion and instead relying on older, familiar work. All three indica-
tors point to a consistent story: a narrower scope of existing knowledge 
is informing contemporary discovery and invention.
Results from a subsequent series of regression models suggest that 
use of less diverse work, more of one’s own work and older work are all 
negatively associated with disruption (Methods, Extended Data Table 1 
and Supplementary Table 3), a pattern that holds even after accounting 
for the average age and number of previous works produced by team 
members. When the range of work used by scientists and inventors 
narrows, disruptive activity declines.","[305.71826086956514, 186.64099378881988, 562.4139130434781, 637.4236024844721]"
0003_s41586-022-05543-x.pdf,5,heading,Discussion,"[304.823850931677, 657.1006211180124, 357.59403726708075, 668.727950310559]"
0003_s41586-022-05543-x.pdf,5,body,"In summary, we report a marked decline in disruptive science and 
technology over time. Our analyses show that this trend is unlikely to 
be driven by changes in citation practices or the quality of published 
work. Rather, the decline represents a substantive shift in science and 
technology, one that reinforces concerns about slowing innovative 
activity. We attribute this trend in part to scientists’ and inventors’ reli-
ance on a narrower set of existing knowledge. Even though philosophers ","[304.823850931677, 670.5167701863353, 563.3083229813665, 747.4360248447205]"
0003_s41586-022-05543-x.pdf,6,body,"of science may be correct that the growth of knowledge is an endog-
enous process—wherein accumulated understanding promotes future 
discovery and invention—engagement with a broad range of extant 
knowledge is necessary for that process to play out, a requirement 
that appears more difficult with time. Relying on narrower slices of 
knowledge benefits individual careers53, but not scientific progress 
more generally.
Moreover, even though the prevalence of disruptive works has 
declined, we find that the sheer number has remained stable. On the 
one hand, this result may suggest that there is a fixed ‘carrying capac-
ity’ for highly disruptive science and technology, in which case, policy 
interventions aimed at increasing such work may prove challenging. On 
the other hand, our observation of considerable churn in the underly-
ing fields responsible for producing disruptive science and technol-
ogy suggests the potential importance of factors such as the shifting 
interests of funders and scientists and the ‘ripeness’ of scientific and 
technologicalknowledge for breakthroughs, in which case the produc-
tion of disruptive work may be responsive to policy levers. In either 
case, the stability we observe in the sheer number of disruptive papers ","[39.184099378881946, 543.5105590062111, 295.8797515527951, 747.4360248447205]"
0003_s41586-022-05543-x.pdf,6,body,"and patents suggests that science and technology do not appear to 
have reached the end of the ‘endless frontier’. Room remains for the 
regular rerouting that disruptive works contribute to scientific and 
technological progress.
Our study is not without limitations. Notably, even though research 
to date supports the validity of the CD index12,34, it is a relatively new 
indicator of innovative activity and will benefit from future work on 
its behaviour and properties, especially across data sources and con-
texts. Studies that systematically examine the effect of different citation 
practices54,55, which vary across fields, would be particularly informative.
Overall, our results deepen understanding of the evolution of knowl-
edge and may guide career planning and science policy. To promote 
disruptive science and technology, scholars may be encouraged to read 
widely and given time to keep up with the rapidly expanding knowl-
edge frontier. Universities may forgo the focus on quantity, and more 
strongly reward research quality56, and perhaps more fully subsidize 
year-long sabbaticals. Federal agencies may invest in the riskier and 
longer-term individual awards that support careers and not simply spe-
cific projects57, giving scholars the gift of time needed to step outside ","[304.823850931677, 542.6161490683229, 562.4139130434781, 746.5416149068323]"
0003_s41586-022-05543-x.pdf,7,body,"the fray, inoculate themselves from the publish or perish culture, and 
produce truly consequential work. Understanding the decline in dis-
ruptive science and technology more fully permits a much-needed 
rethinking of strategies for organizing the production of science and 
technology in the future.","[39.184099378881946, 48.901863354037275, 295.8797515527951, 101.672049689441]"
0003_s41586-022-05543-x.pdf,8,heading,Methods,"[37.395279503105556, 45.32422360248448, 91.95428571428572, 58.74037267080747]"
0003_s41586-022-05543-x.pdf,8,heading,WoS data,"[38.289689440993754, 66.79006211180126, 79.4325465838509, 78.41739130434783]"
0003_s41586-022-05543-x.pdf,8,body,"We limit our focus to research papers published between 1945 and 2010.
Although the WoS data begin in the year 1900, the scale and social organ-
ization of science shifted markedly in the post-war era, thereby making
comparisons with the present difficult and potentially misleading67–69.
We end our analyses of papers in 2010 because some of our measures
require several subsequent years of data following paper publication.
The WoS data archive 65 million documents published in 28,968 journals
between 1900 and 2017 and 735 million citations among them. In addi-
tion, the WoS data include the titles and the full text of abstracts for 65
and 29 million records, respectively, published between 1913 and 2017.
After eliminating non-research documents (for example, book reviews
and commentaries) and subsetting the data to the 1945–2010 window,
the analytical sample consists of n = 24,659,076 papers.","[39.184099378881946, 80.20621118012423, 294.09093167701855, 219.7341614906832]"
0003_s41586-022-05543-x.pdf,8,heading,Patents View data,"[38.289689440993754, 227.783850931677, 110.73689440993789, 239.41118012422356]"
0003_s41586-022-05543-x.pdf,8,body,"We limit our focus to patents granted from 1976, which is the earliest 
year for which machine-readable records are available in the Patents 
View data. As we did with papers, we end our analyses in 2010 because 
some measures require data from subsequent years for calculation. 
The Patents View data are the most exhaustive source of historical 
data on inventions, with information on 6.5 million patents granted 
between 1976 and 2017 and their corresponding 92 million citations. 
The Patents View data include the titles and abstracts for 6.5 million 
patents granted between 1976 and 2017. Following previous work12, 
we focused our attention on utility patents, which cover the vast major-
ity (91% in our data) of patented inventions. After eliminating non-utility 
patents and subsetting the data to the 1976–2010 window, the analytical 
sample consists of n = 3,912,353 patents.","[39.184099378881946, 242.0944099378882, 295.8797515527951, 379.8335403726708]"
0003_s41586-022-05543-x.pdf,8,heading,Highly disruptive papers and patents,"[39.184099378881946, 390.56645962732915, 188.5505590062111, 400.4049689440994]"
0003_s41586-022-05543-x.pdf,8,body,"Observations (and claims) of slowing progress in science and technol-
ogy are increasingly common, supported not only by the evidence we 
report, but also by previous research from diverse methodological and 
disciplinary perspectives10,11,18–24. Yet as noted in the main text, there 
is a tension between observations of slowing progress from aggre-
gate data on the one hand, and continuing reports of seemingly major 
breakthroughs in many fields of science and technology—spanning 
everything from the measurement of gravity waves to the sequencing 
of the human genome—on the other. In an effort to reconcile this ten-
sion, we considered the possibility that whereas overall, discovery and 
invention may be less disruptive over time, the high-level view taken in 
previous work may mask considerable heterogeneity. Put differently, 
aggregate evidence of slowing progress does not preclude the possibil-
ity that some subset of discoveries and inventions is highly disruptive.
To evaluate this possibility, we plot the number of disruptive papers 
(Fig. 4a) and patents (Fig. 4b) over time, where disruptive papers and 
patents are defined as those with CD5 values >0. Within each panel, we 
plot four lines, corresponding to four evenly spaced intervals—(0, 0.25], 
(0.25, 0.5], (0.5, 0.75], (0.75, 1.00]—over the positive values of CD5. The 
first two intervals therefore correspond to papers and patents that 
are relatively weakly disruptive, whereas the latter two correspond to 
those that are more strongly so (for example, where we may expect to 
see major breakthroughs such as some of those mentioned above). 
Despite major increases in the numbers of papers and patents pub-
lished each year, we see little change in the number of highly disrup-
tive papers and patents, as evidenced by the relatively flat red, green 
and orange lines. Notably, this ‘conservation’ of disruptive work holds 
even despite fluctuations over time in the composition of the scientific 
and technological fields responsible for producing the most disrup-
tive work (Fig. 4, inset plots). Overall, these results help to account for 
simultaneous observations of both major breakthroughs in many fields 
of science and technology and aggregate evidence of slowing progress.","[38.289689440993754, 403.0881987577639, 295.8797515527951, 748.3304347826086]"
0003_s41586-022-05543-x.pdf,8,heading,"Relative contribution of field, year and author or inventor 
effects","[303.9294409937888, 55.162732919254665, 536.4760248447204, 77.52298136645963]"
0003_s41586-022-05543-x.pdf,8,body,"Our results show a steady decline in the disruptiveness of science and 
technology over time. Moreover, the patterns we observe are generally 
similar across broad fields of study, which suggests that the factors 
driving the decline are not unique to specific domains of science and 
technology. The decline could be driven by other factors, such as the 
conditions of science and technology at a point in time or the particular 
individuals who produce science and technology. For example, exog-
enous factors such as economic conditions may encourage research 
or invention practices that are less disruptive. Similarly, scientists and 
inventors of different generations may have different approaches, 
which may result in greater or lesser tendencies for producing disrup-
tive work. We therefore sought to understand the relative contribution 
of field, year and author (or inventor) factors to the decline of disruptive 
science and technology.
To do so, we decomposed the relative contribution of field, year 
and author fixed effects to the predictive power of regression models 
of the CD index. The unit of observation in these regressions is the 
author (or inventor) × year. We enter field fixed effects using granular 
subfield indicators (that is, 150 WoS subject areas for papers, 138 NBER 
subcategories for patents). For simplicity, we did not include additional 
covariates beyond the fixed effects in our models. Field fixed effects 
capture all field-specific factors that do not vary by author or year 
(for example, the basic subject matter); year fixed effects capture all 
year-specific factors that do not vary by field or author (for example, the 
state of communication technology); author (or inventor) fixed effects 
capture all author-specific factors that do not vary by field or year 
(for example, the year of PhD awarding). After specifying our model, 
we determine the relative contribution of field, year and author fixed 
effects to the overall model adjusted R2 using Shapley–Owen decom-
position. Specifically, given our n = 3 groups of fixed effects (field, year 
and author) we evaluate the relative contribution of each set of fixed 
effects by estimating the adjusted R2 separately for the 2n models using 
subsets of the predictors. The relative contribution of each set of fixed 
effects is then computed using the Shapley value from game theory70.
Results of this analysis are shown in Extended Data Fig. 5, for both 
papers (top bar) and patents (bottom bar). Total bar size corresponds 
to the value of the adjusted R2 for the fully specified model (that is, with 
all three groups of fixed effects). Consistent with our observations from 
plots of the CD index over time, we observe that for both papers and 
patents, field-specific factors make the lowest relative contribution to 
the adjusted R2 (0.02 and 0.01 for papers and patents, respectively). 
Author fixed effects, by contrast, appear to contribute much more 
to the predictive power of the model, for both papers (0.20) and pat-
ents (0.17). Researchers and inventors who entered the field in more 
recent years may face a higher burden of knowledge and thus resort 
to building on narrower slices of existing work (for example, because 
of more specialized doctoral training), which would generally lead to 
less disruptive science and technology being produced in later years, 
consistent with our findings. The pattern is more complex for year 
fixed effects; although year-specific factors that do not vary by field or 
author hold more explanatory power than field for both papers (0.02) 
and patents (0.16), they appear to be substantially more important 
for the latter than the former. Taken together, these findings suggest 
that relatively stable factors that vary across individual scientists and 
inventors may be particularly important for understanding changes in 
disruptiveness over time. The results also confirm that domain-specific 
factors across fields of science and technology play a very small role in 
explaining the decline in disruptiveness of papers and patents.","[305.71826086956514, 80.20621118012423, 563.3083229813665, 704.5043478260869]"
0003_s41586-022-05543-x.pdf,8,heading,Alternative samples,"[304.823850931677, 711.6596273291924, 388.0039751552795, 724.1813664596273]"
0003_s41586-022-05543-x.pdf,8,body,"We also considered whether the patterns we document may be artefacts 
of our choice of data sources. Although we observe consistent trends ","[304.823850931677, 725.0757763975155, 562.4139130434781, 748.3304347826086]"
0003_s41586-022-05543-x.pdf,9,body,"in both the WoS and Patents View data, and both databases are widely 
used by the Science of Science community, our results may conceivably 
be driven by factors such as changes in coverage (for example, journals 
added or excluded from WoS over time) or even data errors rather 
than fundamental changes in science and technology. To evaluate this 
possibility, we therefore calculated CD5 for papers in four additional 
databases—JSTOR, the American Physical Society corpus, Microsoft 
Academic Graph and PubMed. We included all records from 1930 to 
2010 from PubMed (16,774,282 papers), JSTOR (1,703,353 papers) 
and American Physical Society (478,373 papers). The JSTOR data were 
obtained via a special request from ITHAKA, the data maintainer (http://
www.ithaka.org), as were the American Physical Society data (https://
journals.aps.org/datasets). We downloaded the Microsoft Academic 
Graph data from CADRE at Indiana University (https://cadre.iu.edu/). 
The PubMed data were downloaded from the National Library of Medi-
cine FTP server (ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline). Owing 
to the exceptionally large scale of Microsoft Academic Graph and the 
associated computational burden, we randomly extracted 1 million 
papers. As shown in Extended Data Fig. 6, the downward trend in dis-
ruptiveness is evident across all samples.","[38.289689440993754, 47.11304347826088, 294.9853416149068, 263.56024844720497]"
0003_s41586-022-05543-x.pdf,9,heading,Alternative bibliometric measures,"[39.184099378881946, 272.5043478260869, 178.712049689441, 283.23726708074537]"
0003_s41586-022-05543-x.pdf,9,body,"Several recent papers have introduced alternative specifications of 
the CD index12. We evaluated whether the declines in disruptiveness 
we observe are corroborated using two alternative variations. One 
criticism of the CD index has been that the number of papers that cite 
only the focal paper’s references dominates the measure13. Bornmann 
et al.13 proposes DIl
nok as a variant that is less susceptible to this issue. 
Another potential weakness of the CD index is that it could be very 
sensitive to small changes in the forward citation patterns of papers 
that make no backward citations15. Leydesdorff et al.15 suggests DI* as 
an alternate indicator of disruption that addresses this issue. Therefore, 
we calculated DIl
nok where l = 5 and DI* for 100,000 randomly drawn 
papers and patents each from our analytic sample. Results are pre-
sented in Extended Data Fig. 7a (papers) and b (patents). The blue lines 
indicate disruption based on Bornmann et al.13 and the orange lines 
indicate disruption based on Leydesdorff et al.15. Across science and 
technology, the two alternative measures both show declines in disrup-
tion over time, similar to the patterns observed with the CD index. 
Taken together, these results suggest that the declines in disruption 
we document are not an artefact of our particular operationalization.","[38.289689440993754, 285.02608695652174, 294.9853416149068, 488.95155279503103]"
0003_s41586-022-05543-x.pdf,9,heading,"Robustness to changes in publication, citation and authorship 
practices","[38.289689440993754, 496.1068322981366, 287.83006211180117, 520.2559006211179]"
0003_s41586-022-05543-x.pdf,9,body,"We also considered whether our results may be attributable to changes 
in publication, citation or authorship practices, rather than by sub-
stantive shifts in discovery and invention. Perhaps most critically, as 
noted in the main text, there has been a marked expansion in publish-
ing and patenting over the period of our study. This expansion has 
naturally increased the amount of previous work that is relevant to 
current science and technology and therefore at risk of being cited, 
a pattern reflected in the marked increase in the average number of 
citations made by papers and patents (that is, papers and patents are 
citing more previous work than in previous eras)44,45. Recall that the 
CD index quantifies the degree to which future work cites a focal work 
together with its predecessors (that is, the references in the bibliography 
of the focal work). Greater citation of a focal work independently of its 
predecessors is taken to be evidence of a social process of disruption. 
As papers and patents cite more previous work, however, the prob-
ability of a focal work being cited independently of its predecessors 
may decline mechanically; the more citations a focal work makes, the 
more likely future work is to cite it together with one of its predeces-
sors, even by chance. Consequently, increases in the number of papers 
and patents available for citing and in the average number of citations 
made by scientists and inventors may contribute to the declining values ","[40.078509316770145, 521.1503105590061, 295.8797515527951, 747.4360248447205]"
0003_s41586-022-05543-x.pdf,9,body,"of the CD index. In short, given the marked changes in science and 
technology over our long study window, the CD index of papers and 
patents published in earlier periods may not be directly comparable 
to those of more recent vintage, which could in turn render our conclu-
sions about the decline in disruptive science and technology suspect.  
We addressed these concerns using three distinctive but complemen-
tary approaches—normalization, regression adjustment and simulation.","[304.823850931677, 46.21863354037268, 562.4139130434781, 123.13788819875775]"
0003_s41586-022-05543-x.pdf,9,body,"Verification using normalization. First, following common practice 
in bibliometric research39–43, we developed two normalized versions 
of the CD index, with the goal of facilitating comparisons across time. 
Among the various components of the CD index, we focused our  
attention on the count of papers or patents that only cite the focal 
work’s references (Nk), as this term would seem most likely to scale with 
the increases in publishing and patenting and in the average number of 
citations made by papers and patents to previous work13. Larger values 
of Nk lead to smaller values of the CD index. Consequently, marked 
increases in Nk over time, particularly relative to other components 
of the measure, may lead to a downward bias, thereby inhibiting our 
ability to accurately compare disruptive science and technology in 
later years with earlier periods.
Our two normalized versions of the CD index aim to address this 
potential bias by attenuating the effect of increases in Nk. In the first 
version, which we call ‘Paper normalized’, we subtract from Nk the num-
ber of citations made by the focal paper or patent to previous work 
(Nb). The intuition behind this adjustment is that when a focal paper 
or patent cites more previous work, Nk is likely to be larger because 
there are more opportunities for future work to cite the focal paper or 
patent’s predecessors. This increase in Nk would result in lower values 
of the CD index, although not necessarily as a result of the focal paper 
or patent being less disruptive. In the second version, which we call 
‘field × year normalized’, we subtract Nk by the average number of back-
ward citations made by papers or patents in the focal paper or patent’s 
WoS research area or NBER technology category, respectively, during 
its year of publication (we label this quantity Nb
mean). The intuition 
behind this adjustment is that in fields and time periods in which there 
is a greater tendency for scientists and inventors to cite previous work, 
Nk is also likely to be larger, thereby leading to lower values of the CD 
index, although again not necessarily as a result of the focal paper or 
patent being less disruptive. In cases in which either Nb or Nb
mean exceed 
the value of Nk, we set Nk to 0 (that is, Nk is never negative in the normal-
ized measures). Both adaptations of the CD index are inspired by estab-
lished approaches in the scientometrics literature, and may be 
understood as a form of ‘citing side normalization’ (that is, normaliza-
tion by correcting for the effect of differences in lengths of references 
lists)40.
In Extended Data Fig. 8, we plot the average values of both normalized 
versions of the CD index over time, separately for papers (Extended 
Data Fig. 8a) and patents (Extended Data Fig. 8d). Consistent with our 
findings reported in the main text, we continue to observe a decline 
in the CD index over time, suggesting that the patterns we observe in 
disruptive science and technology are unlikely to be driven by changes 
in citation practices.
Verification using regression adjustment. Second, we adjusted 
for potential confounding using a regression-based approach. This  
approach complements the bibliometric normalizations just described 
by allowing us to account for a broader array of changes in publica-
tion, citation and authorship practices in general (the latter of which 
is not directly accounted for in either the normalization approach or 
the simulation approach described next), and increases the amount 
of previous work that is relevant to current science and technol-
ogy in particular. In Supplementary Table 1, we report the results of  
regression models predicting CD5 for papers (Models 1–4) and patents 
(Models 5–8), with indicator variables included for each year of our ","[306.6126708074534, 133.87080745341615, 562.4139130434781, 747.4360248447205]"
0003_s41586-022-05543-x.pdf,10,body,"study window (the reference categories are 1945 and 1980 for papers 
and patents, respectively). Models 1 and 4 are the baseline models, and 
include no other adjustments beyond the year indicators. In Models 2 
and 5, we add subfield fixed effects (WoS subject areas for papers and 
NBER technology subcategories for patents). Finally, in Models 3–4 and 
7–8, we add control variables for several field × year level—number of 
new papers orpatents, mean number of papers or patents cited, mean 
number of authors or inventors per paper—and paper- or patent-level—
number of papers or patents cited—characteristics, thereby enabling 
more robust comparisons in patterns of disruptive science and tech-
nology over the long time period spanned by our study. For the paper 
models, we also include a paper-level control for the number of unlinked 
references (that is, the number of citations to works that are not indexed 
in WoS). We find that the inclusion of these controls improves model 
fit, as indicated by statistically significant Wald tests presented below 
the relevant models.
Across all eight models shown in Supplementary Table 1, we find that 
the coefficients on the year indicators are statistically significant and 
negative, and growing in magnitude over time, which is consistent with 
the patterns we reported based on unadjusted CD5 values index in the 
main text (Fig. 2). In Extended Data Fig. 8, we visualize the results of 
our regression-based approach by plotting the predicted CD5 values 
separately for each of the year indicators included in Models 4 (papers) 
and 8 (patents). To enable comparisons with raw CD5 values shown in 
the main text, we present the separate predictions made for each year 
as a line graph. As shown in the figure, we continue to observe declining 
values of the CD index across papers and patents, even when accounting 
for changes in publication, citation and authorship practices.
Verification using simulation. Third, following related work in the 
Science of Science14,71–73, we considered whether our results may be an 
artefact of changing patterns in publishing and citation practices by 
using a simulation approach. In essence, the CD index measures disrup-
tion by characterizing the network of citations around a focal paper or 
patent. However, many complex networks, even those resulting from 
random processes, exhibit structures that yield non-trivial values on 
common network measures (for example, clustering)74–76. During the 
period spanned by our study, the citation networks of science and 
technology experienced significant change, with marked increases in 
both the numbers of nodes (that is, papers or patents) and edges (that 
is, citations). Thus, rather than reflecting a meaningful social process, 
the observed declines in disruption may result from these structural 
changes in the underlying citation networks.
To evaluate this possibility, we followed standard techniques from 
network science75,77 and conducted an analysis in which we recomputed 
the CD index on randomly rewired citation networks. If the patterns 
we observe in the CD index are the result of structural changes in the 
citation networks of science and technology (for example, growth in 
the number of nodes or edges) rather than a meaningful social process, 
then these patterns should also be visible in comparable random net-
works that experience similar structural changes. Therefore, finding 
that the patterns we see in the CD index differ for the observed and 
random citation networks would serve as evidence that the decline in 
disruption is not an artefact of the data.
We began by creating copies of the underlying citation network on 
which the values of the CD index used in all analyses reported in the main 
text were based, separately for papers and patents. For each citation 
network (one for papers, one for patents), we then rewired citations 
using a degree-preserving randomization algorithm. In each iteration of 
the algorithm, two edges (for example, A–B and C–D) are selected from 
the underlying citation network, after which the algorithm attempts to 
swap the two endpoints of the edges (for example, A–B becomes A–D, 
and C–D becomes C–B). If the degree centrality of A, B, C and D remains 
the same after the swap, the swap is retained; otherwise, the algorithm 
discards the swap and moves on to the next iteration. When evaluating ","[38.289689440993754, 48.007453416149076, 296.77416149068324, 747.4360248447205]"
0003_s41586-022-05543-x.pdf,10,body,"degree centrality, we consider ‘in-degree’ (that is, citations from other 
papers or patents to the focal paper or patent) and ‘out-degree’ (that 
is, citations from the focal paper or patent to other papers or patents) 
separately. Furthermore, we also required that the age distribution 
of citing and cited papers or patents was identical in the original and 
rewired networks. Specifically, swaps were only retained when the 
publication year of the original and candidate citations was the same. 
In light of these design choices, our rewiring algorithm should be seen 
as fairly conservative, as it preserves substantial structure from the 
original network. There is no scholarly consensus on the number of 
swaps necessary to ensure the original and rewired networks are suf-
ficiently different from one another; the rule we adopt here is 100 × m, 
where m is the number of edges in the network being rewired.
Following previous work14, we created ten rewired copies of the 
observed citation networks for both papers and patents. After creat-
ing these rewired citation networks, we then recomputed CD5. Owing 
to the large scale of the WoS data, we base our analyses on a random 
subsample of ten million papers; CD5 was computed on the rewired 
network for all patents. For each paper and patent, we then compute 
a z score that compares the observed CD5 value to those of the same 
paper or patent in the ten rewired citation networks. Positive z scores 
indicate that the observed CD5 value is greater (that is, more disruptive) 
than would be expected by chance; negative z scores indicate that the 
observed values are lesser (that is, more consolidating).
The results of these analyses are shown in Extended Data Fig. 8, sepa-
rately for papers (Extended Data Fig. 8c) and patents (Extended Data 
Fig. 8f). Lines correspond to the average z score among papers or pat-
ents published in the focal year. The plots reveal a pattern of change in 
the CD index over and beyond that ‘baked in’ to the changing structure 
of the network. We find that on average, papers and patents tend to 
be less disruptive than would be expected by chance, and moreover, 
the gap between the observed CD index values and those from the 
randomly rewired networks is increasing over time, which is consistent 
with our findings of a decline in disruptive science and technology.
Taken together, the results of the foregoing analyses suggest that 
although there have been marked changes in science and technology 
over the course of our long study window, particularly with respect to 
publication, citation and authorship practices, the decline in disrup-
tive science and technology that we document using the CD index is 
unlikely to be an artefact of these changes, and instead represents a 
substantive shift in the nature of discovery and invention.","[305.71826086956514, 47.11304347826088, 562.4139130434781, 488.0571428571428]"
0003_s41586-022-05543-x.pdf,10,heading,Regression analysis,"[304.823850931677, 496.1068322981366, 385.32074534161484, 509.5229813664596]"
0003_s41586-022-05543-x.pdf,10,body,"We evaluate the relationship between disruptiveness and the use of pre-
vious knowledge using regression models, predicting CD5 for individual 
papers and patents, based on three indicators of previous knowledge 
use—the diversity of work cited, mean number of self-citations and 
mean age of work cited. Our measure of the diversity of work cited is 
measured at the field × year level; all other variables included in the 
regressions are defined at the level of the paper or patent. To account 
for potential confounding factors, our models included year and field 
fixed effects. Year fixed effects account for time variant factors that 
affect all observations (papers or patents) equally (for example, global 
economic trends). Field fixed effects account for field-specific factors 
that do not change over time (for example, some fields may intrinsi-
cally value disruptive work over consolidating ones). In contrast to our 
descriptive plots, for our regression models, we adjust for field effects 
using the more granular 150 WoS ‘extended subjects’ (for example, 
‘biochemistry and molecular biology’, ‘biophysics’, ‘biotechnology and 
applied microbiology’, ‘cell biology’, ‘developmental biology’, ‘evolu-
tionary biology’ and ‘microbiology’ are extended subjects within the 
life sciences and biomedicine research area) and 38 NBER technology 
subcategories (for example, ‘agriculture’, ‘food’, ‘textile’; ‘coating’; ‘gas’; 
‘organic’; and ‘resins’ are subcategories within the chemistry technol-
ogy category).","[305.71826086956514, 508.62857142857143, 561.51950310559, 748.3304347826086]"
0004_SciAm-1949-Weaver.pdf,1,author,"THE MATHEMATICS 
OF COMMUNICATION","[110.46367346938773, 96.36244897959185, 536.5861224489795, 182.5665306122449]"
0004_SciAm-1949-Weaver.pdf,1,abstract,"An important new theory is based on the statistical 
character of language. In it the concept of entropy 
is closely linked with the concept of information ","[138.87183673469386, 198.24, 519.9330612244897, 263.8726530612245]"
0004_SciAm-1949-Weaver.pdf,1,author, Warren Weaver ,"[295.60653061224485, 274.64816326530615, 376.9126530612244, 290.3216326530612]"
0004_SciAm-1949-Weaver.pdf,1,date,", July 1949, 
EM","[354.3820408163265, 71.8726530612245, 394.545306122449, 83.62775510204082]"
0004_SciAm-1949-Weaver.pdf,1,body,"HOW 
do 
men 
communicate, 
one 
with 
another? 
The 
spoken 
word, 
either 
direct 
or 
by 
telephone 
or 
radio; the written or printed word, trans- 
mitted by hand, by post, by telegraph, or 
in any other way—these are obvious and 
common forms of communication. But 
there are many others. A nod or a wink, 
a drumbeat in the jungle, a gesture pic- 
tured on a television screen, the blinking 
of a signal light, a bit of music that re- 
minds one of an event in the past, puffs 
of smoke in the desert air, the move- 
ments and posturing in a ballet—all of 
these are means men use to convey ideas. 
The word communication, in fact, will 
be used here in a very broad sense to in- 
clude all of the procedures by which one 
mind can affect another. Although the 
language used will often refer specifical- 
ly to the communication of speech, prac- 
tically everything said applies equally to 
music, to pictures, to a variety of other 
methods of conveying information. 
In communication there seem to be 
problems at three levels: 1) technical, 
2) semantic, and 3) influential. 
The technical problems are concerned 
with the accuracy of transference of in- 
formation from sender to receiver. They 
are inherent in all forms of communica- 
tion, whether by sets of discrete symbols 
(written speech), or by a varying signal 
(telephonic 
or 
radio 
transmission 
of 
voice or music), or by a varying two- 
dimensional pattern (television). 
The semantic problems are concerned 
with the interpretation of meaning by 
the receiver, as compared with the in- 
tended meaning of the sender. This is a 
very   deep   and   involved   situation,   even ","[71.28, 299.1379591836735, 237.8106122448979, 725.2604081632652]"
0004_SciAm-1949-Weaver.pdf,1,body,"when one deals only with the relative- 
ly simple problems of communicating 
through speech. For example, if Mr. X is 
suspected not to understand what Mr. Y 
says, then it is not possible, by having 
Mr. Y do nothing but talk further with 
Mr. X, completely to clarify this situation 
in any finite time. If Mr. Y says “Do you 
now understand me?” and Mr. X says 
“Certainly I do,” this is not necessarily 
a 
certification 
that 
understanding 
has 
been achieved. It may just be that Mr. X 
did 
not 
understand 
the 
question. 
If 
this sounds silly, try it again as “Czy pan 
mnie rozumie?” with the answer “Hai 
wakkate imasu.” In the restricted field 
of speech communication, the difficulty 
may be reduced to a tolerable size, but 
never completely eliminated, by “expla- 
nations.” 
They 
are 
presumably 
never 
more than approximations to the ideas 
being explained, but are understandable 
when phrased in language that has pre- 
viously been made reasonably clear by 
usage. For example, it does not take long 
to make the symbol for “yes” in any lan- 
guage understandable. 
The problems of influence or effective- 
ness are concerned with the success with 
which the meaning conveyed to the re- 
ceiver leads to the desired conduct on his 
part. It may seem at first glance undesir- 
ably narrow to imply that the purpose of 
all communication is to influence the 
conduct of the receiver. But with any 
reasonably broad definition of conduct, 
it is clear that communication either af- 
fects conduct or is without any discern- 
ible and provable effect at all. 
One might be inclined to think that 
the  technical   problems   involve   only   the ","[245.6473469387755, 299.1379591836735, 412.1779591836734, 724.2808163265306]"
0004_SciAm-1949-Weaver.pdf,1,body,"engineering details of good design of 
a communication system, while the se- 
mantic and the effectiveness problems 
contain most if not all of the philosophi- 
cal content of the general problem of 
communication. To see that this is not 
the case, we must now examine some im- 
portant recent work in the mathematical 
theory of communication. 
HIS is by no means a wholly new
theory. 
As 
the 
mathematician 
John
von Neumann has pointed out, the 19th-
century Austrian physicist Ludwig Boltz-
mann suggested that some concepts of
statistical mechanics were applicable to
the concept of information. Other scien-
tists, notably Norbert Wiener of the
Massachusetts 
Institute 
of 
Technology,
have made profound contributions. The
work which will be here reported is that
of Claude Shannon of the Bell Telephone
Laboratories, 
which 
was 
preceded 
by
that of H. Nyquist and R. V. L. Hartley
in the same organization. This work ap-
plies in the first instance only to the tech-
nical problem, but the theory has broad-
er significance. To begin with, meaning
and effectiveness are inevitably restrict-
ed by the theoretical limits of accuracy in
symbol transmission. Even more signifi-
cant, a theoretical analysis of the techni-
cal problem reveals that it overlaps the
semantic and the effectiveness problems
more than one might suspect. 
A communication system is symboli-
cally represented in the drawing on pages
12 and 13. The information source se-
lects a desired message out of a set of
possible messages. (As will be shown,
this    is    a    particularly    important    func- 
T ","[420.9942857142857, 300.1175510204082, 589.484081632653, 724.2808163265306]"
0004_SciAm-1949-Weaver.pdf,2,body,"tion.) The transmitter changes this mes- 
sage into a signal which is sent over the 
communication channel to the receiver. 
The receiver is a sort of inverse trans- 
mitter, changing the transmitted signal 
back into a message, and handing this 
message on to the destination. When I 
talk to you, my brain is the information 
source, yours the destination; my vocal 
system is the transmitter, and your ear 
with the eighth nerve is the receiver. 
In the process of transmitting the sig- 
nal, it is unfortunately characteristic that 
certain things not intended by the infor- 
mation source are added to the signal. 
These unwanted additions may be dis- 
tortions of sound (in telephony, for ex- 
ample), or static (in radio), or distor- 
tions in the shape or shading of a picture 
(television), 
or 
errors 
in 
transmission 
(telegraphy 
or 
facsimile). 
All 
these 
changes in the signal may be called noise. 
The questions to be studied in a com- 
munication system have to do with the 
amount of information, the capacity of 
the communication channel, the coding 
process that may be used to change a 
message into a signal and the effects of 
noise. 
First off, we have to be clear about 
the rather strange way in which, in this 
theory, the word “information” is used; 
for it has a special sense which, among 
other things, must not be confused at all 
with meaning. It is surprising but true 
that, from the present viewpoint, two 
messages, one heavily loaded with mean- 
ing and the other pure nonsense, can be 
equivalent as regards information. 
In fact, in this new theory the word 
information relates not so much to what 
you do say, as to what you could say. 
That is, information is a measure of 
your freedom of choice when you select 
a message. If you are confronted with a 
very 
elementary 
situation 
where 
you 
have to choose one of two alternative 
messages, then it is arbitrarily said that 
the information associated with this situ- 
ation is unity. The concept of informa- 
tion applies not to the individual mes- 
sages, as the concept of meaning would, 
but  rather  to  the  situation  as  a  whole, the ","[56.586122448979566, 46.40326530612246, 223.11673469387753, 593.0155102040817]"
0004_SciAm-1949-Weaver.pdf,2,body,"unit information indicating that in this 
situation one has an amount of freedom 
of choice, in selecting a message, which 
it is convenient to regard as a standard 
or unit amount. The two messages be- 
tween which one must choose in such a 
selection can be anything one likes. One 
might be the King James version of the 
Bible, and the other might be “Yes.” 
THE remarks thus far relate to artifi- 
cially simple situations where the in- 
formation source is free to choose only 
among several definite messages—like a 
man picking out one of a set of standard 
birthday-greeting 
telegrams. 
A 
more 
natural and more important situation is 
that in which the information source 
makes a sequence of choices from some 
set of elementary symbols, the selected 
sequence 
then 
forming 
the 
message. 
Thus a man may pick out one word after 
another, 
these 
individually 
selected 
words then adding up to the message. 
Obviously probability plays a major 
role in the generation of the message, 
and the choices of the successive sym- 
bols depend upon the preceding choices. 
Thus, if we are concerned with English 
speech, and if the last symbol chosen is 
“the,” then the probability that the next 
word will be an article, or a verb form 
other than a verbal, is very small. After 
the three words “in the event,” the prob- 
ability for “that” as the next word is 
fairly high, and for “elephant” as the next 
word is very low. Similarly, the probabil- 
ity is low for such a sequence of words 
as “Constantinople fishing nasty pink.” 
Incidentally, it is low, but not zero, for 
it is perfectly possible to think of a pas- 
sage in which one sentence closes with 
“Constantinople fishing,” and the next 
begins with “Nasty pink.” (We might 
observe in passing that the sequence un- 
der discussion has occurred in a single 
good English sentence, namely the one 
second preceding.) 
As a matter of fact, Shannon has 
shown that when letters or words chosen 
at random are set down in sequences 
dictated 
by 
probability 
considerations 
alone,  they  tend  to  arrange  themselves   in ","[231.93306122448976, 46.40326530612246, 400.42285714285714, 593.0155102040817]"
0004_SciAm-1949-Weaver.pdf,2,body,"meaningful words and phrases (see illus- 
tration on page 15). 
Now let us return to the idea of infor- 
mation. 
The 
quantity 
which 
uniquely 
meets the natural requirements that one 
sets up for a measure of information 
turns out to be exactly that which is 
known in thermodynamics as entropy, or 
the degree of randomness, or of ""shuf- 
fledness"" if you will, in a situation. It is 
expressed in terms of the various proba- 
bilities involved. 
To those who have studied the physi- 
cal sciences, it is most significant that an 
entropy-like expression appears in com- 
munication theory as a measure of infor- 
mation. The concept of entropy, intro- 
duced by the German physicist Rudolf 
Clausius nearly 100 years ago, closely 
associated with the name of Boltzmann, 
and 
given 
deep 
meaning 
by 
Willard 
Gibbs of Yale in his classic work on 
statistical 
mechanics, 
has 
become 
so 
basic and pervasive a concept that Sir 
Arthur 
Eddington 
remarked: 
“The 
law 
that entropy always increases—the sec- 
ond 
law 
of 
thermodynamics—holds, 
I 
think, the supreme position among the 
laws of Nature.” 
Thus when one meets the concept of 
entropy in communication theory, he has 
a right to be rather excited. That infor- 
mation should be measured by entropy 
is, after all, natural when we remember 
that information is associated with the 
amount of freedom of choice we have in 
constructing messages. Thus one can say 
of a communication source, just as he 
would also say of a thermodynamic en- 
semble: “This situation is highly organ- 
ized; it is not characterized by a large 
degree of randomness or of choice—that 
is to say, the information, or the entropy, 
is low.” 
We must keep in mind that in the 
mathematical 
theory 
of 
communication 
we are concerned not with the meaning 
of 
individual 
messages 
but 
with 
the 
whole statistical nature of the informa- 
tion source. Thus one is not surprised 
that the capacity of a channel of com- 
munication is to be described in terms 
of   the   amount    of    information    it    can ","[407.2799999999999, 45.42367346938776, 577.7289795918367, 593.0155102040817]"
0004_SciAm-1949-Weaver.pdf,3,body,"transmit, or better, in terms of its ability 
to transmit what is produced out of a 
source of a given information. 
The transmitter may take a written 
message and use some code to encipher 
this message into, say, a sequence of 
numbers, these numbers then being sent 
over the channel as the signal. Thus one 
says, in general, that the function of the 
transmitter is to encode, and that of 
the receiver to decode, the message. The 
theory provides for very sophisticated 
transmitters and receivers—such, for ex- 
ample, as possess “memories,” so that the 
way they encode a certain symbol of 
the message depends not only upon this 
one symbol but also upon previous sym- 
bols of the message and the way they 
have been encoded. 
We are now in a position to state the 
fundamental 
theorem 
for 
a 
noiseless 
channel 
transmitting 
discrete 
symbols. 
This theorem relates to a communication 
channel which has a capacity of C units 
per second, accepting signals from an in- 
formation source of H units per second. 
The theorem states that by devising 
proper coding procedures for the trans- 
mitter it is possible to transmit symbols 
over the channel at an average rate 
which is nearly C/H, but which, no mat- 
ter how clever the coding, can never be 
made to exceed C/H. 
VIEWED 
superficially, 
say 
in 
rough 
analogy to the use of transformers to 
match impedances in electrical circuits, 
it seems very natural, although certainly 
pretty neat, to have this theorem which 
says that efficient coding is that which 
matches the statistical characteristics of 
information 
source 
and 
channel. 
But 
when it is examined in detail for any 
one of the vast array of situations to 
which this result applies, one realizes 
how deep and powerful this theory is. 
How 
does 
noise 
affect 
information? 
Information, we must steadily remem- 
ber, is a measure of one’s freedom of 
choice in selecting a message. The great- 
er this freedom of choice, the greater is 
the uncertainty that the message actually 
selected   is   some   particular   one.     Thus ","[55.6065306122449, 47.38285714285715, 223.11673469387753, 593.9951020408163]"
0004_SciAm-1949-Weaver.pdf,3,body,"greater freedom of choice, greater uncer- 
tainty and greater information all go 
hand in hand. 
If noise is introduced, then the re- 
ceived message contains certain distor- 
tions, certain errors, certain extraneous 
material, that would certainly lead to in- 
creased uncertainty. But if the uncer- 
tainty is increased, the information is 
increased, and this sounds as though the 
noise were beneficial! 
It is true that when there is noise, the 
received signal is selected out of a more 
varied set of signals than was intended 
by the sender. This situation beauti- 
fully illustrates the semantic trap into 
which one can fall if he does not remem- 
ber that “information” is used here with 
a special meaning that measures freedom 
of choice and hence uncertainty as to 
what choice has been made. Uncertainty 
that arises by virtue of freedom of choice 
on the part of the sender is desirable un- 
certainty. 
Uncertainty 
that 
arises 
be- 
cause of errors or because of the influ- 
ence of noise is undesirable uncertainty. 
To get the useful information in the re- 
ceived signal we must subtract the spu- 
rious portion. This is accomplished, in 
the theory, by establishing a quantity 
known as the “equivocation,” meaning 
the amount of ambiguity introduced by 
noise. One then refines or extends the 
previous definition of the capacity of a 
noiseless channel, and states that the 
capacity of a noisy channel is defined to 
be equal to the maximum rate at which 
useful information (i.e., total uncertain- 
ty minus noise uncertainty) can be trans- 
mitted over the channel. 
Now, finally, we can state the great 
central theorem of this whole communi- 
cation theory. Suppose a noisy channel 
of capacity C is accepting information 
from a source of entropy H, entropy cor- 
responding to the number of possible 
messages from the source. If the channel 
capacity C is equal to or larger than H, 
then by devising appropriate coding sys- 
tems the output of. the source can be 
transmitted over the channel with as 
little error as one pleases. But if the chan- 
nel  capacity  C  is  less  than  H, the entropy ","[230.9534693877551, 47.38285714285715, 397.484081632653, 595.9542857142858]"
0004_SciAm-1949-Weaver.pdf,3,body,"of the source, then it is impossible to 
devise codes which reduce the error 
frequency as low as one may please. 
However clever one is with the coding 
process, it will always be true that after 
the signal is received there remains some 
undesirable uncertainty about what the 
message was; and this undesirable uncer- 
tainty—this noise or equivocation—will 
always be equal to or greater than H 
minus C. But there is always at least one 
code capable of reducing this undesir- 
able uncertainty down to a value that ex- 
ceeds H minus C by a small amount. 
This powerful theorem gives a precise 
and almost startlingly simple description 
of the utmost dependability one can ever 
obtain from a communication channel 
which operates in the presence of noise. 
One must think a long time, and con- 
sider many applications, before he fully 
realizes how powerful and general this 
amazingly compact theorem really is. 
One single application can be indicated 
here, but in order to do so, we must go 
back for a moment to the idea of the 
information of a source. 
Having calculated the entropy (or the 
information, or the freedom of choice) 
of a certain information source, one can 
compare it to the maximum value this 
entropy could have, subject only to the 
condition that the source continue to 
employ the same symbols. The ratio of 
the actual to the maximum entropy is 
called the relative entropy of the source. 
If the relative entropy of a certain source 
is, say, eight-tenths, this means roughly 
that this source is, in its choice of sym- 
bols to form a message, about 80 per 
cent as free as it could possibly be with 
these same symbols. One minus the rela- 
tive entropy is called the “redundancy.” 
That is to say, this fraction of the mes- 
sage is unnecessary in the sense that if 
it were missing the message would still 
be essentially complete, or at least could 
be completed. 
It is most interesting to note that the 
redundancy of English is just about 50 
per cent. In other words, about half of 
the letters or words we choose in writing 
or   speaking   are   under   our   free   choice, ","[405.32081632653063, 46.40326530612246, 574.7902040816326, 595.9542857142858]"
0004_SciAm-1949-Weaver.pdf,4,body,"and about half are really controlled by 
the statistical structure of the language, 
although we are not ordinarily aware of 
it. Incidentally, this is just about the 
minimum of freedom  (or relative en- 
tropy) in the choice of letters that one 
must have to be able to construct satis- 
factory   crossword  puzzles.   In  a  lan- 
guage that had only 20 per cent of free- 
dom, or 80 per cent redundancy, it would 
be  impossible  to  construct  crossword 
puzzles in sufficient complexity and num- 
ber to make the game popular. 
Now since English is about 50 per 
cent redundant, it would be possible to 
save about one-half the time of ordinary 
telegraphy by a proper encoding process, 
provided one transmitted over a noise- 
less channel. When there is noise on a 
channel, however, there is some real ad- 
vantage in not using a coding process 
that eliminates all of the redundancy. 
For 
the 
remaining 
redundancy 
helps 
combat the noise. It is the high re- 
dundancy of English, for example, that 
makes it easy to correct errors in spelling 
that have arisen during transmission. 
HE 
communication 
systems 
dealt 
with so far involve the use of a dis- 
crete set of symbols—say letters—only 
moderately 
numerous. 
One 
might 
well 
expect that the theory would become 
almost 
indefinitely 
more 
complicated 
when it seeks to deal with continuous 
messages such as those of the speaking 
voice, with its continuous variation of 
pitch and energy. As is often the case, 
however, a very interesting mathematical 
theorem comes to the rescue. As a prac- 
tical matter, one is always interested in 
a continuous signal which is built up of 
simple harmonic constituents, not of all 
frequencies but only of those that lie 
wholly within a band from zero to, say, 
W cycles per second. Thus very satisfac- 
tory 
communication 
can 
be 
achieved 
over a telephone channel that handles 
frequencies up to about 4,000, although 
the human voice does contain higher fre- 
quencies. With frequencies up to 10,000 
or 12,000, high-fidelity radio transmis- 
sion of symphonic music is possible. 
The theorem that helps us is one 
which states that a continuous signal, T 
seconds in duration and band-limited in 
frequency to the range from zero to W, 
can be completely specified by stating 
2TW numbers. This is really a remark- 
able 
theorem. 
Ordinarily 
a 
continuous 
curve can be defined only approximately 
by a finite number of points. But if the 
curve is built up out of simple harmonic 
constituents of a limited number of fre- 
quencies, as a complex sound is built up 
out of a limited number of pure tones, 
then a finite number of quantities is all 
that is necessary to define the curve com- 
pletely. 
Thanks partly to this theorem, and 
partly to the essential nature of the situ- 
ation,    it    turns    out     that    the    extended 
T ","[27.198367346938696, 45.42367346938776, 199.6065306122449, 789.9134693877551]"
0004_SciAm-1949-Weaver.pdf,4,body,"theory of continuous communication is 
somewhat more difficult and complicated 
mathematically, but not essentially dif- 
ferent from the theory for discrete sym- 
bols. Many of the statements for the 
discrete case require no modification for 
the continuous case, and others require 
only minor change. 
The mathematical theory of communi- 
cation is so general that one does not 
need to say what kinds of symbols are 
being 
considered—whether 
written 
let- 
ters or words, or musical notes, or spoken 
words, or symphonic music, or pictures. 
The relationships it reveals apply to all 
these and to other forms of communi- 
cation. The theory is so imaginatively 
motivated that it deals with the real inner 
core of the communication problem. 
One evidence of its generality is that 
the 
theory 
contributes 
importantly 
to, 
and in fact is really the basic theory 
of, cryptography, which is of course a 
form of coding. In a similar way, the 
theory contributes to the problem of 
translation from one language to an- 
other,  although   the   complete   story   here ","[207.44326530612244, 43.46448979591837, 375.93306122448973, 325.5869387755102]"
0004_SciAm-1949-Weaver.pdf,4,body,"clearly requires consideration of mean- 
ing, as well as of information. Similarly, 
the ideas developed in this work connect 
so closely with the problem of the logical 
design of computing machines that it is 
no surprise that Shannon has written a 
paper on the design of a computer that 
would be capable of playing a skillful 
game of chess. And it is of further per- 
tinence to the present contention that 
his paper closes with the remark that 
either one must say that such a com- 
puter “thinks,” or one must substantially 
modify the conventional implication of 
the verb “to think.” 
The 
theory 
goes 
further. 
Though 
ostensibly applicable only to problems at 
the technical level, it is helpful and sug- 
gestive at the levels of semantics and 
effectiveness as well. The formal dia- 
gram of a communication system on 
pages 12 and 13 can, in all likelihood, 
be extended to include the central issues 
of meaning and effectiveness. 
Thus when one moves to those levels 
it may prove to be essential to take 
account of the statistical characteristics 
of the destination. One can imagine, as 
an addition to the diagram, another box 
labeled “Semantic Receiver” interposed 
between   the   engineering   receiver  (which ","[208.4228571428571, 469.5869387755102, 375.93306122448973, 789.9134693877551]"
0004_SciAm-1949-Weaver.pdf,4,body,"changes signals to messages) and the 
destination. This semantic receiver sub- 
jects the message to a second decoding, 
the demand on this one being that it 
must match the statistical semantic char- 
acteristics of the message to the statis- 
tical semantic capacities of the totality 
of receivers, or of that subset of re- 
ceivers which constitutes the audience 
one wishes to affect. 
Similarly one can imagine another box 
in the diagram which, inserted between 
the information source and the transmit- 
ter, would be labeled “Semantic Noise” 
(not to be confused with “engineering 
noise”). 
This 
would 
represent 
distor- 
tions of meaning introduced by the in- 
formation source, such as a speaker, 
which are not intentional but neverthe- 
less affect the destination, or listener. 
And the problem of semantic decoding 
must 
take 
this 
semantic 
noise 
into 
account. It is also possible to think of 
a treatment or adjustment of the original 
message that would make the sum of 
message meaning plus semantic noise 
equal to the desired total message mean- 
ing at the destination. 
ANOTHER way in which the theory 
 can 
be 
helpful 
in 
improving 
com- 
munication is suggested by the fact that 
error and confusion arise and fidelity de- 
creases when, no matter how good the 
coding, one tries to crowd too much over 
a channel. A general theory at all levels 
will surely have to take into account not 
only the capacity of the channel but also 
(even the words are right!) the capacity 
of the audience. If you overcrowd the 
capacity of the audience, it is probably 
true, by direct analogy, that you do not 
fill the audience up and then waste only 
the remainder by spilling. More likely, 
and again by direct analogy, you force 
a general error and confusion. 
The concept of information developed 
in this theory at first seems disappoint- 
ing and bizarre—disappointing because 
it has nothing to do with meaning, and 
bizarre because it deals not with a single 
message but rather with the statistical 
character of a whole ensemble of mes- 
sages, bizarre also because in these sta- 
tistical terms the words information and 
uncertainty find themselves partners. 
But we have seen upon further ex- 
amination of the theory that this analysis 
has so penetratingly cleared the air that 
one is now perhaps for the first time 
ready for a real theory of meaning. An 
engineering 
communication 
theory 
is 
just like a very proper and discreet girl 
at the telegraph office accepting your 
telegram. She pays no attention to the 
meaning, whether it be sad or joyous 
or embarrassing. But she must be pre- 
pared to deal intelligently with all mes- 
sages that come to her desk. This idea 
that a communication system ought to 
try to deal with all possible messages, 
and  that  the  intelligent  way   to   try   is  to ","[384.749387755102, 42.484897959183684, 553.2391836734694, 790.8930612244898]"
0004_SciAm-1949-Weaver.pdf,5,body,"base design on the statistical character 
of the source, is surely not without 
significance for communication in gen- 
eral. Language must be designed, or de- 
veloped, with a view to the totality of 
things that man may wish to say; but not 
being able to accomplish everything, it 
should do as well as possible as often as 
possible. That is to say, it too should 
deal with its task statistically. 
This study reveals facts about the 
statistical structure of the English lan- 
guage, as an example, which must seem 
significant to students of every phase of 
language 
and 
communication. 
It 
sug- 
gests, as a particularly promising lead, 
the application of probability theory to 
semantic 
studies. 
Especially 
pertinent 
is the powerful body of probability the- 
ory dealing with what mathematicians 
call the Markoff processes, whereby past 
events 
influence 
present 
probabilities, 
since this theory is specifically adapted 
to handle one of the most significant but 
difficult aspects of meaning, namely the 
influence of context. One has the vague 
feeling 
that 
information 
and 
meaning 
may prove to be something like a pair 
of 
canonically 
conjugate 
variables 
in 
quantum theory, that is, that informa- 
tion and meaning may be subject to some 
joint restriction that compels the sacri- 
fice of one if you insist on having much 
of the other. 
Or perhaps meaning may be shown to 
be analogous to one of the quantities on 
which the entropy of a thermodynamic 
ensemble depends. Here Eddington has 
another apt comment: 
“Suppose that we were asked to ar- 
range the following in two categories— 
distance, mass, electric force, entropy, 
beauty, melody. 
“I 
think 
there 
are 
the 
strongest 
grounds for placing entropy alongside 
beauty and melody, and not with the first 
three. Entropy is only found when the 
parts are viewed in association, and it 
is by viewing or hearing the parts in 
association that beauty and melody are 
discerned. All three are features of ar- 
rangement. It is a pregnant thought that 
one of these three associates should be 
able to figure as a commonplace quan- 
tity of science. The reason why this 
stranger can pass itself off among the 
aborigines of the physical world is that 
it is able to speak their language, viz., 
the language of arithmetic.” 
One feels sure that Eddington would 
have been willing to include the word 
meaning along with beauty and melody; 
and one suspects he would have been 
thrilled to see, in this theory, that en- 
tropy not only speaks the language of 
arithmetic; it also speaks the language 
of language. ","[19.36163265306115, 46.40326530612246, 185.89224489795913, 741.9134693877551]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,2,title,"The Tortoise and the (Soft)ware: 
Moore’s Law, Amdahl’s Law, and 
Performance Trends for Human-
Machine Systems","[88.9624844720498, 116.4968944099379, 461.9314285714287, 223.82608695652175]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,2,abstract,"Human interaction with computing and communication 
systems involves a mix of parallel and serial processing by 
the human-computer system. Moore’s Law provides an 
illustration of the fact that the performance of the digital 
components of any human-computer system has improved 
rapidly. But what of the performance of those human 
components? While we humans are amazing information 
processing machines, our information processing capabilities 
are relatively fixed. This paper reviews 100 years of the 
human performance literature and shows, graphically, the 
disparity between the non-growth in human performance and
the geometrical improvements in computational capability. 
Further, Amdahl’s Law demonstrates, algebraically, that 
increasingly the (non-parallelizable) human performance 
becomes the determining factor of speed and success in 
most any human-computer system. Whereas engineered 
products improve daily, and the amount of information for us 
to potentially process is growing at an ever quickening pace, 
the fundamental building blocks of human-information 
processing (e.g., reaction time, short-term memory capacity)
have the same speed and capacity as they did for our 
grandparents. Or, likely, for the ancient Greeks. This implies 
much for human-computer interaction design; rather than 
hoping our users to read or to type faster, we must look for 
optimally chosen human channels and maximally matched 
human and machine functions. This tortoise and the (hard- 
and soft-)ware race demands renewed enthusiasm for, and 
increased, systematic attention paid to the practice of 
usability and to research in human-computer interaction.  ","[239.22335403726717, 247.97515527950307, 503.96869565217395, 547.6024844720496]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,2,date," 
 August 2014 ","[392.1674534161491, 82.50931677018633, 450.30409937888203, 92.34782608695652]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,2,author,"Randolph Bias 
Professor ","[89.85689440993798, 230.0869565217391, 152.46559006211191, 239.03105590062108]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,2,author,"Clayton Lewis 
Professor ","[88.9624844720498, 372.2981366459627, 149.78236024844733, 382.13664596273287]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,2,author,"Douglas Gillan 
Professor and He","[88.9624844720498, 451.9006211180124, 152.46559006211191, 460.84472049689435]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,11,body,"And thus, as in Amdahl's Law for parallelism, if some portion of a task cannot be done by the 
computer, the increase in speed due to increasing computer performance diminishes, as task 
completion time comes to be dominated by the time required for the human’s portion of the 
work. 
An example employing a task that will be familiar to all is the speed of creating a document, 
perhaps a business letter. Today, this task is nearly always supported by computer. At an early 
stage of the development of the relevant computational tools, improvements in the performance 
of these tools had substantial impact on the overall task time. But the time required today to 
produce a document is dominated by the time required by the human operations; even reducing 
the time required for all of the computational operations to zero would make little practical 
difference in total task time. Improvement can come only from improvements in the human 
operations in this particular hybrid computing-human system. A slow typist would be much 
better served by spending his or her money on typing lessons than by replacing his or her 1.66 
GHz computer with a 3.2 GHz computer.  ","[88.9624844720498, 444.7453416149068, 508.44074534161496, 593.2173913043479]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,11,heading,Reliability Has a Similar Logic to Speed,"[88.0680745341616, 605.7391304347826, 297.3600000000001, 617.366459627329]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,11,body,"In many situations obtaining results more reliably is more important than increased speed. Here 
the characteristics of digital technology also often offer advantages. In most situations, well-
specified procedures can be carried out more reliably by a machine than by human operators. 
But when human operations must be combined with digital operations (i.e., when an entire task 
cannot be automated), how does the reliability of a hybrid system respond to improvements in 
the performance of its elements? 
Consider two simple ways in which these elements could be combined. In a serial combination, 
the results of some machine operation are passed on for human processing, or vice versa, or ","[89.85689440993798, 623.6273291925465, 505.7575155279503, 710.3850931677018]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,12,body,"even chained in alternation. In parallel combination, results of human and machine operations 
are passed on for post-processing to determine a result. In both of these constructions—serial 
and parallel—it is reasonable to assume that both human and machine operations have to be 
correct to obtain a correct final result. 
This leads to a similar relationship to that seen above for speed. If the error rate for 
computational operations is Ec and that for the operations of people is Ep, the combined error 
rate, when both computer and human operations must be correct, is 
Ec+(1-Ec)Ep 
As in the speed analysis, one sees that as Ec decreases, the overall error rate is dominated by 
Ep. Once Ec is small, further decreases have very little impact. ","[88.0680745341616, 110.2360248447205, 495.91900621118026, 224.72049689440993]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,12,heading,"Improvements in Human Performance, Plus an Analogy ","[89.85689440993798, 237.24223602484471, 387.69540372670815, 249.76397515527947]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,12,body,"Except for cases of total automation that do not require a human serving even as a monitor, it 
appears that improvements in machine speed or reliability, on their own, provide limited 
benefits without accompanying improvements in human performance. And Figure 2 suggests 
that we can’t expect improvements in basic human-information processing any time soon. 
Imagine, if you will, the accomplishment of any human task as being like trying to get from one 
place to another—from a place of not having the task done to the place of having the task 
completed. For our purposes, we will consider the task of writing a letter, and we will liken it to 
getting from New York to Los Angeles. In the days before papyrus and ink, writing a letter, 
perhaps with a stone and a piece of flint (“Dear Oog”), was a laborious task, much like traveling 
from NY to LA in a horse-drawn wagon. Paper and ink made letter writing faster, like perhaps 
driving in a Model T on dirt roads. The Gutenberg press moves us to traveling on one-lane 
highways (though, really, it didn’t speed up the first letter so much as the copies), and with the 
typewriter we’re in an Edsel on blacktop. Comes the PC, and we are buzzing along the interstate 
highway system. With human fingers and brains, we are not going to type a letter any faster. 
But, oh wait, what about the airplane? So now we have changed human channels; instead of 
typing we are using speech-to-text, and we are generating our letter—we are getting to LA 
much faster. But—and here is our fundamental point—we may do a better job of selecting which 
human-information processing channel to employ, and we may improve the connection between 
some information activity and that channel, but given the laws of physics and the almost-flat-
lined parameters of human-information processing, the human is destined to be the limiting 
factor in any human-machine system.  
In these terms, given the trends observed in Figure 3, the human brain and 
sensory/perceptual/cognitive/motor system is ever increasingly the limiting factor. We have 
more and more information and the possibility of more and more online tasks, all presented and 
represented by more and more powerful, parallel technological systems, trying to squeeze 
through eyes, ears, and brains that evolved for hunting, gathering food, finding mates, and 
avoiding getting eaten by predators. However, as the distinction that we made above between 
hard-wired and soft human processes suggests, the picture in Figure 3 may be misleadingly 
dismal. Although there are indeed aspects of human performance that have not improved, we’ll 
suggest that many others have improved dramatically and could be improved further. ","[88.9624844720498, 253.34161490683226, 503.96869565217395, 572.6459627329192]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,12,heading,"Is Multitasking Special? 
Human beings have always h","[88.0680745341616, 578.0124223602484, 211.49664596273303, 590.5341614906832]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,12,body,"Human beings have always had a capacity to attend to several things at once, although perhaps 
with some cost in task performance depending on the relations between the tasks in 
characteristics like sensory modality or visual channel (e.g., Wickens, 2008). Mothers have done 
it since the hunter-gatherer era—picking berries while suckling an infant. Nor is electronic 
multitasking entirely new; people have been driving while listening to car radios since the radios 
became popular in the 1930s. But there is no doubt that the phenomenon has reached a kind of 
warp speed in the era of Web-enabled computers (Wallis, 2006). 
Today, “We produce and consume data at rates that are agonizingly slow by computer 
standards” (Lee, 2010, para. 4). But the proliferation of digitized information, plus the 
associated proliferation of devices to afford access to this information, leads to some new 
possibilities. Does the supposed ability of Generation M to multitask allow them to do a better, 
or worse, job of processing information? Has the multitasking skill helped narrow the gap ","[90.75130434782619, 591.4285714285713, 505.7575155279503, 718.4347826086956]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,13,body,"between human performance and machine performance? Might it be tapped more fully in hybrid 
systems?  
In telephony the term is multiplexing, whereby multiple analog message signals or digital data 
streams may be combined into a single signal that travels over some medium 
(Multiplexing, n.d.). In psychology the term tends to be parallel processing, whereby one 
person, or some single entity, carries out multiple operations or tasks at the same time (Parallel 
processing, n.d.). Ask any middle school boy what it’s called when he’s doing his homework, 
listening to his iPod, engaging three friends in separate texting threads, and emailing his 
teacher about why that homework may be late, and he’ll say “multitasking.” Are people actually 
getting better at multitasking?  
Some researchers would argue that what appears to be multitasking may actually be, at least in 
some cases, rapid task switching, involving shifting attention from one task to another every 
few seconds (e.g., Salvucci, Taatgen, & Borst, 2009). Whether it be multitasking or task 
switching or some combination, those of us who think we’re good at it tend to believe it 
strongly. Think how long it would take us if we were to do all those things one at a time! The 
empirical data seem to be mixed, with a tendency towards the notion that the time spent 
switching and ramping back up on the new task, washing out any real speed or reliability 
advances afforded by multitasking (e.g., Liefooghe, Barrouillet, Vandierendonck, & Camos, 
2008; Meiran, Chorev, & Sapir, 2000). Whether or not multitasking or task switching tends to 
improve overall attentional abilities (e.g., Green & Bavelier, 2003, 2007) or otherwise improve 
performance in general, people are doing it, and work is needed to support it effectively in UI 
designs. Park (2014) has employed real-world tasks to examine what variables lead to the 
experience of “flow states” when multitasking. And this provides additional motivation for a 
renewed focus on usability. What is it that we have done already, in the realm of UI design or 
software engineering practices or HCI research, to support or reflect this focus?  ","[90.75130434782619, 109.3416149068323, 503.96869565217395, 376.7701863354037]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,13,heading,"Empirical Research Data Related to Multitasking 
As said, there are many researchers who would argue tha","[88.9624844720498, 381.2422360248447, 334.9252173913044, 393.76397515527947]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,13,body,"As said, there are many researchers who would argue that true multitasking, in which a human 
attends to two or more tasks at once, cannot be accomplished by the human 
perceptual/cognitive system. They would argue that what appears to be multitasking is actually 
rapid task switching, involving shifting attention from one task to another every few seconds 
(e.g., Burgess, 2000). An argument against that position is that humans engage in many 
responses that typically require no attention—breathing, blinking eye, maintaining posture—
while engaged in other attention-requiring tasks. However, ask a person having a serious 
asthma attack if they are capable of doing other tasks when they have to think about taking 
each breath (although we suggest that you wait until the attack has subsided before asking). 
Also, as Wallis (2006) suggested, as we become skilled drivers, we may listen to the radio or 
carry on a conversation while steering, navigating, and accelerating the car, seemingly dividing 
our attention among these different activities. When the car in front of us suddenly stops, we 
can shift our attention to braking, or when we are searching for a particular freeway exit, we 
can shift to navigation, but it seems as if highly experienced drivers can perform many tasks at 
once under normal, placid driving conditions. However, recall what it was like as a novice driver 
when a friend tried to carry on a conversation; many novice drivers have difficulty doing both 
tasks. Another example is a skilled typist who can type a manuscript and carry on a 
conversation simultaneously. Novice typists can’t do both tasks, but the highly experienced 
touch typist can. So, it appears that multitasking can occur when at least one of the tasks is 
highly practiced so that it requires no conscious attention to the task.  
Many other situations that are referred to as multitasking are more likely cases of frequent and 
relatively rapid task switching. For example, in a report on the effects of the Internet on the 
behavior of young Americans (Lenhart, Rainie, & Lewis, 2001), a 17-year-old girl was quoted, “I 
get bored if it’s not all going at once, because everything has gaps—waiting for a website to 
come up, commercials on TV, etc.” This quote was used in a subsequent report on media 
multitasking (Foehr, 2006). But as the debate and the research on the relative value of 
multitasking continues (e.g., Park & Bias, 2012), there can be no doubt the at least some of us 
are attempting to multitask, or task switch with short cycle times, much more frequently than in 
years past.  ","[89.85689440993798, 394.6583850931677, 504.86310559006216, 696.968944099379]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,14,heading,"How Have Designers Responded to a (Perhaps Tacit) Awareness That the 
Human Is the Tortoise in This Race? ","[88.9624844720498, 107.55279503105591, 483.39726708074534, 131.70186335403727]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,14,body,"We don’t believe we are the first to recognize a need to attend to both the machines and the 
people, to maximize the interaction between the two. The following are some examples of 
apparent accommodations that people producing information products and devices have devised 
to help us. ","[89.85689440993798, 137.06832298136646, 504.86310559006216, 179.10559006211182]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,14,heading,"Software Tools and Features 
With the acknowledged explosion o","[89.85689440993798, 180.89440993788818, 236.5401242236026, 195.2049689440994]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,14,body,"With the acknowledged explosion of information has come the development of online tools and 
features in ways that have acknowledged the limitations of human-information processing 
capabilities. One example is RSS feeds and other software agents; while we may read only 250 
words per minute (e.g., Haught, & Walls, 2002) and are unlikely to improve significantly in the 
foreseeable future, these feeds allow us a much bigger signal-to-noise ratio, filtering out 
relatively uninteresting words.  
As more and more types of information get stored online, video and image search tools have 
improved to reduce similarly in the set of items users must consider before finding their target 
items. 
Accessibility tools such as screen readers have helped computer users with visual impairments 
gain access to information and capabilities to process that information that would not otherwise 
be available to them. Relatedly, tools such as speech-to-text tools let people carry out some 
tasks more efficiently and perhaps in parallel with visual tasks. ","[90.75130434782619, 194.31055900621118, 501.2854658385094, 338.3105590062111]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,14,heading,"Trends in Information Management 
With the proliferation of digitally-stored inf","[89.85689440993798, 342.78260869565213, 271.42211180124224, 354.4099378881987]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,14,body,"With the proliferation of digitally-stored information, of new and particular import is concern 
with how to manage all this information. There are several trends in the field of Information 
Studies that this research thread will fuel: 
 
Information retrieval (IR): Once created and stored, how shall information be sought 
and retrieved by human users? IR (e.g., Agichtein, Brill, & Dumais, 2006; van 
Rijsbergen, 2006) is concerned with tools to help humans filter the vast expanse of 
information and to help maximize the chances that any information they confront is 
valuable, and that all valuable information is confronted.  
 
Data mining (DM): Related to IR, DM (e.g., Frawley, Piatetsky-Shapiro, & Matheus, 
1992; Hand, Mannila, & Smyth, 2001) entails performing automatic searches of large 
volumes of data for patterns such as association rules or groups of data records (Data 
mining, n.d.). As Dumas points out, “What data mining does is find patterns that 
humans are not likely to find because the trends are buried in too much data. This is an 
example in which the technology overcomes a human weakness” (personal 
communication, December 5, 2013). 
 
Information visualization: How shall information be presented to human users to 
maximize the chances that they can extract the information they need once they have 
accessed the document or other information container (Bederson & Shneiderman, 
2003; Tufte, 2001)? Here is another example of how with attention to particular human 
capabilities (here, in pattern recognition and processing of pictorial information) we can 
design better user interfaces. 
 
Social computing and crowdsourcing: Extending computer-supported cooperative work, 
new advances in distributed, shared conduct of tasks (e.g., Paolacci, 2010) allow for 
multiple people in disparate locations to make progress in parallel on an ever increasing 
variety of tasks. 
 ","[88.9624844720498, 355.3043478260869, 504.86310559006216, 626.3105590062111]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,15,heading,"Software Engineering Practices 
In order for information to be retrieve","[88.0680745341616, 107.55279503105591, 249.95627329192547, 120.0745341614907]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,15,body,"In order for information to be retrieved, data to be mined, and information to be visualized, 
software developers are responsible for producing computer-based functionality. Indeed, they 
get paid and promoted for doing so, especially for doing so on time and with quality. With 
increased functionality, though, comes, without concomitant improvements in the matching of 
that visualization to the needs of the user, increased UI complexity. Additional functionality is 
then created to deal with the complexity, such as the following: 
 
Graphical user interface (GUI) developers provide menus, which acknowledges the fact 
that cued recall is typically better than free recall (Haist, Shimamura, & Squire, 1992), 
and icons (with a nod to Fitts’ Law [e.g., Card, English, & Burr, 1978]), which affords 
the user relatively large targets to hit with their pointing devices. 
 
Internet standards have evolved, such as blue, underlined text to indicate links (given 
that the healthy human visual system is typically good at distinguishing color, except 
for the 10% of males and 1% of females with some form of color vision deficiency, and 
underlines). 
 
The aforementioned RSS feeds and other software agents help us sort through the 
proliferation of digital information, and spam filters help similarly. 
Still words must be read, blue links must be distinguished from purple ones, targets must be 
acquired visually and clicked, and decisions must be made. Research in such topics will drive 
best practices in UI design. If average human capabilities in reading, identifying, clicking, and 
deciding are not likely to change in our lifetime, then computer system developers have to be 
aware of, and work within, these limitations. Whether or not we have reached the limit in taking 
advantage of human processing capabilities—and we assume we have not—someday we will 
approach that limit, and the human will not be advancing further. And so, what software 
engineering practices might be amended, what research threads might we pursue, if we are to 
appropriately acknowledge the tortoise-like advances in rudimentary human-information 
processing? ","[88.9624844720498, 120.0745341614907, 504.86310559006216, 403.60248447204964]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,15,heading,"How Can We Better Accommodate the Fixed Human-Information Processing
Capabilities? ","[89.85689440993798, 414.335403726708, 494.13018633540383, 436.69565217391295]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,15,body,"Clearly, some interaction design practices have acknowledged and taken advantage of the 
human’s higher-level processes, beyond our basic information processing skills. The move from 
text-based screens with keyboard control to GUIs controlled by a mouse to touch-based 
interfaces to speech recognition represents this evolution in our exploitation of human 
capabilities; we can now fly to LA instead of taking that beast-drawn wagon. Barring total 
automation or the hands/eyes/ears-free brain interface—before we get to electron-transfer from 
New York to LA—what are the next steps, or what will fuel them? Below are some areas 
whereby an appreciation of the ever-increasing importance of the human in any human-
computer system may well lead to improved HCI performance. ","[89.85689440993798, 442.9565217391304, 504.86310559006216, 535.0807453416148]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,15,heading,Context-Aware Initiative ,"[89.85689440993798, 540.447204968944, 217.7575155279504, 550.2857142857143]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,15,body,"T. V. Raman (personal communication, June 6, 2009) has suggested that rapid enhancement of 
the ability of devices like phones to sense their environment (e.g., location, time, orientation) 
creates the opportunity to replace human actions and decisions that are now required with 
machine initiative. He cites the example of accessing a bus schedule. Currently, this usually 
requires human initiative, but in some situations a machine could determine that the schedule is 
very likely to be needed, and simply present it. For example, the user may be in a place at a 
time of day when he or she very often calls for the schedule, such as leaving work toward the 
end of the day. ","[88.0680745341616, 551.1801242236025, 505.7575155279503, 632.5714285714287]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,15,heading,"Improvements in Learnability 
A study by Babbage (2011) showed","[89.85689440993798, 638.8322981366459, 241.01217391304354, 650.4596273291925]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,15,body,"A study by Babbage (2011) showed that typical users can use a range of iPhone applications 
with no training. In fact, users with serious brain injuries, including people for whom 
remembering training experiences or instructions is difficult, did quite well too. Increased 
attention to what makes applications easy and hard to learn will pay big dividends in user 
productivity and satisfaction. ","[89.85689440993798, 651.3540372670807, 488.76372670807456, 702.335403726708]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,16,heading,Changes in the Human Operations That Are Required ,"[89.85689440993798, 108.44720496894409, 359.0742857142858, 118.28571428571428]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,16,body,"Changes in the Human Operations That Are Required 
The logic of visualization is the replacement of difficult judgments by easy ones, for example, 
replacing the task of finding the largest number in a list with the task of seeing the highest 
point on a curve. More generally, replacing difficult cognitive processes with easier perceptual 
ones will continue to deliver improvements in UIs. ","[88.9624844720498, 118.28571428571428, 493.23577639751556, 159.42857142857144]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,16,heading,"Cognitive Dimensions Analysis for Configuration Facilities 
Thomas Green and others (e.g., Green & Petre, 1996) have documen","[88.9624844720498, 165.68944099378882, 383.2233540372672, 176.4223602484472]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,16,body,"Thomas Green and others (e.g., Green & Petre, 1996) have documented many ways in which 
representations can support complex tasks like programming better or less well. They captured 
these insights in the form of dimensions for designers to consider. For example, a 
representation is viscous if it is difficult to make changes to it. Increased attention to such ideas
can help make it easier for users to configure their computational tools and thus enable the 
machine to do more of the work in a hybrid human-machine system. ","[89.85689440993798, 177.31677018633542, 502.17987577639747, 239.9254658385093]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,16,heading,"IT Acquisitions/Practice 
Imagine if your company or o","[88.9624844720498, 241.71428571428572, 214.17987577639764, 255.1304347826087]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,16,body,"Imagine if your company or organization were considering two competing employee 
management systems. System 1 costs $100,000 to purchase, and System 2 costs $200,000 to 
purchase. However, System 2 requires less user training, affords the users quicker and more 
direct access to the system functions, and leads to less user frustration. At what point does it 
make more sense to purchase System 2? A consideration of total cost of ownership, informed 
by crisp awareness of user capabilities, will lead to maximally efficient human-machine systems 
(e.g., Menard, 2009).  ","[89.85689440993798, 255.1304347826087, 503.0742857142858, 328.472049689441]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,16,heading,"Higher Education Curricula 
This consideration of the fixed h","[88.9624844720498, 332.94409937888196, 226.70161490683233, 342.78260869565213]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,16,body,"This consideration of the fixed human-information processing capabilities needs to inform higher
education curricula, in programs of Information Studies, Computer Science, Communications, 
and beyond. We have written elsewhere on the dangers of amateur usability engineering (Bias, 
2003). Curricula (beyond traditional HF programs) need to include UCD and to address the 
increasing relative importance of the human, and the human’s strengths and weaknesses, in 
any human-computer systems, for the ultimate betterment of subsequent UI designs.  ","[89.85689440993798, 343.67701863354034, 502.17987577639747, 405.39130434782606]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,16,heading,"HCI Research 
More research sh","[88.9624844720498, 408.96894409937886, 160.51527950310566, 421.49068322981356]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,16,body,"More research should intentionally combine the study of psychophysics and perception with 
experimentation on computer presentation of information. An even larger opportunity exists for 
work on the cost structure of human operations in programming. The few beginnings in this 
area show that the critique of Newell and Card (1985) over 25 years ago still holds: “Millions for 
compilers but hardly a penny for understanding human programming language use” (p. 212). 
The cost structure of social operations also badly needs inquiry. Our ability to predict what 
operations will and will not be effectively carried out by self-organizing volunteers is weak, yet 
this kind of work is already of decisive importance in many real-world activities.  
One act that would help drive integration of UCD would be cross-publishing in HCI and computer 
science venues. Perhaps this call is to editorial boards, who will need to avoid a dependency on 
insider lingo and appreciate the value of the contributions from other “camps.” The answer here 
requires action on all fronts—writers need to pursue new venues and the editors and reviewers 
need to be receptive.  ","[88.9624844720498, 421.49068322981356, 505.7575155279503, 565.4906832298136]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,16,heading,Why Not Usability? ,"[88.9624844720498, 575.3291925465838, 191.81962732919266, 589.639751552795]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,16,body,"Why aren’t the things we’ve just called for already being done or (since all of them are being 
done in some places) done more? Since the early days of the study of HCI (e.g., the first 
Human Factors in Computer Systems conference in Gaithersburg, Maryland, in 1982, or the 
publication of Card, Moran, and Newell’s The Psychology of Human-Computer Interaction in 
1983), numerous researchers and computer scientists have argued that usability deserves a 
place at the design table (see Johnson & Henderson, 2012). The attempts to make this 
argument or to support it have taken numerous forms. In those early days of the late 1970s 
and ‘80s much of the focus was on establishing research methods and a scientific knowledge 
base (e.g., Card, English, & Burr, 1978). Later, in the ’80s and into the ’90s, many researchers 
developed models of the interaction between humans and computers and used those models to 
further expand the usability toolkit (e.g., Carroll & Rosson, 1992; Lewis, Polson, Wharton, & 
Rieman, 1990). Researchers later addressed the potential economic effects of usability by ","[88.9624844720498, 594.1118012422361, 500.39105590062115, 717.5403726708074]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,17,body,"examining various approaches to cost justifying usability (e.g., Bias & Mayhew, 1994, 2005). 
These analyses focused on the importance of usability for (a) reducing user errors and task 
completion time and increasing user efficiency and satisfaction, thus leading to increased sales 
or usage; (b) decreasing expensive changes late in the design process; (c) decreasing training 
time and effort as well as turnover of employees; and (d) decreasing the costly customer 
support burden. More recently, usability has been expanded to focus on the entire user 
experience (e.g., Hassenzahl & Tractinsky, 2006). User experience can be considered to 
encompass more of the user’s interaction with a technological artifact than did traditional 
usability analyses, including affective responses, preferences, and beliefs, in addition to the 
perceptual and cognitive underpinnings more often examined in usability analyses (e.g., ISO 
FDIS 9241-210, 2009). Yet, despite all of the research reports, chapters, and books extolling 
the importance of usability, it seems that still more progress is needed to increase the 
acceptance of usability in the product design process. The situation is better than it was two 
decades ago when Landauer (1995) asserted “Systems are only rarely tried out on users in their 
environment before they are sold” (p. 133). But maybe not as much better as we would have 
hoped. 
Why has so much work yielded so few (or at least insufficient, we assert) results? Below we 
suggest some possible factors. Overcoming these factors may be necessary to create conditions 
for accommodating our fixed human-information processing capabilities.  
Poor science base for usability engineering 
Much of the basic research on perception and cognition doesn’t generalize well to real world 
tasks (or at least this generalization has not been a focus). For example, task switching 
research tends to use operations such as adding numbers or searching for a target letter and to 
force the participant to task switch with no preparation (Panepinto, 2010). 
There is a need to push the contextual paradigm for cognitive research, especially applied 
cognitive research, for example, by studying explicitly simulations of real world tasks. 
Panepinto’s (2010) study of task switching, for example, entailed real-world tasks (document 
proofreading and completing a Sudoku puzzle) and real-world situations (forced vs. self-
selected task switching). 
Poor communication of the relevant science base to usability engineers 
The relevant science base that does apply to HCI is frequently misapplied. For example, Fitts’ 
Law, which predicts movement times based on the ratio of the movement distance and the size 
of the target being moved to, has been applied to designs of buttons that are of a constant size 
(e.g., Silfverberg, MacKenzie, & Korhonen, 2000)—the lack of variation in the target size makes 
the use of Fitts’ Law meaningless. A second example is the misapplication of Miller’s (1956) 
“magical number seven plus or minus two.” In his classic paper, Miller describes research in 
one-dimensional absolute judgment tasks (e.g., identifying levels of saltiness or brightness) and 
in short-term memory tasks that suggest limitations in information processing and short-term 
storage of about five to nine items. However, this idea of limited processing has been too 
broadly applied to topics like menu design, where HCI designers have proposed that the ideal 
menu size is seven because of Miller’s magical number (Comber, 1995). A list of menu items is 
neither a one-dimensional stimulus nor does it involve short-term memory (well, not to the 
same degree as Miller’s task) because the items are all visually available. Even when limitations 
on short-term memory are important in design, such as, with an interactive voice response 
(IVR) system, the concept of the magical number has been misapplied to suggest that menus 
should be limited to no more than five items (Commarford, Lewis, Smither, & Gentzler, 2008). 
Commarford et al. (2008) showed that a broader IVR menu produced better performance than a 
shallow menu, in part because users can discard menu items that don’t match their goals from 
working memory, thereby reducing the load on working memory. The discipline of Psychology 
will do well to develop and nurture a cadre of applications specialists whose job involves the 
transfer of knowledge from basic research to application and to translate the issues in applied 
domains into interesting problems for basic researchers (e.g., Gillan & Bias, 1992; Gillan & 
Schvaneveldt, 1999). ","[88.9624844720498, 110.2360248447205, 505.7575155279503, 694.2857142857142]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,18,body,"We need to develop ways to make science base understandable and usable by its consumers—
designers and usability engineers. One example of this approach is the development of 
workload measures as a method for applying the basic science construct of cognitive workload 
to design. A historically useful workload measure is the NASA-TLX (Hart & Staveland, 1988). To 
enhance its ease of application even further, researchers have developed online versions of the 
NASA-TLX (e.g., Sharek, 2011). Another example would be CogTools, which integrate into a UI 
prototyping tool some automated evaluation based on empirically-based human performance 
models (Bellamy, John, & Kogan, 2011). 
Users haven’t demanded usable products  
It has been noted frequently that users who experience usability problems have a tendency to 
blame themselves for the problem. One focus during the 2010s should be educating users about
usability—perhaps a Usability Reports magazine modeled after Consumer Reports?. 
The computer industry has a history of “satisficing” 
Imagine if drug manufacturers had as many recalls as software has bugs (software defects and 
usability problems). Of course, most software problems don’t lead to such serious consequences 
as medication problems might. (Though certainly some do.) But still, there has been a sense 
that it is acceptable to “get something out there” and make incremental improvements as the 
actual users unearth problems. We believe that as stakeholders (and purse-string holders) get 
educated as to the costs (in terms of customer/user satisfaction and, directly or indirectly, profit 
margins) of this approach, a more proactive approach to attention to human needs and 
capabilities will take hold. An emerging science of design (Baldwin, 2007), especially as applied 
to web pages and other software UIs, is serving to drive this change. Also, the aforementioned 
work by Babbage and by the CogTools team highlights the increasing, and increasingly, 
perceived value of attention to human limitations in the design of computing systems.  ","[88.9624844720498, 108.44720496894409, 503.96869565217395, 386.6086956521739]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,18,heading,Recommendations ,"[89.85689440993798, 399.1304347826087, 189.136397515528, 409.8633540372671]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,18,body,"Moving forward, the discipline of usability would do well to reinforce 
 
the recognition of the disparity between the speed of advancement of technology and 
the speed of advancement of human-information processing capability, 
 
the use of this fact to advocate for more resources devoted to usability practice and 
HCI research, and  
 
the systematic attention to ways in which system design can accommodate our fixed 
human-information processing capabilities. ","[89.85689440993798, 416.1242236024844, 494.13018633540383, 498.4099378881987]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,18,heading,Conclusion ,"[89.85689440993798, 510.93167701863354, 148.88795031055912, 520.7701863354037]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,18,body,"Given the proliferation of digitized information and the unmistakable and inexorable advances in 
computing technology, the human user is losing the race in advancing information processing 
capabilities. In this treatise we have leaned on a century of published psychological literature to 
demonstrate this. We have identified software engineering and design practice, and research 
threads, that have already (explicitly or implicitly) acknowledged the need for ongoing and 
increased attention to the human component of any human-computer system, and we have 
proposed future actions that will undergird better design. Our intention is to motivate strides 
both in our understanding of the human limits that should drive all computer design and in our 
communicating this fact to the greater HCI-design community.  ","[88.9624844720498, 526.1366459627328, 504.86310559006216, 620.0496894409938]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,19,heading,Tips for Usability Practitioners ,"[88.9624844720498, 108.44720496894409, 252.63950310559017, 118.28571428571428]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,19,body,"It is our hope that an explicit awareness of the ever diverging functions in Figure 3, above, will 
motivate an increased level of intentionality among web and all designers to pursue, 
systematically, data about what the users of their designs will find usable. The following tips 
may help usability practitioners or, more likely, help them educate their designer/developer 
colleagues: 
 
When vying for resources (for time or money to conduct usability studies or to respond 
to the findings of such studies) it is important to advocate for your findings. 
 
The ever-growing disparity between the non-growth in human performance and the 
geometrical improvements in computational capability help argue for more attention to 
the user in any human-computer system. 
 
Humbly, we invite you to use (with proper acknowledgement of the Journal of Usability 
Studies) Figure 3, in this article, to help make this point. ","[89.85689440993798, 124.54658385093167, 503.96869565217395, 260.49689440993785]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,19,heading,Acknowledgements ,"[88.9624844720498, 273.01863354037266, 195.39726708074542, 283.75155279503105]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,19,body,"The first author would like to gratefully acknowledge financial support from the University of 
Texas at Austin IC2 Institute. The second author would like to thank the NSF Broadening 
Participation in Computing program. Also we thank Joe Dumas for incisive comments on an 
earlier draft, Marie Panepinto for mining the research studies on human-information processing, 
and Ji Hyun Park and Virginia Luehrsen for help presenting those data graphically. We also 
thank Cheryl Bias for commenting on earlier drafts.  ","[88.9624844720498, 288.22360248447205, 503.96869565217395, 349.9378881987577]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,19,heading,References ,"[88.9624844720498, 362.45962732919253, 150.6767701863355, 374.0869565217391]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,19,references,"Agichtein, E., Brill, E., & Dumais, S. T. (2006). Improving web search ranking by incorporating 
user behavior. In Proceedings of SIGIR 2006. Retrieved October 19, 2006, from 
http://research.microsoft.com/~sdumais/SIGIR2006-fp345-Ranking-agichtein.pdf 
Amdahl’s law. (n.d.). In Wikipedia. Retrieved September 27, 2009, from 
http://en.wikipedia.org/wiki/Amdahl's_law 
Babbage, D. (2011, June). Cognitive barriers to mainstream mobile computing devices in 
neurorehabilitation. Advancing Rehabilitation Technologies for an Aging Society 
(RESNA/ICTA), Toronto. 
Baldwin, C. Y. (2007, March 1). Steps toward a science of design. NSF PI Conference on the 
Science of Design, Alexandria, VA. Retrieved July, 1, 2014, from 
http://www.people.hbs.edu/cbaldwin/dr2/baldwinscienceofdesignsteps.pdf  
Bayliss, D. M., Jarrold, C., Gun, D. M., & Baddeley, A. D. (2003). The complexities of complex 
span: Explaining individual differences in working memory in children and adults. Journal of 
Experimental Psychology: General, 132, 71–92. 
Beck, C. H. (1963). Paced and self-paced serial simple reaction time. Canadian Journal of 
Psychology 17, 90–97. 
Bederson, B. B., & Shneiderman, B. (2003). The craft of information visualization: Readings and 
reflections. San Francisco: Morgan Kaufmann. 
Bellamy, R., John, B. E., & Kogan, S. (2011). Deploying CogTool: Integrating quantitative 
usability assessment into real-world software development. In Proceedings of the 33rd 
International Conference on Software Engineering (ICSE ’11; pp. 691–700). New York, NY: 
ACM. 
Bias, R. G. (2003). The dangers of amateur usability engineering. In S. Hirsch (chair), Usability 
in practice: Avoiding pitfalls and seizing opportunities. Annual meeting of the American 
Society of Information Science and Technology, October, Long Beach. 
Bias, R. G., & Mayhew, D. J. (Eds.). (1994). Cost-justifying usability. Boston: Academic Press. 
Bias, R. G., & Mayhew, D. J. (Eds.). (2005). Cost-justifying usability: Update for the Internet 
age (2nd ed.). San Francisco: Morgan Kaufmann. ","[88.9624844720498, 378.5590062111801, 507.5463354037267, 716.6459627329192]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,20,references,"Brynjolfsson, E. (1993, December). The productivity paradox of information technology: Review 
and assessment. Communications of the ACM. 36, 66-77. 
Burgess, P. W. (2000). Real-world multitasking from a cognitive neuroscience perspective. In S. 
Monsel & J. Driver (Eds.), Control of cognitive processes, attention and performance (No. 
XVIII; pp. 465–472). Cambridge, MA: MIT Press. 
Card, S. K., English, W. K., & Burr, B. J. (1978). Evaluation of mouse, rate-controlled isometric 
joystick, step keys, and text keys for text selection on a CRT. Ergonomics, 21, 601–613. 
Card, S. K., Moran, T., & Newell, A. (1983). The psychology of human-computer interaction. 
Hillsdale, NJ: Erlbaum. 
Carroll, J. M., & Rosson, M. (1992). Getting around the task-artifact cycle: How to make claims 
and design by scenario. ACM Transactions on Information Systems, 10, 181–212. 
Chincotta, D., Underwood, G. A., Ghani, K., Papadopoulou, E., & Wresinski, M. (1999). Memory 
span for Arabic numerals and digit words: Evidence for a limited-capacity, visuo-spatial 
storage system. The Quarterly Journal of Experimental Psychology. Section A: Human 
Experimental Psychology, 52A(2), 325–351. 
Comber, T. (1995). Building usable web pages: An HCI perspective. In A. Ellis & R. Debreceny 
(Eds.), AusWeb95, innovation and diversity: The World Wide Web in Australia, Proceedings 
of AusWeb95, the first Australian World Wide Web Conference (pp. 119–124). Lismore, 
Australia: Norsearch Ltd. 
Commarford, P. M., Lewis, J. R., Smither, J. A., & Gentzler, M. D. (2008). A comparison of 
broad versus deep auditory menu structures. Human Factors, 50, 77–89.  
Cothran, D. L., & Larsen, R. (2008). Comparison of inhibition in two timed reaction tasks: The 
color and emotion Stroop tasks. Journal of Psychology, 142, 373–385. 
Data mining. (n.d.) In Wikipedia. Retrieved October, 19, 2006, from 
http://en.wikipedia.org/wiki/Data_mining#_ref-1 
Donders, F. C. (1969). On the speed of mental processes. In W.G. Koster (Ed.), Attention and 
performance II. Acta Psychologica, 30, 412-431. (Original work published in 1868.) 
Dubash, M. (2005, April 13). Moore’s Law is dead, says Gordon Moore. Techworld.  Retrieved 
July 1, 2011, from http://news.techworld.com/operating-systems/3477/moores-law-is-
dead-says-gordon-moore/  
Dulaney, C. L., & Rogers, W.A. (1994). Mechanisms underlying reduction in Stroop interference 
with practice for young and old adults. Journal of Experimental Psychology, 20, 470–484. 
Emmerich, D. S., Fantini, D. A., & Ellermeier, W. (1989). An investigation of the facilitation of 
simple auditory reaction time by predictable background stimuli. Perception & 
Psychophysics, 45, 66–70. 
Fay, P. J. (1936). The effect of cigarette smoking on simple and choice reaction time to colored 
lights. Journal of Experimental Psychology, 19, 592–603. 
Foehr, U. (2006). Media multitasking among American youth: Prevalence, predictors and 
pairings. Report Number 7592. Kaiser Family Foundation. Retrieved February 22, 2012 from
http://www.kff.org/entmedia/upload/7592.pdf   
Frawley, W. J., Piatetsky-Shapiro, G., & Matheus, C. J. (1992). Knowledge discovery in 
databases: An overview. AI Magazine, 13, 57–70. 
Gates, A. I. (1916). The mnemonic span for visual and auditory digits. Journal of Experimental 
Psychology, 1(5), 393–403. 
Gillan, D. J., & Bias, R. G. (1992). The interface between human factors and design. 
Proceedings of the Human Factors Society 36th Annual Meeting (pp. 443–447). 
Gillan, D. J., & Schvaneveldt, R. W. (1999). Applying cognitive psychology: Bridging the gulf 
between basic research and cognitive artifacts. In F. T. Durso, R. Nickerson, R. 
Schvaneveldt, S. Dumais, M. Chi, & S. Lindsay (Eds.), The handbook of applied cognition 
(pp. 3–31). Chichester, England: Wiley. ","[88.9624844720498, 109.3416149068323, 503.96869565217395, 722.0124223602484]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,3,heading,Introduction ,"[89.85689440993798, 114.70807453416151, 158.72645962732926, 127.22981366459629]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,3,body,"People do things. From birth until death, humans engage in tasks—intentional and automatic, 
visible and invisible, learned and reflexive, atomic and complex—performing them individually 
and in groups. Many of these tasks are heavily supported by computing and communication 
systems. Much of the technology that we use extends and enhances our abilities to act on the 
world. For example, the lever, much beloved by Archimedes who wanted to move the world 
with one, enhances the effective strength of its user. Early in human history technologies 
tended to increase our physical abilities in acting in the physical world probably because most 
tasks relied heavily on physical interactions with the world. In contrast, modern technology 
often extends and enhances our cognitive abilities to act in representational worlds. These 
technological systems are used to manipulate representations of the relevant domains of the 
tasks, such as airframes, insurance risks, employees of an organization, sounds, or disease 
organisms. Thus, the representational/technological systems are valuable because they map 
work from the domains of interest into representational domains in which the needed work can 
be performed more quickly, more accurately, more safely, or better in some other way than it 
can be done directly. These advantages of computational and communication systems have 
been fueled by stunning improvements in the speed and cost of the technologies concerned. 
In nearly all of these systems, however, key operations remain to be performed by human 
participants. In typical systems, people have to configure the technology to apply it to particular 
tasks by creating new programs or by interacting with existing systems to specify needed 
parameters and options. Often, people must provide inputs and interpret outputs. Thus, typical 
computational systems are hybrids, combining operations carried out by digital technology with 
operations carried out by people.  
The net value of any hybrid system depends on the contributions of disparate parts and the 
interactions among those parts. And those parts are likely to develop at different rates. For 
computing and communication systems, we know that the performance of the digital parts has 
increased rapidly. But what is happening to the performance of the human parts and the 
interactions among the parts? And what might the implications be—for the design of human-
computer interfaces, for human-computer interaction (HCI) curricula, and for the HCI research 
agenda—of these evolving technological and human capabilities? ","[88.0680745341616, 130.8074534161491, 503.96869565217395, 442.9565217391304]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,3,heading,Background and Purpose: The Problem ,"[88.9624844720498, 453.6894409937888, 298.2544099378883, 464.42236024844715]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,3,body,"Over two decades ago Donald Norman characterized usable design as “the next competitive 
frontier” (1990). Although user-centered design (UCD) motivated strides in our discipline, being 
seen as an area worthy of study and rigorous practice (e.g., Vredenburg, Isensee, & Righi, 
2002), the progress has been slow and incomplete. Usability processes (requirements 
gathering, empirically-based design, prototype testing, end-of-the-cycle testing, field testing) 
grew from, among other disciplines, human factors (HF) and are concerned with people’s ability 
to carry out intended tasks with tools. These usability engineering practices have spawned 
information architecture and have evolved, in some quarters, to “user experience” or 
“information experience design.” But it is still too often the case that the gathering of user data 
to inform or evaluate user interfaces (UIs) is an afterthought or left to the feedback gleaned 
from post-ship customer gripes; as evinced by some remarkably unusable website UIs, 
sometimes a site or software product’s first visitors/users are still its first test subjects.  
In 1999, Meister reported the results of a survey he conducted of HF professionals who received 
their degrees before 1965. He noted the early conflicts with other engineers and observed that 
HF professionals were often not taken seriously. In 1985 Newell and Card quoted Muckler’s 
(1984) “lament” about human-factors specialists not being taken seriously. Our intent here is to 
honor this sentiment, believing that the struggle persists decades later, in some measure, at 
least for the usability or UCD professional, but to honor it in a constructive way, illustrating 
possible causes and proposing solutions. 
While there have been many demonstrations of the quantifiable value of following a UCD 
approach (e.g., Bias & Mayhew, 1994, 2005), the siren call of features and schedule can 
relegate usability to a post-ship issue. Good design can reduce the human costs associated with 
the human operations. Strategically, for example, a greater share of development resources ","[88.9624844720498, 469.78881987577637, 503.96869565217395, 718.4347826086956]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,21,references,"Goodrich, S., Henderson, L., Allchin, N., & Jeevaratnam, A. (1990). On the peculiarity of simple 
reaction time. The Quarterly Journal of Experimental Psychology. 42A, 763–775. 
Green, C. S., & Bavelier, D. (2007). Action video game experience alters the spatial resolution 
of attention. Psychological Science, 18(1), 88–94. 
Green, C.S., & Bavelier, D. (2003). Action video games modify visual selective attention. 
Nature, 423, 534-537. 
Green, T. R. G., & Petre, M. (1996). Usability analysis of visual programming environments: A 
‘cognitive dimensions’ framework. Journal of Visual Languages and Computing, 7, 131–174. 
Haist, F., Shimamura, A. P., & Squire, L. R. (1992). On the relationship between recall and 
recognition memory. Journal of Experimental Psychology: Human Learning and Memory, 
18, 691–702.  
Hand, D., Mannila, H., & Smyth, P. (2001). Principles of data mining. Cambridge, MA: MIT 
Press. 
Hart, S. G., & Staveland, L. E. (1988). Development of a NASA-TLX (Task Load Index): Results 
of empirical and theoretical research. In P.A. Hancock and N. Meshkati (Eds.), Human 
mental workload (pp. 139–183). Amsterdam: North-Holland. 
Hassenzahl, M., & Tractinsky, N. (2006). User experience—A research agenda. Behaviour and 
Information Technology, 25, 91–97.  
Haught, P. A., & Walls, R. T. (2002). Adult learners: New norms on the Nelson-Denny reading 
test for healthcare professionals. Reading Psychology, 23, 217–238. 
Heathcote, A., Popiel, S. J., & Mewhort, D. J. K. (1991). Analysis of response time distributions: 
An example using the Stroop Task. Psychological Bulletin, 109, 340–347. 
Hess, T. M. (2005). Memory and aging in context. Psychological Bulletin, 131, 383–406. 
Hintsman, D. L., Carre, F. A., Eskridge, V. L., Owens, A. M., Shaff, S. S., & Sparks, M. E. 
(1972). ""Stroop"" effect: Input or output phenomenon? Journal of Experimental Psychology 
95, 458–459. 
Inui, N. (1997). Simple reaction times and timing of serial reactions of middle-aged and old 
men. Perceptual and Motor Skills, 84, 219–225. 
ISO FDIS 9241-210:2009. (2009). Ergonomics of human system interaction—Part 210: Human-
centred design for interactive systems (formerly known as 13407). International 
Organization for Standardization (ISO). Switzerland. 
Johnson, J., & Henderson, A. (2012). Usability of interactive systems: It will get worse before it 
gets better. Journal of Usability Studies, 7(3), 88–93 
Klemmer, E. T. (1956). Time uncertainty in simple reaction time. Journal of Experimental 
Psychology, 51, 179–84. 
Kohfeld, D. L. (1969). Effects of the intensity of auditory and visual ready signals on simple 
reaction time. Journal of Experimental Psychology, 82, 88–95. 
Kryder, M. H., & Kim, C. S. (2009). After hard drives—What comes next? IEEE Transactions on 
Magnetics, 45(10), 3406–3413. 
Landauer, T. K. (1995). The trouble with computers: Usefulness, usability and productivity. 
Cambridge, MA: MIT Press. 
Lee, T. B. (2010). Open systems user interfaces suck. Retrieved November 18, 2010, from 
http://timothyblee.com/2010/11/15/open-user-interfaces-suck/  
Lee, Y., Lu, M., & Ko, H. (2007). Effects of skill training on working memory capacity. Learning 
and Instruction, 17, 336–344. 
Lenhart, A., Rainie, L., & Lewis, O. (2001). Teenage Life Online: The Rise of the Instant-
Message Generation and the Internet's Impact on Friendships and Family Relationships. 
Washington, DC: Pew Internet & American Life Project.  ","[88.0680745341616, 108.44720496894409, 503.0742857142858, 710.3850931677018]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,22,references,"Lewis, C., Polson, P, Wharton, C., & Rieman, J. (1990). Testing a walkthrough methodology for 
theory-based design of walk-up-and-use interfaces. Proceedings of the SIGCHI conference 
on human factors in computing systems: Empowering people (pp. 235–242). New York: 
Association for Computing Machinery. 
Liefooghe, B., Barrouillet, P, Vandierendonck, A., & Camos, V. (2008). Working memory costs of 
task switching. Journal of Experimental Psychology: Learning, Memory, and Cognition, 34, 
478–494. 
Martin, M. (1978). Memory span as a measure of individual differences in memory capacity. 
Memory & Cognition, 6, 194–198. 
Meister, D. (1999). The history of human factors and ergonomics. Mahwah, NJ: Lawrence 
Erlbaum Associates. 
Meiran N., Chorev Z., & Sapir A. (2000). Component processes in task switching. Cognitive 
Psychology, 41, 211–253. 
Menard, R. (2009, September 2). It ain’t the price—It’s the cost, stupid! Purchasing and 
Negotiation Training. Retrieved July 1, 2011 from 
http://purchasingnegotiationtraining.com/purchasing/it-aint%E2%80%99-the-price-
%E2%80%93-it%E2%80%99s-the-cost-stupid/  
Miller, G. (1956). The magical number seven, plus or minus two: Some limits on our capacity 
for processing information, Psychological Review, 63, 81–97. 
Miyake, A., Friedman, N.P., Emerson, M.J., Witzki, A.H., & Howerter, A. (2000). The unity and 
diversity of executive functions and their contributions to complex “frontal lobe” tasks: A 
latent variable analysis. Cognitive Psychology, 41, 49–100.  
Muckler, F. (1984). The future of human factors. Human Factors Society Bulletin, 27(2), 1. 
Multiplexing. (n.d.). In Wikipedia. Retrieved September 27, 2009, from 
http://en.wikipedia.org/wiki/Multiplexing  
Newell, A., & Card, S. (1985). The prospects for psychological science in human-computer 
interaction. Human Computer Interaction, 209–242. 
Norman, Donald A. (1990). The design of everyday things. New York: Doubleday. 
Panepinto, M. (2010). Voluntary versus forced task switching. In Proccedings of the Human 
Factors and Ergonomics Society 54th annual meeting (pp. 453–457). Santa Monica, CA: 
HFES. 
Paolacci, C. (2010). Running experiments on Amazon Mechanical Turk. Judgment and Decision 
Making 5 (5), 411–419. 
Parallel processing. (n.d.). In Wikipedia. Retrieved September 27, 2009, from 
http://en.wikipedia.org/wiki/Parallel_processing   
Park, J. H. (2014). Flow in multitasking. Unpublished dissertation proposal, the University of 
Texas at Austin, Austin, TX. 
Park, J. H., & Bias, R. G. (2012). Understanding human multitasking behaviors through a lens of 
Goal-Systems Theory. Paper presented at the annual meeting of the Association for Library 
and Information Science Education, Dallas, January. 
Patterson, D. (2010). The trouble with multicore. IEEE Spectrum. July. Retrieved July 1, 2011, 
from http://spectrum.ieee.org/computing/software/the-trouble-with-multicore/1   
Raab, D. H. (1962). Effect of stimulus-duration on auditory reaction time. The American Journal 
of Psychology, 75, 298–301. 
van Rijsbergen, C. J. (2006). Information retrieval. Retrieved October 19, 2006, from, 
http://www.dcs.gla.ac.uk/Keith/Preface.html  
Salo, R., Henik, A., & Robertson, L. C. (2001). Interpreting Stroop interference: An analysis of 
differences between task versions. Neuropsychology, 15, 462-471. ","[88.9624844720498, 107.55279503105591, 505.7575155279503, 705.0186335403725]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,23,references,"Salthouse, T. A. (1996). The processing-speed theory of adult age differences in cognition. 
Psychological Review, 103, 403–428. 
Salvucci, D. D., Taatgen, N. A., & Borst, J. P. (2009). Toward a unified theory of the 
multitasking continuum: From concurrent performance to task switching, interruption, and 
resumption. In Proceedings of CHI2009 (pp. 1819–1828). New York: ACM. 
Schilling, W. (1921). The effect of caffein and acetanilid on simple reaction time. Psychological 
Review, 28, 72–79. 
Sharek, D. (2011). Developing a usable online NASA-TLX tool. In Proceedings of the Human 
Factors and Ergonomics Society 55th Annual Meeting 55 (pp. 1375–1379). 
Silfverberg, M., MacKenzie, I. S. & Korhonen, P. (2000). Predicting text entry speed on mobile 
phones. In Proceedings of CHI2000 (pp. 9–16). New York: ACM. 
Simon, J. R., & Sudalaimuthu, P. (1979)  Effects of S-R mapping and response modality on 
performance in a Stroop task.  Journal of Experimental Psychology: Human Perception and 
Performance, 5(1), 176-187. 
Steinman, A., & Venia, S. (1944). Simple reaction time to change as a substitute for the 
disjunctive reaction. Journal of Experimental Psychology, 34, 152–158. 
Stroop, J. R. (1935). Studies of interference in serial verbal reactions. Journal of Experimental 
Psychology, 18, 643–662.  
Suchoon, S. M., & George, E. J. (1976) Foreperiod effect on time estimation and simple reaction 
time. Acta Psychologica, 41, 47–59. 
Taub, H. A. (1972). A comparison of young adult and old groups on various digit span tasks. 
Developmental Psychology, 6, 60–65. 
Tehrani, R. (2000). As we may communicate. Retrieved November 18, 2013, from 
http://www.tmcnet.com/articles/comsol/0100/0100pubout.htm 
Tufte, E. (2001). The visual display of quantitative information (2nd ed.). Cheshire, CT: Graphics 
Press. 
Vredenburg, K., Isensee, S., & Righi, C. (2002). User-centered design: An integrated approach. 
Upper Saddle River, NJ: Prentice Hall. 
Vroon, P. A., & Vroon, A. G. (1973). Tapping rate and expectancy in simple reaction time tasks. 
Journal of Experimental Psychology, 98, 85–90. 
Wallis, C. (2006, March 27). The multitasking generation. Time. Retrieved February 22, 2012 
from http://www.balcells.com/blog/Images/Articles/Entry558_2465_multitasking.pdf  
Wickens, C. D. (2008). Multiple resources and mental workload. Human Factors, 50(3), 
449–455. 
Wright, B. C., & Wanley, A. (2003). Adults' versus children's performance on the Stroop task: 
Interference and facilitation. British Journal of Psychology, 94, 475–485. 
Zhang, H., & Kornblum, A. (1998). The effects of stimulus-response mapping and irrelevant 
stimulus-response and stimulus-stimulus overlap in four-choice Stroop tasks with single-
carrier stimuli. Journal of Experimental Psychology: Human Perception and Performance, 
24, 3–19. ","[88.9624844720498, 108.44720496894409, 504.86310559006216, 608.4223602484471]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,4,body,"could be devoted to the human aspects of design. And this, we assert, is best instantiated as 
systematic attention to usability in practice, in research, and in pedagogy. To support this 
assertion, we consider trends in machine and human performance. ","[88.9624844720498, 110.2360248447205, 489.65813664596277, 138.85714285714286]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,4,heading,Trends in Performance ,"[88.0680745341616, 151.37888198757764, 214.17987577639764, 163.00621118012424]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,4,body,"Across the past four decades, improvements in computing technology/systems have been 
exciting and inexorable. One characterization of this march, and its inexorability, is Moore's law: 
the empirical observation that the number of transistors on an integrated circuit doubles every 
24 months (see Figure 1). (While it does not diminish our argument, we note that Moore 
suggested that a continued doubling at 24-month intervals cannot be sustained much longer 
due to physical limitations [Dubash, 2005], and the ability to translate increased transistor 
density to performance improvements has been tailing off for some time [see Patterson, 
2010].) Along with the density of the transistor count on integrated circuits, other computing-
technology-related features have shown similar advances, such as magnetic disk storage 
(Kryder & Kim, 2009) and the cost of transmitting information over an optical network (Tehrani, 
2000). Henceforth we will use Moore’s Law to symbolize all of the general advancement of 
technology. ","[88.9624844720498, 167.47826086956522, 503.96869565217395, 292.69565217391295]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,5,body,"In stark contrast are the human-information processing capabilities over the past few decades, 
represented in Figure 2. Key human capabilities include (a) a simple response indicating the 
perception of a visual or auditory stimulus, (b) storage of information in working memory, and 
(c) executive control of attention. These capabilities represent perceptual, cognitive, and motor 
performance. Figure 2 shows performance on tasks designed to measure these various 
capabilities. We searched the literature for papers that reported research on visual simple 
reaction time, auditory simple reaction time, working memory digit span, and the Stroop Effect.
(The Stroop Effect is the interference caused by having color names appear in a task where 
people must name the color of the ink in which the words are printed. When a color name is 
printed in a different color ink, reaction times to announce the color of the ink are negatively 
influenced. This interference effect is popularly interpreted as a reflection of central processing 
in the brain.) We looked for research reports from early in the 20th century to early in the 21st 
century and tried to identify studies with similar conditions (e.g., age and health status of 
participants, type of stimulus, mode of response). The data from these studies are summarized 
in Table 1 and Table 2 and presented graphically in Figure 2. ","[90.75130434782619, 109.3416149068323, 501.2854658385094, 264.0745341614907]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,7,body,"The graphs in Figure 2A and 2B represent the data from research on simple reaction time (RT)—
the time, in msec, to make a simple response (e.g., pressing a key) to the presentation of a 
single, simple stimulus (here, a light [Figure 2A] or a sound [Figure 2B]). Figure 2C is a 
summary of working memory capacity studies, with the values being a number (the number of 
items recalled). And Figure 2D represents the Stroop Effect data, again in msec. In each case, 
in order to represent the values for each variable relative to all the other values, we have 
arbitrarily set the oldest data value as the 100% value and represented all subsequent findings 
as a percentage of that first value. That is, the Gates (1916) “mnemonic span” of 7.9 items 
(actually a mean of two experiments, published in the first year of the Journal of Experimental 
Psychology!) represents the base value, and so the Taub (1972) finding of 7.0 items is 
represented as 88.6% of that first value. It would not matter which value, or even some other 
random value, we chose as the baseline; the relationship among the obtained values, as 
represented by the regression line, would be the same.  
Simple RTs are among the earliest scientifically-studied measures of human performance. In 
1868 Donders described a method for measuring simple reaction times and observed times 
similar to those measured in more recent times (Donders, 1969). Each data value from Table 1 
(that is, each point in Figures 2A and 2B) represents a single experiment or a condition from an 
experiment, and the line in each figure represents the best-fitting line (least squares regression 
function). Both simple RT functions show only modest change during the period of time 
captured in the data. The regression function for RT with visual stimuli is essentially flat, 
increasing only .18 msec per year. The increase in the regression function for auditory stimuli 
was slightly greater, 1.35 msec per year, but the slope of that line was strongly influenced by a 
relatively fast RT from a study in 1920. Note that rather than a decrease in RT over the years, 
the data show a slight increase; people are taking slightly more time, not less time, on these 
simple tasks. These small changes, though, are likely due to procedural differences among the 
studies rather than changes in the basic sensory/perceptual/cognitive/motor functioning of 
humans. RT can be influenced by variables like stimulus intensity, level of test subject arousal, 
training, or even test participant body mass index. The mean RTs across the studies that we 
sampled were 220 msec for visual RT and 229 msec for auditory RT, falling within the range 
that is typically reported for simple RT.  
Another human-information processing variable is expressed by Miller (1956) in which he 
proposed that humans could maintain five to nine “chunks” of information in short-term (or 
working) memory. The graph in Figure 2C shows the data for six papers (and 10 separate 
experiments) that measured the number of items (letters or numbers) that participants could 
recall immediately after hearing or reading a larger set of digits. This measure is typically 
known as the digit span. All of the data presented here are from forward digit span tasks in 
which the participants list the digits recalled in the order in which they were presented; an 
alternative version is the backward digit span task in which the participant responds with the 
recalled digits in the reverse order relative to presentation. As with visual RT, working memory 
performance that was reported changes only slightly over the years (a decline of .007 items per 
year based on the regression model). The slight change is a decline in working memory 
capacity, but is unlikely to be meaningful. The mean number of items recalled across the 10 
experiments in six studies was 7.5, essentially Miller’s (1956) “magic number 7.” ","[89.85689440993798, 261.391304347826, 505.7575155279503, 717.5403726708074]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,8,body,"Stroop (1935) reported two experiments focused on naming colors. Stroop’s study illustrates a 
third human-information processing variable. In Experiment 1, he presented two lists of words. 
For one list, participants had to read aloud a series of color names printed in black. For a second
list, they read a series of color names printed in a color that differed from the one named (e.g., 
the word “red” printed in green ink). He observed no difference in the times to read the two 
lists. In Experiment 2, one list contained the color names printed in a colored ink in such a way 
that the color of the ink was always different from the color name. So, again, the word “red” 
printed in green ink. A second list contained not words but solid colored rectangles, printed in 
the same sequence of colors as the inks used for the word list. In both cases in Experiment 2, 
the participants’ task was to name the color of the ink used to print the word in one list and to 
print the rectangle in the other list. In this comparison, Stroop observed much longer times to 
name the colors when the list was made up of words than when it was made up of blocks. That 
difference in color naming time in Experiment 2 is typically called the Stroop Effect and is 
commonly explained as being a function of the interference of the reading of the color word with
the naming of the color, with reading being seen as the more automatic process that is 
accomplished first, thereby interfering with naming the color of the ink. Researchers have 
proposed that the Stroop Test provides a good measure of executive functioning, an activity 
linked to the frontal lobe (e.g., Miyake, Friedman, Emerson, Witzki, & Howerter, 2000). The 
graph in Figure 2D shows the mean time to respond to name the color in a Stroop Test in eight 
experiments from 1972 to 2008. The regression model shows a very small decrease of 0.8 msec
per year, but the model accounts for only 3% of the variance in the data. 
The stability of many basic human-information processing capabilities across time has allowed 
some researchers to provide engineering-style models of human cognition in which basic 
capabilities can be represented parametrically. For example, Card, Moran, and Newell (1983) 
proposed the Model Human Processor (MHP), equipped with numerous estimates based on 
research findings for simple reaction time, sensory memory decay times, short-term memory 
span, eye movement dwell times, sensitivity of movement time to movement difficulty, and so 
on. Although human-information-processing test performance is sensitive to a variety of factors 
like age, gender, drug states, and sleep deprivation, taken over the population or large samples 
thereof, the basic capabilities that underlie the performance are likely to change only over 
evolutionary time scales.  
However, one failing of the engineering-type models of human cognition such as the MHP is that
cognition consists of more than a set of parameters from basic processes. One area of research 
in which this has become evident is in cognitive aging. Most of the aforementioned information 
processing capabilities increase as people age from birth to about 20 years, then decline, 
gradually for 20 to 30 years, followed by a more dramatic decrease after we turn 40 to 50 (e.g.,
Salthouse, 1996). Yet, human performance in most everyday tasks don’t show similar declines; 
research on the effects of aging on cognition in real-world contexts has demonstrated that older 
adults can perform as well as or better than their younger counterparts in complex tasks to 
which they can bring their greater knowledge and experience (e.g., Hess, 2005). The 
experience of older adults allows them to develop strategies that compensate for the losses in 
fundamental capabilities such as memory capacity, reaction time, and the speed of target-
directed movements. Thus, we might consider human cognition to involve a complex interplay 
of relatively inflexible, hard-wired processes like simple reaction to a stimulus and more flexible,
soft processes like the application of strategies that determine when and how to apply the hard-
wired processes. Interestingly, the hard-wired processes, which are shaped by species’ 
experience with the world, seem to show little change over decades if we examine people of the 
same age, but decline in individuals over time; while humans are good at language and pattern 
recognition and visual processing and memory of pictures and fine motor manipulations with 
fingers, we are not likely to be getting any better any time soon. 
In contrast, the soft processes, because they are based on experience and consequent 
knowledge of an individual, are much more flexible. As the environment in which individuals 
work and live changes over years, such as with the increasing dominance of computing 
technologies, those individuals can acquire new strategies to improve performance working with 
those technologies, even when the technologies are a poor fit with the hard-wired processes. 
This use of soft, strategic processes to overcome loss of function resembles compensation for 
the declines due to age. That is, developments like the application of better personal strategies ","[88.0680745341616, 107.55279503105591, 503.0742857142858, 713.0683229813664]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,9,body,"can help an individual accommodate to the diminution of hard-wired processes like simple 
reaction to a stimulus due to aging, but as a species we are not getting better at a rate that is 
likely to help anyone reading this. In thinking about this we can distinguish changes to soft 
processes, changes to how we do things, from hard-wired changes, changes in our underlying 
mental machinery. Hard-wired changes seem to be happening slowly at best. And indeed, it will 
be the task of HCI designers to increasingly take advantage of these higher-level functions in 
future designs. 
Thus we have the broad message of this review: Figure 3 shows the best fitting line (least 
squares regression function) of each of the four human-information processing variables 
juxtaposed with the function representing Moore’s Law. Digital technologies are advanced by 
deliberate design and engineering improvements and are improving very rapidly; whereas, basic 
human-information processing capabilities are advanced by a slow and weakly directed 
evolutionary process and are improving little if at all. When human operations can actually be 
replaced by machine operations, that is, when true automation is possible, the ultimate limits 
on the possible improvement either in speed or reliability are found in physics1. But what if 
substitution is not possible (or at least not possible yet)? And what are the implications of these 
mismatched trends for net system performance? ","[88.9624844720498, 110.2360248447205, 505.7575155279503, 289.1180124223602]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,10,heading,The Amdahl’s Law Case ,"[88.0680745341616, 108.44720496894409, 216.86310559006222, 120.0745341614907]"
0005_TortoiseandSoftware-MooresLawetcJUS8-14.pdf,10,body,"The interplay between technology and humans is similar to the situation in parallel computation, 
where performance comes to be dominated by the non-parallelizable parts of a problem, as 
characterized by Amdahl's Law (Amdahl’s Law, n.d.). What happens to the performance of a 
system in which some operations are performed by a machine, and hence speed up rapidly over 
time, and some must be performed by a human, and do not speed up? 
Some simple situations are easy to analyze. Suppose a task requires some work to be done by 
a computer and a human working together, so that the computer requires Tc (computer time) 
to do its part and the person requires Tp (person time) to do his or her part. Now suppose the 
computer is made faster by a factor F. The time required for the computer's work will now be 
(1/F)Tc 
which (of course) represents a speedup of F: new time / old time = ((1/F)Tc)/Tc = 1/F 
 
But if we consider the overall task time, including the human's work, we get 
new time / old time = ((1/F)Tc+Tp)/(Tc+Tp) 
As F increases, the numerator is dominated by Tp, the time required for the person’s work. The 
ratio of new time to old time tends towards 
Tp(Tc+Tp) = 1/((Tc/Tp)+1) 
representing a speedup of at most ((Tc/Tp)+1) no matter how large F gets. That is, as the 
computer gets faster, the performance of the human plays a bigger role in determining the total 
task time. 
What happens if some work can be overlapped between the computer and the person? The 
basic situation doesn’t change. As the computer speeds up, the amount of possible overlap 
diminishes, and the time required for human operations still comes to dominate the overall time 
required for the task. 
The situation is shown graphically in Figure 4. The graph shows total task time for 11 doublings 
of computer performance, corresponding to 22 years or so of improvement. No improvement in 
human performance is assumed over this period. The traces in the graph correspond to different 
divisions of labor, Tc/Tp. As can be seen, the effects of improvements in computer performance 
appear early on and are greater when the computer's share of the work is initially larger. But as 
time goes on, further performance improvements deliver essentially no benefit. As in Amdahl's 
Law for parallelism, if some portion of a task cannot be sped up, the increase in speed due to 
improvements diminishes as task completion time comes to be dominated by the time required 
for the unimproved portion. ","[88.9624844720498, 124.54658385093167, 505.7575155279503, 514.5093167701863]"
